{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "* **After applying initial EDA to our datasets, We'll now target the following preprocessing tasks:**\n",
    "\n",
    "    * Handle null values and inconsistencies for all features.\n",
    "    * Create a target variable (issue_happened) reflecting whether\n",
    "    a problem happened during the transfer of this package/receptacle.\n",
    "    * Propose, validate assumptions (with Ms.Lasmi), and work based on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df = pd.read_csv('../data/raw/packages_data_2023_2025.csv', encoding='latin-1', delimiter=';')\n",
    "receptacles_df = pd.read_csv('../data/raw/receptacle_data_2023_2025.csv', encoding='latin-1', delimiter=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "* Columns Renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df = packages_df.rename(columns={'établissement_postal': 'etablissement_postal', 'next_établissement_postal': 'next_etablissement_postal'})\n",
    "receptacles_df = receptacles_df.rename(columns={'ï»¿RECPTCL_FID': 'RECPTCL_FID', 'EVENT_TYPECD': 'EVENT_TYPE_CD', 'nextetablissement_postal': 'next_etablissement_postal'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "* `date` type adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df['date'] = pd.to_datetime(packages_df['date'])\n",
    "receptacles_df['date'] = pd.to_datetime(receptacles_df['date'])\n",
    "packages_df['RECPTCL_FID'] = packages_df['RECPTCL_FID'].str.strip()\n",
    "packages_df['MAILITM_FID'] = packages_df['MAILITM_FID'].str.strip()\n",
    "packages_df['etablissement_postal'] = packages_df['etablissement_postal'].str.strip()\n",
    "packages_df['next_etablissement_postal'] = packages_df['next_etablissement_postal'].str.strip()\n",
    "receptacles_df['etablissement_postal'] = receptacles_df['etablissement_postal'].str.strip()\n",
    "receptacles_df['next_etablissement_postal'] = receptacles_df['next_etablissement_postal'].str.strip()\n",
    "receptacles_df['RECPTCL_FID'] = receptacles_df['RECPTCL_FID'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "- split into train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "didn't use time series split since the split would be done into folds instead of one train test split \n",
    "even if in the future the time series split could be more usefull but for now it is just a complication "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df = packages_df.sort_values('date').reset_index(drop=True)\n",
    "receptacles_df = receptacles_df.sort_values('date').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-respecting split\n",
    "pkg_train_df, pkg_test_df = train_test_split(\n",
    "    packages_df,\n",
    "    test_size=0.2,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "rcl_train_df, rcl_test_df = train_test_split(\n",
    "    receptacles_df,\n",
    "    test_size=0.2,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df = pkg_train_df\n",
    "receptacles_df= rcl_train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df['etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "* `etablissement_postal` have 26621 null values (2.7% of the whole dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "* As its null values are less than 5% of the dataset (2.7%), we drop these null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df = packages_df[~packages_df['etablissement_postal'].isna()]\n",
    "packages_df['etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "* We propose to consider the packages having null `next_etablissement_postal`\n",
    "as having issue during transfer, we'll try to validate that using\n",
    "`EVENT_TYPE_CD` also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages_df['next_etablissement_postal'] = packages_df['next_etablissement_postal'].fillna('Unknown')\n",
    "# packages_df['next_etablissement_postal'].isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "* Let's check if `EVENT_TYPE_CD` can indicate whether the `next_etablissement_postal` is null or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_unknown_next_etablissement = packages_df[packages_df['next_etablissement_postal'].isna()]\n",
    "# keep only top EVENT_TYPES_ID\n",
    "packages_unknown_next_etablissement = packages_unknown_next_etablissement['EVENT_TYPE_CD'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "packages_unknown_next_etablissement.head(10).plot(kind='bar')\n",
    "plt.xlabel('EVENT TYPE CD')\n",
    "plt.ylabel('Null Next Etablissement Postal')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "* `EVENT_TYPE_CD` doesn't indicate null values of `next_etablissement_postal`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "* the following code cell fills the `next_etablissement_postal` using the next `etablissement_postal` for the same package.\n",
    "* if the last route for a specific package is null, then it keeps it null because there's no next `etablissement_postal` for that package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Ensure the dataframe is sorted (same as before)\n",
    "packages_df = packages_df.sort_values(['MAILITM_FID', 'date'])\n",
    "\n",
    "# 2. Look ahead to the next row's postal code and ID\n",
    "shifted_postal = packages_df['etablissement_postal'].shift(-1)\n",
    "shifted_id = packages_df['MAILITM_FID'].shift(-1)\n",
    "\n",
    "# 3. Identify the \"boundaries\" where the postal code changes within the same package\n",
    "# This marks the last row of a block with the value of the start of the next block\n",
    "is_boundary = (packages_df['etablissement_postal'] != shifted_postal) & \\\n",
    "              (packages_df['MAILITM_FID'] == shifted_id)\n",
    "\n",
    "# 4. Use grouped backfill to broadcast those values to all preceding rows in the block\n",
    "# This replaces your 'blocks.map' logic with a single vectorized pass\n",
    "fill_values = shifted_postal.where(is_boundary).groupby(packages_df['MAILITM_FID']).bfill()\n",
    "\n",
    "# 5. Fill only the NaNs in the existing column to match your original logic\n",
    "packages_df['next_etablissement_postal'] = packages_df['next_etablissement_postal'].fillna(fill_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Ensure the dataframe is sorted (same as before)\n",
    "packages_df = packages_df.sort_values(['MAILITM_FID', 'date'])\n",
    "\n",
    "# 2. Look ahead to the next row's postal code and ID\n",
    "shifted_postal = packages_df['etablissement_postal'].shift(-1)\n",
    "shifted_id = packages_df['MAILITM_FID'].shift(-1)\n",
    "\n",
    "# 3. Identify the \"boundaries\" where the postal code changes within the same package\n",
    "# This marks the last row of a block with the value of the start of the next block\n",
    "is_boundary = (packages_df['etablissement_postal'] != shifted_postal) & \\\n",
    "              (packages_df['MAILITM_FID'] == shifted_id)\n",
    "\n",
    "# 4. Use grouped backfill to broadcast those values to all preceding rows in the block\n",
    "# This replaces your 'blocks.map' logic with a single vectorized pass\n",
    "fill_values = shifted_postal.where(is_boundary).groupby(packages_df['MAILITM_FID']).bfill()\n",
    "\n",
    "# 5. Fill only the NaNs in the existing column to match your original logic\n",
    "packages_df['next_etablissement_postal'] = packages_df['next_etablissement_postal'].fillna(fill_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df_copy = packages_df.copy()\n",
    "# make a copy to prevent executing the cell\n",
    "# above many times (it takes too long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df_copy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "* To handle those null values of `next_etablissement_postal`, we'll try to predict them based on the `etablissement_postal` they appear with more frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "etablissement_dict = {} # will hold each etablissement_postal\n",
    "# and the next_etablissement_postal that appeared the most \n",
    "# with it.\n",
    "for etablissement, group in packages_df_copy.groupby('etablissement_postal'):\n",
    "    next_counts = group['next_etablissement_postal'].dropna().value_counts()\n",
    "    if next_counts.empty:\n",
    "        # no observed next_etablissement_postal for this etablissement; skip or fallback\n",
    "        # use predefined next_etablissement if available, else skip\n",
    "        fallback = globals().get('next_etablissement', None)\n",
    "        if fallback is not None:\n",
    "            etablissement_dict[etablissement] = fallback\n",
    "        continue\n",
    "    etablissement_dict[etablissement] = next_counts.index[0]\n",
    "\n",
    "etablissement_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(etablissement_dict.keys()), packages_df_copy['etablissement_postal'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mask\n",
    "null_mask = packages_df_copy['next_etablissement_postal'].isna()\n",
    "\n",
    "# Apply to original dataframe directly\n",
    "packages_df_copy.loc[null_mask, 'next_etablissement_postal'] = (\n",
    "    packages_df_copy.loc[null_mask, 'etablissement_postal'].map(etablissement_dict)\n",
    ")\n",
    "\n",
    "packages_df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df_copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df_copy['MAILITM_FID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "* Like this, we've handled all null values and inconsitencies for `packages` dataset, and we can start applying tasks further.\n",
    "* Now, I propose to encode the following categorical features `etablissement_postal`, `next_etablissement_postal`, `origin_hub`, `destination_country`, `arrival_hub`, `inbound_unit`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "* If we Encode the following features `origin_hub`, `arrival_hub`, `inbound_unit`, and `destination_country` with one-hot encoding, we'd have more than 100 features, so I'm just gonna keep them categorical for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "* **We'll be doing the same steps for `receptacle` dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df = receptacles_df[~receptacles_df['etablissement_postal'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "* apply the function that fills null values of `next_etablissement_postal` using `etablissement_postal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Ensure sorted order\n",
    "receptacles_df = receptacles_df.sort_values(['RECPTCL_FID', 'date'])\n",
    "\n",
    "# 2. Identify transitions where the postal code changes within the same receptacle\n",
    "# This finds the \"start of the next block\"\n",
    "next_postal = receptacles_df['etablissement_postal'].shift(-1)\n",
    "next_id = receptacles_df['RECPTCL_FID'].shift(-1)\n",
    "\n",
    "# Logic: If current postal != next postal AND we are still in the same ID, \n",
    "# then 'next_postal' is the value of the next block.\n",
    "is_boundary = (receptacles_df['etablissement_postal'] != next_postal) & \\\n",
    "              (receptacles_df['RECPTCL_FID'] == next_id)\n",
    "\n",
    "# 3. Create a series that only contains values at those boundaries\n",
    "boundary_values = next_postal.where(is_boundary)\n",
    "\n",
    "# 4. Use grouped backfill (bfill) to propagate the next block's source \n",
    "# to all rows in the current block\n",
    "filled_values = boundary_values.groupby(receptacles_df['RECPTCL_FID']).bfill()\n",
    "\n",
    "# 5. Fill NaNs in the target column as per your original logic\n",
    "receptacles_df['next_etablissement_postal'] = receptacles_df['next_etablissement_postal'].fillna(filled_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df_copy = receptacles_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "* fill remaining null values of `next_etablissement_postal` with the most frequent value appearing with its `etablissement_postal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcp_etablissement_dict = {} # will hold each etablissement_postal\n",
    "# and the next_etablissement_postal that appeared the most \n",
    "# with it.\n",
    "for etablissement, group in receptacles_df_copy.groupby('etablissement_postal'):\n",
    "    next_counts = group['next_etablissement_postal'].dropna().value_counts()\n",
    "    if next_counts.empty:\n",
    "        # no observed next_etablissement_postal for this etablissement; skip or fallback\n",
    "        # use predefined next_etablissement if available, else skip\n",
    "        fallback = globals().get('next_etablissement', None)\n",
    "        if fallback is not None:\n",
    "            rcp_etablissement_dict[etablissement] = fallback\n",
    "        continue\n",
    "    rcp_etablissement_dict[etablissement] = next_counts.index[0]\n",
    "\n",
    "rcp_etablissement_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mask\n",
    "null_mask = packages_df_copy['next_etablissement_postal'].isna()\n",
    "\n",
    "# Apply to original dataframe directly\n",
    "packages_df_copy.loc[null_mask, 'next_etablissement_postal'] = (\n",
    "    packages_df_copy.loc[null_mask, 'etablissement_postal'].map(etablissement_dict)\n",
    ")\n",
    "\n",
    "packages_df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df_copy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "* Null values are gone, but there are still some illogical packages' and receptacles' routes between `etablissement`\n",
    "* We'll treat these logical routes now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each package (group of rows), check whether there's any illogical route\n",
    "# between 'etablissement_postal' and 'next_etablissement_postal'\n",
    "def isPackageIllogical(group):\n",
    "    return (\n",
    "        group['next_etablissement_postal']\n",
    "        .iloc[:-1]\n",
    "        .ne(group['etablissement_postal'].shift(-1).iloc[:-1])\n",
    "        .any()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "illogical_packages = packages_df_copy.groupby('MAILITM_FID').apply(isPackageIllogical)\n",
    "illogical_packages.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "105262 / packages_df_copy['MAILITM_FID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "* 103376 Packages have illogical routes (98%) of all packages\n",
    "* as a lot of packages have at least one illogical route, we'll create three datasets and keep the one that gives the best values\n",
    "* 1st dataset `clean` -> drop all packages having at leeast one illogical route\n",
    "* 2nd dataset `slightly` -> drop all packages having more than one illogical route (we tolerate one illogical route)\n",
    "* 3rd dataset `chaotic` -> the current dataset.\n",
    "* For the last two datasets, we add a feature `illogical` to flag illogical routes, it may be useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_packages_df = packages_df_copy[~packages_df_copy['MAILITM_FID'].isin(illogical_packages[illogical_packages].index)]\n",
    "clean_packages_df['MAILITM_FID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_packages_df['MAILITM_FID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "* Creating `slightly`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Standardize IDs in the source dataframe first\n",
    "packages_df_copy['MAILITM_FID'] = packages_df_copy['MAILITM_FID'].astype(str).str.strip().str.upper()\n",
    "\n",
    "# 2. Vectorized calculation (Logic equivalent to your iloc[:-1] comparison)\n",
    "# We look at the next row's postal code and check if the package ID is the same\n",
    "shifted_postal = packages_df_copy['etablissement_postal'].shift(-1)\n",
    "is_same_package = packages_df_copy['MAILITM_FID'] == packages_df_copy['MAILITM_FID'].shift(-1)\n",
    "\n",
    "# A mismatch counts if the next row is the same package but has a different postal code\n",
    "mismatches = (packages_df_copy['next_etablissement_postal'] != shifted_postal) & is_same_package\n",
    "\n",
    "# 3. Sum the mismatches per group (this produces a unique ID list)\n",
    "illogical_counts = (\n",
    "    mismatches.groupby(packages_df_copy['MAILITM_FID'], sort=False)\n",
    "    .sum()\n",
    "    .astype(int)\n",
    "    .reset_index(name='n_illogical_routes')\n",
    ")\n",
    "\n",
    "illogical_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "illogical_counts['MAILITM_FID'] = illogical_counts['MAILITM_FID'].str.strip().str.upper()\n",
    "illogical_counts[illogical_counts['n_illogical_routes'] > 1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "illogical_counts[illogical_counts['n_illogical_routes'] <= 1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "illogical_counts[illogical_counts['MAILITM_FID'].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_illogical_routes_packages = illogical_counts[illogical_counts['n_illogical_routes'] > 1]['MAILITM_FID'].unique()\n",
    "print (len(multiple_illogical_routes_packages))\n",
    "slightly_packages_df = packages_df_copy[~packages_df_copy['MAILITM_FID'].isin(multiple_illogical_routes_packages)]\n",
    "slightly_packages_df['MAILITM_FID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify sums\n",
    "print(f\"Total packages: {packages_df_copy['MAILITM_FID'].nunique()}\")\n",
    "print(f\"Clean (0 illogical): {clean_packages_df['MAILITM_FID'].nunique()}\")\n",
    "print(f\"Slightly (0 or 1 illogical): {slightly_packages_df['MAILITM_FID'].nunique()}\")\n",
    "print(f\"More than 1 illogical: {len(multiple_illogical_routes_packages)}\")\n",
    "print(f\"\\nSlightly + Multiple = {slightly_packages_df['MAILITM_FID'].nunique() + len(multiple_illogical_routes_packages)}\")\n",
    "print(f\"Should equal total: {packages_df_copy['MAILITM_FID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "* Storing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_packages_df.to_csv('../data/interim/clean_packages_df.csv')\n",
    "slightly_packages_df.to_csv('../data/interim/slightly_packages_df.csv')\n",
    "packages_df_copy.to_csv('../data/interim/chaotic_packages_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "### Handling Illogical Routes for receptacles (same logic with packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mask\n",
    "null_mask = receptacles_df_copy['next_etablissement_postal'].isna()\n",
    "\n",
    "# Apply to original dataframe directly\n",
    "receptacles_df_copy.loc[null_mask, 'next_etablissement_postal'] = (\n",
    "    receptacles_df_copy.loc[null_mask, 'etablissement_postal'].map(rcp_etablissement_dict)\n",
    ")\n",
    "\n",
    "receptacles_df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df_copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each receptacle (group of rows), check whether there's any illogical route\n",
    "# between 'etablissement_postal' and 'next_etablissement_postal'\n",
    "def isReceptacleIllogical(group):\n",
    "    return (\n",
    "        group['next_etablissement_postal']\n",
    "        .iloc[:-1]\n",
    "        .ne(group['etablissement_postal'].shift(-1).iloc[:-1])\n",
    "        .any()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "illogical_receptacles = receptacles_df_copy.groupby('RECPTCL_FID').apply(isReceptacleIllogical)\n",
    "illogical_receptacles.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of illogical receptacles on the total number of receptacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_receptacles_df = receptacles_df_copy[~receptacles_df_copy['RECPTCL_FID'].isin(illogical_receptacles[illogical_receptacles].index)]\n",
    "clean_receptacles_df['RECPTCL_FID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Standardize IDs in the source dataframe first\n",
    "receptacles_df_copy['RECPTCL_FID'] = receptacles_df_copy['RECPTCL_FID'].astype(str).str.strip().str.upper()\n",
    "\n",
    "# 2. Vectorized calculation (Logic equivalent to your iloc[:-1] comparison)\n",
    "# We look at the next row's postal code and check if the receptacle ID is the same\n",
    "shifted_postal = receptacles_df_copy['etablissement_postal'].shift(-1)\n",
    "is_same_receptacle = receptacles_df_copy['RECPTCL_FID'] == receptacles_df_copy['RECPTCL_FID'].shift(-1)\n",
    "\n",
    "# A mismatch counts if the next row is the same package but has a different postal code\n",
    "mismatches = (receptacles_df_copy['next_etablissement_postal'] != shifted_postal) & is_same_receptacle\n",
    "\n",
    "# 3. Sum the mismatches per group (this produces a unique ID list)\n",
    "rcp_illogical_counts = (\n",
    "    mismatches.groupby(receptacles_df_copy['RECPTCL_FID'], sort=False)\n",
    "    .sum()\n",
    "    .astype(int)\n",
    "    .reset_index(name='n_illogical_routes')\n",
    ")\n",
    "\n",
    "rcp_illogical_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_illogical_routes_receptacles = rcp_illogical_counts[rcp_illogical_counts['n_illogical_routes'] > 1]['RECPTCL_FID'].unique()\n",
    "print (len(multiple_illogical_routes_receptacles))\n",
    "slightly_receptacles_df = receptacles_df_copy[~receptacles_df_copy['RECPTCL_FID'].isin(multiple_illogical_routes_receptacles)]\n",
    "slightly_receptacles_df['RECPTCL_FID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify sums\n",
    "print(f\"Total receptacles: {receptacles_df_copy['RECPTCL_FID'].nunique()}\")\n",
    "print(f\"Clean (0 illogical): {clean_receptacles_df['RECPTCL_FID'].nunique()}\")\n",
    "print(f\"Slightly (0 or 1 illogical): {slightly_receptacles_df['RECPTCL_FID'].nunique()}\")\n",
    "print(f\"More than 1 illogical: {len(multiple_illogical_routes_receptacles)}\")\n",
    "print(f\"\\nSlightly + Multiple = {slightly_receptacles_df['RECPTCL_FID'].nunique() + len(multiple_illogical_routes_receptacles)}\")\n",
    "print(f\"Should equal total: {receptacles_df_copy['RECPTCL_FID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_receptacles_df.to_csv('../data/interim/clean_receptacles_df.csv')\n",
    "slightly_receptacles_df.to_csv('../data/interim/slightly_receptacles_df.csv')\n",
    "receptacles_df_copy.to_csv('../data/interim/chaotic_receptacles_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
