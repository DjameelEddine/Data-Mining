{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Load Merged Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#used clean one\n",
    "merged_df = pd.read_csv( '../data/interim/interim_merged_packages_receptacle_df.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Sort Data Chronologically\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sort chronologically\n",
    "merged_df = merged_df.sort_values('date_package').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "\n",
    "### Remove unnecessary index columns that were created during the merge operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(columns=[\n",
    "    'Unnamed: 0_package',\n",
    "    'Unnamed: 0_receptacle',\n",
    "    #'RECPTCL_FID', 'MAILITM_FID', 'serial_number'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Convert the date column to datetime format and extract temporal features:\n",
    "- **hour**: Hour of day when package was processed\n",
    "- **day_of_week**: Day of the week (Sunday -->Thursday)\n",
    "- **is_weekend**: Binary flag indicating weekend (Friday/Saturday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['date_package'] = pd.to_datetime(merged_df['date_package'])\n",
    "\n",
    "merged_df['hour'] = merged_df['date_package'].dt.hour\n",
    "merged_df['day_of_week'] = merged_df['date_package'].dt.dayofweek\n",
    "merged_df['is_weekend'] = merged_df['day_of_week'].isin([4, 5]).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "\n",
    "### Create a binary feature indicating whether a package exceeded the 15-day processing threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['delay_flag'] = (merged_df['processing_duration_days'] > 15).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "\n",
    "### Normalize the processing delay by the number of establishments the package passed through. This accounts for packages that take longer routes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['delay_per_etab'] = (\n",
    "    merged_df['processing_duration_days'] /\n",
    "    (merged_df['num_etablissements_package'] + 1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "\n",
    "### Create a route identifier by combining the current and next etablissment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['pkg_route_step'] = (\n",
    "    merged_df['etablissement_postal_package'] + '→' + merged_df['next_etablissement_postal_package']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "\n",
    "### Calculate how common each package route is in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg_route_freq = merged_df['pkg_route_step'].value_counts(normalize=True)\n",
    "merged_df['pkg_route_freq'] = merged_df['pkg_route_step'].map(pkg_route_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "\n",
    "### Calculate how frequently each postal establishment appears as the current or next location in the routing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "etab_freq = pd.concat([\n",
    "    merged_df['etablissement_postal_package'],\n",
    "    merged_df['next_etablissement_postal_package']\n",
    "]).value_counts()\n",
    "\n",
    "merged_df['current_etab_freq'] = merged_df['etablissement_postal_package'].map(etab_freq)\n",
    "merged_df['next_etab_freq'] = merged_df['next_etablissement_postal_package'].map(etab_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Package-Level Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg_features = [\n",
    "    'processing_duration_days',\n",
    "    'delay_flag',\n",
    "    'delay_per_etab',\n",
    "    'num_etablissements_package',\n",
    "    'pkg_route_freq',\n",
    "    'current_etab_freq',\n",
    "    'next_etab_freq',\n",
    "    'hour',\n",
    "    'is_weekend'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Receptacle Level\n",
    "\n",
    "### receptacle route identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['rec_route_step'] = (\n",
    "    merged_df['etablissement_postal_receptacle'] + '→' + merged_df['next_etablissement_postal_receptacle']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Aggregate package-level statistics at the receptacle level to create features such as:\n",
    "- Number of packages in receptacle\n",
    "- Average and standard deviation of processing duration\n",
    "- Average delay metrics\n",
    "- Average package route rarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacle_route_stats = (\n",
    "    merged_df\n",
    "    .groupby('RECPTCL_FID')\n",
    "    .agg(\n",
    "        rec_route=('rec_route_step', 'first'),\n",
    "        num_packages=('MAILITM_FID', 'count'),\n",
    "        avg_processing_days=('processing_duration_days', 'mean'),\n",
    "        std_processing_days=('processing_duration_days', 'std'),\n",
    "        avg_delay_per_etab=('delay_per_etab', 'mean'),\n",
    "        avg_pkg_route_rarity=('pkg_route_freq', 'mean')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Fill NaNs\n",
    "receptacle_route_stats['std_processing_days'] = receptacle_route_stats['std_processing_days'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Receptacle Route Frequency\n",
    "\n",
    "Calculate how common each receptacle route is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_route_freq = receptacle_route_stats['rec_route'].value_counts(normalize=True)\n",
    "receptacle_route_stats['rec_route_freq'] = receptacle_route_stats['rec_route'].map(rec_route_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Receptacle Flow Type Frequency\n",
    "\n",
    "Map flow type frequencies to each receptacle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_flow_type = merged_df.groupby('RECPTCL_FID')['flow_type_receptacle'].first().reset_index()\n",
    "\n",
    "flow_type_freq = rec_flow_type['flow_type_receptacle'].value_counts(normalize=True)\n",
    "rec_flow_type['flow_type_freq'] = rec_flow_type['flow_type_receptacle'].map(flow_type_freq)\n",
    "\n",
    "receptacle_route_stats = receptacle_route_stats.merge(\n",
    "    rec_flow_type[['RECPTCL_FID', 'flow_type_freq']],\n",
    "    on='RECPTCL_FID',\n",
    "    how='left'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Receptacle Level Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_features = [\n",
    "    'num_packages',\n",
    "    'avg_processing_days',\n",
    "    'std_processing_days',\n",
    "    'avg_delay_per_etab',\n",
    "    'avg_pkg_route_rarity',\n",
    "    'rec_route_freq',\n",
    "    'flow_type_freq'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# Check data sizes\n",
    "n_packages = len(merged_df)\n",
    "n_receptacles = len(receptacle_route_stats)\n",
    "\n",
    "print(f\"Package records: {n_packages}\")\n",
    "print(f\"Receptacle records: {n_receptacles}\")\n",
    "\n",
    "# Calculate recommended max k using heuristic: k <= sqrt(n/2)\n",
    "max_k_pkg = int(math.sqrt(n_packages / 2))\n",
    "max_k_rec = int(math.sqrt(n_receptacles / 2))\n",
    "\n",
    "print(f\"\\nMax recommended k for packages: {max_k_pkg}\")\n",
    "print(f\"Max recommended k for receptacles: {max_k_rec}\")\n",
    "\n",
    "# Set K_range based on calculated maximums\n",
    "K_range = range(2, min(max(max_k_pkg, max_k_rec) + 1, 15))\n",
    "\n",
    "print(f\"\\nK_range set to: {list(K_range)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in pck_features find the ones with Nan\n",
    "nan_pkg_features = merged_df[pkg_features].isna().sum()\n",
    "nan_pkg_features = nan_pkg_features[nan_pkg_features > 0]\n",
    "print(\"Package features with NaN values:\")\n",
    "print(nan_pkg_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Prepare data for clustering\n",
    "scaler_pkg = StandardScaler()\n",
    "X_pkg_scaled = scaler_pkg.fit_transform(merged_df[pkg_features])\n",
    "\n",
    "scaler_rec = StandardScaler()\n",
    "X_rec_scaled = scaler_rec.fit_transform(receptacle_route_stats[rec_features])\n",
    "\n",
    "# Evaluate different k values for packages\n",
    "silhouette_scores_pkg = []\n",
    "\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_pkg_scaled)\n",
    "    silhouette_scores_pkg.append(silhouette_score(X_pkg_scaled, labels))\n",
    "\n",
    "# Evaluate different k values for receptacles\n",
    "silhouette_scores_rec = []\n",
    "\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_rec_scaled)\n",
    "    silhouette_scores_rec.append(silhouette_score(X_rec_scaled, labels))\n",
    "\n",
    "\n",
    "# Find optimal k values\n",
    "best_k_pkg = list(K_range)[np.argmax(silhouette_scores_pkg)]\n",
    "best_k_rec = list(K_range)[np.argmax(silhouette_scores_rec)]\n",
    "\n",
    "print(f\"\\nOptimal k for packages (Silhouette): {best_k_pkg}\")\n",
    "print(f\"Optimal k for receptacles (Silhouette): {best_k_rec}\")\n",
    "\n",
    "# Visualization - Elbow Method and Silhouette Scores\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
    "\n",
    "\n",
    "# Package Level - Silhouette\n",
    "axes[0].plot(list(K_range), silhouette_scores_pkg, 'ro-', linewidth=2, markersize=8)\n",
    "axes[0].axvline(x=best_k_pkg, color='green', linestyle='--', linewidth=2, label=f'Best k={best_k_pkg}')\n",
    "axes[0].set_xlabel('Number of Clusters (k)', fontsize=11)\n",
    "axes[0].set_ylabel('Silhouette Score', fontsize=11)\n",
    "axes[0].set_title('Silhouette Analysis - Package Level', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "\n",
    "# Receptacle Level - Silhouette\n",
    "axes[1].plot(list(K_range), silhouette_scores_rec, 'ro-', linewidth=2, markersize=8)\n",
    "axes[1].axvline(x=best_k_rec, color='green', linestyle='--', linewidth=2, label=f'Best k={best_k_rec}')\n",
    "axes[1].set_xlabel('Number of Clusters (k)', fontsize=11)\n",
    "axes[1].set_ylabel('Silhouette Score', fontsize=11)\n",
    "axes[1].set_title('Silhouette Analysis - Receptacle Level', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "summary_df = pd.DataFrame({\n",
    "    'k': list(K_range),\n",
    "    'Silhouette_Pkg': [round(s, 4) for s in silhouette_scores_pkg],\n",
    "\n",
    "    'Silhouette_Rec': [round(s, 4) for s in silhouette_scores_rec],\n",
    " })\n",
    "\n",
    "print(\"\\nCluster Quality Metrics Comparison:\")\n",
    "print(summary_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PCA for 2D visualization\n",
    "pca_pkg = PCA(n_components=2)\n",
    "pkg_2d = pca_pkg.fit_transform(X_pkg_scaled)\n",
    "\n",
    "pca_rec = PCA(n_components=2)\n",
    "rec_2d = pca_rec.fit_transform(X_rec_scaled)\n",
    "\n",
    "print(f\"Package PCA Explained Variance Ratio: {pca_pkg.explained_variance_ratio_}\")\n",
    "print(f\"Cumulative Variance: {sum(pca_pkg.explained_variance_ratio_):.2%}\")\n",
    "\n",
    "print(f\"\\nReceptacle PCA Explained Variance Ratio: {pca_rec.explained_variance_ratio_}\")\n",
    "print(f\"Cumulative Variance: {sum(pca_rec.explained_variance_ratio_):.2%}\")\n",
    "\n",
    "# Perform KMeans clustering on packages\n",
    "optimal_k_pkg = best_k_pkg\n",
    "kmeans_pkg = KMeans(n_clusters=optimal_k_pkg, random_state=42, n_init=10)\n",
    "merged_df['kmeans_cluster'] = kmeans_pkg.fit_predict(X_pkg_scaled)\n",
    "\n",
    "# Perform DBSCAN clustering on packages\n",
    "dbscan_pkg = DBSCAN(eps=0.5, min_samples=5)\n",
    "merged_df['dbscan_cluster'] = dbscan_pkg.fit_predict(X_pkg_scaled)\n",
    "\n",
    "# Perform KMeans clustering on receptacles\n",
    "optimal_k_rec = best_k_rec\n",
    "kmeans_rec = KMeans(n_clusters=optimal_k_rec, random_state=42, n_init=10)\n",
    "receptacle_route_stats['kmeans_cluster'] = kmeans_rec.fit_predict(X_rec_scaled)\n",
    "\n",
    "# Perform DBSCAN clustering on receptacles\n",
    "dbscan_rec = DBSCAN(eps=0.5, min_samples=5)\n",
    "receptacle_route_stats['dbscan_cluster'] = dbscan_rec.fit_predict(X_rec_scaled)\n",
    "\n",
    "# Visualize Package Clustering\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# KMeans package clustering\n",
    "scatter1 = axes[0].scatter(pkg_2d[:, 0], pkg_2d[:, 1], c=merged_df['kmeans_cluster'], \n",
    "                           cmap='viridis', s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "axes[0].set_xlabel(f'PC1 ({pca_pkg.explained_variance_ratio_[0]:.1%})', fontsize=11)\n",
    "axes[0].set_ylabel(f'PC2 ({pca_pkg.explained_variance_ratio_[1]:.1%})', fontsize=11)\n",
    "axes[0].set_title('Package Clustering - KMeans', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "\n",
    "# DBSCAN package clustering\n",
    "scatter2 = axes[1].scatter(pkg_2d[:, 0], pkg_2d[:, 1], c=merged_df['dbscan_cluster'], \n",
    "                           cmap='plasma', s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "axes[1].set_xlabel(f'PC1 ({pca_pkg.explained_variance_ratio_[0]:.1%})', fontsize=11)\n",
    "axes[1].set_ylabel(f'PC2 ({pca_pkg.explained_variance_ratio_[1]:.1%})', fontsize=11)\n",
    "axes[1].set_title('Package Clustering - DBSCAN', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize Receptacle Clustering\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# KMeans receptacle clustering\n",
    "scatter3 = axes[0].scatter(rec_2d[:, 0], rec_2d[:, 1], c=receptacle_route_stats['kmeans_cluster'], \n",
    "                           cmap='viridis', s=100, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "axes[0].set_xlabel(f'PC1 ({pca_rec.explained_variance_ratio_[0]:.1%})', fontsize=11)\n",
    "axes[0].set_ylabel(f'PC2 ({pca_rec.explained_variance_ratio_[1]:.1%})', fontsize=11)\n",
    "axes[0].set_title('Receptacle Clustering - KMeans', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter3, ax=axes[0], label='Cluster')\n",
    "\n",
    "# DBSCAN receptacle clustering\n",
    "scatter4 = axes[1].scatter(rec_2d[:, 0], rec_2d[:, 1], c=receptacle_route_stats['dbscan_cluster'], \n",
    "                           cmap='plasma', s=100, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "axes[1].set_xlabel(f'PC1 ({pca_rec.explained_variance_ratio_[0]:.1%})', fontsize=11)\n",
    "axes[1].set_ylabel(f'PC2 ({pca_rec.explained_variance_ratio_[1]:.1%})', fontsize=11)\n",
    "axes[1].set_title('Receptacle Clustering - DBSCAN', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter4, ax=axes[1], label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## Clustering Results Comparison\n",
    "\n",
    "Compare KMeans and DBSCAN clustering results using 2D PCA visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN clustering for packages\n",
    "# eps and min_samples need to be tuned based on your data characteristics\n",
    "dbscan_pkg = DBSCAN(eps=0.5, min_samples=10)\n",
    "merged_df['dbscan_cluster'] = dbscan_pkg.fit_predict(X_pkg_scaled)\n",
    "\n",
    "# DBSCAN clustering for receptacles\n",
    "dbscan_rec = DBSCAN(eps=0.5, min_samples=5)\n",
    "receptacle_route_stats['dbscan_cluster'] = dbscan_rec.fit_predict(X_rec_scaled)\n",
    "\n",
    "print(f\"DBSCAN clustering completed!\")\n",
    "print(f\"\\nPackage DBSCAN clusters distribution:\")\n",
    "dbscan_pkg_dist = merged_df['dbscan_cluster'].value_counts().sort_index()\n",
    "print(dbscan_pkg_dist)\n",
    "n_clusters_pkg = len(dbscan_pkg_dist) - (1 if -1 in merged_df['dbscan_cluster'].values else 0)\n",
    "n_noise_pkg = (merged_df['dbscan_cluster'] == -1).sum()\n",
    "print(f\"Number of clusters: {n_clusters_pkg}, Noise points: {n_noise_pkg}\")\n",
    "\n",
    "print(f\"\\nReceptacle DBSCAN clusters distribution:\")\n",
    "dbscan_rec_dist = receptacle_route_stats['dbscan_cluster'].value_counts().sort_index()\n",
    "print(dbscan_rec_dist)\n",
    "n_clusters_rec = len(dbscan_rec_dist) - (1 if -1 in receptacle_route_stats['dbscan_cluster'].values else 0)\n",
    "n_noise_rec = (receptacle_route_stats['dbscan_cluster'] == -1).sum()\n",
    "print(f\"Number of clusters: {n_clusters_rec}, Noise points: {n_noise_rec}\")\n",
    "\n",
    "# Visualize DBSCAN results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "merged_df['dbscan_cluster'].value_counts().sort_index().plot(kind='bar', ax=axes[0], color='lightgreen', edgecolor='black')\n",
    "axes[0].set_title('DBSCAN Cluster Distribution - Packages\\n(Cluster -1 = Noise)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Cluster')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "receptacle_route_stats['dbscan_cluster'].value_counts().sort_index().plot(kind='bar', ax=axes[1], color='lightyellow', edgecolor='black')\n",
    "axes[1].set_title('DBSCAN Cluster Distribution - Receptacles\\n(Cluster -1 = Noise)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Cluster')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "# anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to do"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
