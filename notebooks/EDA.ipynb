{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "make sure to rename the columns by removing é\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) for Packages and Receptacle Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "packages_df = pd.read_csv('../data/raw/packages_data_2023_2025.csv',delimiter=';')\n",
    "receptacle_df = pd.read_csv('../data/raw/receptacle_data_2023_2025.csv',delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename for consistency\n",
    "receptacle_df.rename(columns={\n",
    "    'EVENT_TYPECD': 'EVENT_TYPE_CD',\n",
    "    'nextetablissement_postal': 'next_etablissement_postal'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df.shape, receptacle_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dates to dateTime for compatibility\n",
    "packages_df['date'] = pd.to_datetime(packages_df['date'])\n",
    "receptacle_df['date'] = pd.to_datetime(receptacle_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacle_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacle_df.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    " Initial Observations\n",
    "- both datasets cover the period from 2023 to 2025\n",
    "- we have no target variable in either dataset\n",
    "- for packages dataset:\n",
    "    - 6 features in total with 5 categorical and 1 numerical\n",
    "    - MAILITM_FID is unique identifier for each package\n",
    "    - RECPTCL_FID is foreign key linking to receptacle dataset\n",
    "    - etablissement_postal and next_etablissement_postal have some null values\n",
    "- for receptacle dataset:\n",
    "    - 5 features in total with 4 categorical and 1 numerical\n",
    "    - RECPTCL_FID is unique identifier for each receptacle\n",
    "    - EVENT_TYPE_CD has some null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in packages_df.columns:\n",
    "    print(f'{column} has {packages_df[column].nunique()} unique values.')\n",
    "    print(f'{column} has {packages_df[column].isnull().sum()} null values.')\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in receptacle_df.columns:\n",
    "    print(f'{column} has {receptacle_df[column].nunique()} unique values.')\n",
    "    print(f'{column} has {receptacle_df[column].isnull().sum()} null values.')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "we notice the following:<br>\n",
    "- receptacle dataset has more unique values for RECPTCL_FID than packages dataset, indicating one-to-many relationship<br>\n",
    "- MAILITM_FID is unique in packages dataset.<br>\n",
    "- packages dataset have more unique date values than receptacle dataset.<br>\n",
    "- both datasets have null values in etablissement_postal and next_etablissement_postal columns. This requires processing later on<br>\n",
    "- packages dataset has more unique values in the next_etablissement_postal column compared to receptacle dataset but also more null values. **further investigation is needed to understand why**<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacle_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "for EVENT_TYPE_CD we notice different range of values for packages and receptacle datasets indicating different types of events.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "#### Visualizations\n",
    "for now we will visualize the distribution of EVENT_TYPE_CD in both datasets.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of EVENT_TYPE_CD in packages dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=packages_df,x='EVENT_TYPE_CD')\n",
    "plt.title('distribution of EVENT_TYPE_CD in packages dataset')\n",
    "plt.xlabel('EVENT_TYPE_CD')\n",
    "plt.ylabel('count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of EVENT_TYPE_CD in receptacle dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=receptacle_df,x='EVENT_TYPE_CD')\n",
    "plt.title('distribution of EVENT_TYPE_CD in Receptacle dataset')\n",
    "plt.xlabel('EVENT_TYPE_CD')\n",
    "plt.ylabel('count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "for etablissement_postal and next_etablissement_postal I will start with visualizing the receptacle dataset since the packages dataset has a lot of unique values<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of etablissement_postal in receptacle dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=receptacle_df,x='etablissement_postal')\n",
    "plt.title('distribution of etablissement_postal in Receptacle dataset')\n",
    "plt.xlabel('etablissement_postal')\n",
    "plt.ylabel('count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of next_etablissement_postal in receptacle dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=receptacle_df,x='next_etablissement_postal')\n",
    "plt.title('distribution of next_etablissement_postal in Receptacle dataset')\n",
    "plt.xlabel('next_etablissement_postal')\n",
    "plt.ylabel('count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.  Top 10 locations to keep the map small\n",
    "top_locs = receptacle_df['etablissement_postal'].value_counts().head(10).index\n",
    "top_next = receptacle_df['next_etablissement_postal'].value_counts().head(10).index\n",
    "\n",
    "# 2. include only these top locations\n",
    "filtered_df = receptacle_df[\n",
    "    (receptacle_df['etablissement_postal'].isin(top_locs)) &\n",
    "    (receptacle_df['next_etablissement_postal'].isin(top_next))\n",
    "]\n",
    "\n",
    "# 3. Create a Matrix (Cross-tabulation)\n",
    "matrix = pd.crosstab(filtered_df['etablissement_postal'], filtered_df['next_etablissement_postal'])\n",
    "\n",
    "# 4. Plot Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(matrix, cmap='Reds', annot=True, fmt='d', linewidths=.5)\n",
    "plt.title(\"Most Frequent Routes (Origin vs Destination) - Receptacle Dataset\")\n",
    "plt.xlabel(\"Next Destination\")\n",
    "plt.ylabel(\"Current Location\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "we notice that some etablissements have significantly higher traffic compared to others, indicating  major distribution centers.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "for etablissement_postal and next_etablissement_postal we will create a heatmap to visualize the flow between current location and next destination.<br>\n",
    "Count of parcels moving from A to B to see the density of connections between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.  Top 10 locations to keep the map small\n",
    "top_locs = packages_df['etablissement_postal'].value_counts().head(10).index\n",
    "top_next = packages_df['next_etablissement_postal'].value_counts().head(10).index\n",
    "\n",
    "# 2. include only these top locations\n",
    "filtered_df = packages_df[\n",
    "    (packages_df['etablissement_postal'].isin(top_locs)) &\n",
    "    (packages_df['next_etablissement_postal'].isin(top_next))\n",
    "]\n",
    "\n",
    "# 3. Create a Matrix (Cross-tabulation)\n",
    "matrix = pd.crosstab(filtered_df['etablissement_postal'], filtered_df['next_etablissement_postal'])\n",
    "\n",
    "# 4. Plot Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(matrix, cmap='Reds', annot=True, fmt='d', linewidths=.5)\n",
    "plt.title(\"Most Frequent Routes (Origin vs Destination) - Packages Dataset\")\n",
    "plt.xlabel(\"Next Destination\")\n",
    "plt.ylabel(\"Current Location\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count packages per location\n",
    "location_counts = packages_df['etablissement_postal'].value_counts().reset_index()\n",
    "location_counts.columns = ['Location', 'Volume']\n",
    "\n",
    "# keep only top 20 busiest centers\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Volume', y='Location', data=location_counts.head(20), palette='viridis')\n",
    "plt.title(\"Top 20 Busiest Postal Centers\")\n",
    "plt.xlabel(\"Number of Packages\")\n",
    "plt.ylabel(\"Center ID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "we notice the same pattern as before with some etablissements having significantly higher traffic compared to others.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Extract Time Features\n",
    "packages_df['hour'] = packages_df['date'].dt.hour\n",
    "packages_df['day_of_week'] = packages_df['date'].dt.day_name()\n",
    "\n",
    "# 2. Create a Pivot Table (Cross-tabulation)\n",
    "# Rows = Day, Cols = Hour, Values = Count of Scans\n",
    "heatmap_data = pd.crosstab(\n",
    "    packages_df['day_of_week'],\n",
    "    packages_df['hour']\n",
    ")\n",
    "\n",
    "days_order = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "heatmap_data = heatmap_data.reindex(days_order)\n",
    "\n",
    "# 3. Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(heatmap_data, cmap='YlOrRd', linewidths=.5, annot=False)\n",
    "plt.title(\"Package Scan Activity by Day and Hour\")\n",
    "plt.xlabel(\"Hour of Day (0-23)\")\n",
    "plt.ylabel(\"Day of Week\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "we notice that the busiest times for package scans are during weekdays, particularly from mid-morning to late afternoon.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets on RECPTCL_FID to analyze relationship\n",
    "merged_df = pd.merge(\n",
    "    packages_df,\n",
    "    receptacle_df,\n",
    "    on='RECPTCL_FID',\n",
    "    how='left',\n",
    "    suffixes=('_package', '_receptacle')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of packages per receptacle\n",
    "packages_per_receptacle = packages_df.groupby('RECPTCL_FID')['MAILITM_FID'].nunique()\n",
    "packages_per_receptacle.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "# Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Use raw\n",
    "# packages_df = pd.read_csv('../data/raw/packages_data_2023_2025.csv',delimiter=';')\n",
    "# receptacle_df = pd.read_csv('../data/raw/receptacle_data_2023_2025.csv',delimiter=';')\n",
    "\n",
    "#this is to use the preprocessed one\n",
    "packages_df = pd.read_csv('../data/interim/clean_packages_df.csv',delimiter=',')\n",
    "receptacle_df = pd.read_csv('../data/interim/clean_receptacles_df.csv',delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### stripe white spaces from IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "packages_df['RECPTCL_FID'] = packages_df['RECPTCL_FID'].str.strip()\n",
    "packages_df['MAILITM_FID'] = packages_df['MAILITM_FID'].str.strip()\n",
    "receptacle_df['RECPTCL_FID'] = receptacle_df['RECPTCL_FID'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Check RECPTCL_FID and MAILITM_FID having same length formats\n",
    "if yes then we can split them into meaningfull parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "same=1\n",
    "print(\"\\n=== RECPTCL_FID  ===\")\n",
    "print(f\"testing if the lengths of RECPTCL_FID values are all the same:\")\n",
    "for val in packages_df['RECPTCL_FID'].values:\n",
    "    if len(str(val)) != 29 :\n",
    "        print(f\"  {val} (length: {len(str(val))})\")\n",
    "        same=0\n",
    "        break\n",
    "if same==1:\n",
    "    print('all same length' )\n",
    "\n",
    "print(\"\\n=== MAILITM_FID  ===\")\n",
    "print(f\"testing if the lengths of MAILITM_FID values are all the same:\")\n",
    "for val in packages_df['MAILITM_FID'].values:\n",
    "    if len(str(val)) != 13 :\n",
    "        print(f\"  {val} (length: {len(str(val))})\")\n",
    "        same=0\n",
    "        break\n",
    "if same==1:\n",
    "    print ('all same length' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### RECPTCL_FID Analysis\n",
    "- **Format:** 29-character string (e.g., `USORDADZALGDAUN30050001900005`)\n",
    "- **Data Quality:** No null values (1,000,000) | 215,867 unique values in receptacle dataset and 45306 unique values in packages dataset\n",
    "- **Extractable Features:**\n",
    "  - Origin Country (2 chars): US, FR, AE, etc.\n",
    "  - Destination Country (2 chars): DZ, AI, AA, etc.\n",
    "\n",
    "### MAILITM_FID Analysis\n",
    "- **Format:** 13-character string according to the S10-12 patern (e.g., `CA000132868US`, `CA000340856PK`)\n",
    "- **Data Quality:** No null values (1,000,000 packages)\n",
    "- **Extractable Features:**\n",
    "  - Service Indicator (2 chars): CA, etc.\n",
    "  - Serial Number (8 chars): 00013286, 00034085, etc.\n",
    "  - Check Digit (1 char): 8, 6, etc.\n",
    "  - Country Code (3 chars, right-stripped): US, PK.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## Definition of the parser funtions\n",
    "These functions are responsible for spliting the IDs into parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_recptcl_fid(id_str):\n",
    "    origin_country = id_str[0:2]\n",
    "    destination_country = id_str[6:8]\n",
    "    return origin_country, destination_country\n",
    "\n",
    "def parse_mailitm_fid(id_str):\n",
    "    service_indicator = id_str[0:2]\n",
    "    serial_number = id_str[2:11]\n",
    "    country_code = id_str[11:14].strip()\n",
    "    return service_indicator, serial_number, country_code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### Apply parsing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_receptacle_df = receptacle_df.copy()\n",
    "# parsed_receptacle_df[['origin_country', 'destination_country']] = parsed_receptacle_df['RECPTCL_FID'].apply(lambda x: pd.Series(parse_recptcl_fid(x)))\n",
    "\n",
    "# Faster method:\n",
    "parsed_data = list(receptacle_df['RECPTCL_FID'].apply(parse_recptcl_fid))\n",
    "\n",
    "# Assign to new columns by creating a temporary DataFrame\n",
    "parsed_receptacle_df = receptacle_df.copy()\n",
    "parsed_receptacle_df[['origin_country', 'destination_country']] = pd.DataFrame(\n",
    "    parsed_data, index=receptacle_df.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_packages_df = packages_df.copy()\n",
    "# parsed_packages_df[['service_indicator', 'serial_number', 'country_code']] = parsed_packages_df['MAILITM_FID'].apply(lambda x: pd.Series(parse_mailitm_fid(x)))\n",
    "# parsed_packages_df[['origin_country','destination_country']] = parsed_packages_df['RECPTCL_FID'].apply(lambda x: pd.Series(parse_recptcl_fid(x)))\n",
    "\n",
    "# Faster method:\n",
    "parsed_packages_df = packages_df.copy()\n",
    "\n",
    "# 1. Optimize MAILITM_FID parsing\n",
    "mailitm_data = list(parsed_packages_df['MAILITM_FID'].apply(parse_mailitm_fid))\n",
    "parsed_packages_df[['service_indicator', 'serial_number', 'country_code']] = pd.DataFrame(\n",
    "    mailitm_data, index=parsed_packages_df.index\n",
    ")\n",
    "\n",
    "# 2. Optimize RECPTCL_FID parsing\n",
    "recptcl_data = list(parsed_packages_df['RECPTCL_FID'].apply(parse_recptcl_fid))\n",
    "parsed_packages_df[['origin_country', 'destination_country']] = pd.DataFrame(\n",
    "    recptcl_data, index=parsed_packages_df.index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### show samples of new parsed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=== packages_df sample with new parsed columns ===\")\n",
    "parsed_packages_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== receptacle_df sample with new parsed columns ===\")\n",
    "parsed_receptacle_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "# Analysis of the extrcted features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "## 1. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n--- Unique Value Counts for parsed_packages_df ---\")\n",
    "print(\"\\nFor receptacle FID parsing:\")\n",
    "print(f\"Unique origin_country values: {parsed_packages_df['origin_country'].nunique()}\")\n",
    "print(f\"Unique destination_country values: {parsed_packages_df['destination_country'].nunique()}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"\\nFor mail item FID parsing:\")\n",
    "print(f\"Unique service_indicator values: {parsed_packages_df['service_indicator'].nunique()}\")\n",
    "print(f\"Unique country_code values: {parsed_packages_df['country_code'].nunique()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "## 2. parsed_receptacle_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Unique Value Counts for parsed_receptacle_df ---\")\n",
    "print(f\"Unique origin_country values: {parsed_receptacle_df['origin_country'].nunique()}\")\n",
    "print(f\"Unique destination_country values: {parsed_receptacle_df['destination_country'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "## List values of the new columns obtained from receptacle FID parsing for both parsed dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### 1. for parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#listing the values \n",
    "print(\"\\n--- Values of origin_country ---\")\n",
    "print(parsed_packages_df['origin_country'].unique())\n",
    "\n",
    "print(\"\\n--- Values of destination_country ---\")\n",
    "print(parsed_packages_df['destination_country'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "### 2. for parsed_receptacle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#listing the values \n",
    "print(\"\\n--- Values of origin_country ---\")\n",
    "print(parsed_receptacle_df['origin_country'].unique())\n",
    "\n",
    "print(\"\\n--- Values of destination_country ---\")\n",
    "print(parsed_receptacle_df['destination_country'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "### Do the intersection of origin_country of both dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the intersection of origin_country values in both parsed datasets\n",
    "packages_origin_countries = set(parsed_packages_df['origin_country'].unique())\n",
    "receptacle_origin_countries = set(parsed_receptacle_df['origin_country'].unique())\n",
    "common_origin_countries = packages_origin_countries.intersection(receptacle_origin_countries)\n",
    "print(\"number of common origin_country values in both parsed datasets:\", len(common_origin_countries))\n",
    "print(f\"\\nCommon origin_country values in both paesed datasets: \")\n",
    "print(common_origin_countries)\n",
    "# remaining ones \n",
    "remaining_in_packages = packages_origin_countries - common_origin_countries\n",
    "remaining_in_receptacle = receptacle_origin_countries - common_origin_countries\n",
    "print(f\"Remaining origin_country values only in parsed_packages_df:\")\n",
    "print(remaining_in_packages)\n",
    "print(f\"Remaining origin_country values only in parsed_receptacle_df:\")\n",
    "print(remaining_in_receptacle )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "### list the values of both service indicators and country code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "### 1. service indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Values of service_indicator ---\")\n",
    "print(parsed_packages_df['service_indicator'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    " we can see that there are values that don't follow the standards in the S10-12 format so we need to handle that correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform country_code to uppercase for consistency\n",
    "parsed_packages_df['service_indicator'] = parsed_packages_df['service_indicator'].str.upper()\n",
    "print(\"values of service_indicator after transformation to uppercase:\")\n",
    "print(parsed_packages_df['service_indicator'].unique())\n",
    "print(\"number of unique service indicators after transformation:\", parsed_packages_df['service_indicator'].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "### 2. country code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Values of country codes ---\")\n",
    "\n",
    "print(parsed_packages_df['country_code'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "we can see that many values for the country codes are numbers instead of ISO 3166-1 format these values should be replaced by the values of origin country gotten from the receptacle when doing the preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "### replace them with the correct origin country code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Vectorized string capitalization\n",
    "parsed_packages_df['country_code'] = parsed_packages_df['country_code'].str.upper()\n",
    "\n",
    "# 2. Vectorized comparison to find mismatches\n",
    "mismatch_mask = parsed_packages_df['origin_country'] != parsed_packages_df['country_code']\n",
    "\n",
    "# 3. Count the Trues\n",
    "count = mismatch_mask.sum()\n",
    "\n",
    "print(f\"Number of rows where origin_country does not match country_code: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "### replace them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .loc to find rows where they don't match, and update only the 'country_code' column\n",
    "parsed_packages_df.loc[parsed_packages_df['origin_country'] != parsed_packages_df['country_code'], 'country_code'] = parsed_packages_df['origin_country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the unique values again\n",
    "print(\"\\n--- Values of country codes after correction ---\")\n",
    "print(parsed_packages_df['country_code'].unique())\n",
    "print(\"number of unique country codes after correction:\", parsed_packages_df['country_code'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "## visualization of Origin Country distribution according to number of packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "### 1. for the parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_country_counts = parsed_packages_df['origin_country'].value_counts().head(20)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.barh(origin_country_counts.index, origin_country_counts.values, color='steelblue')\n",
    "plt.xlabel('Count', fontsize=11)\n",
    "plt.ylabel('Origin Country', fontsize=11)\n",
    "plt.title('Top 20 Origin Countries by packages count', fontsize=12, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "### 2. for the parsed_receptacle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_country_counts = parsed_receptacle_df['origin_country'].value_counts().head(20)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.barh(origin_country_counts.index, origin_country_counts.values, color='steelblue')\n",
    "plt.xlabel('Count', fontsize=11)\n",
    "plt.ylabel('Origin Country', fontsize=11)\n",
    "plt.title('Top 20 Origin Countries by receptacle count', fontsize=12, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "## Visualiation of the service indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_indicator_count = parsed_packages_df['service_indicator'].value_counts().head(20)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.barh(service_indicator_count.index, service_indicator_count.values, color='mediumseagreen')\n",
    "plt.xlabel('Count', fontsize=11)\n",
    "plt.ylabel('Service Indicator', fontsize=11)\n",
    "plt.title('Top 20 Service Indicators by packages count', fontsize=12, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "# Origin–Destination Flow Analysis\n",
    "\n",
    "This section investigates the flow of receptacles and packages from origin countries to destination. We examine:\n",
    "- packages count by origin country\n",
    "- Top origin countries delivering to each destination\n",
    "- Visual representation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "### 1. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages count by origin country\n",
    "origin_country_volume = parsed_packages_df['origin_country'].value_counts()\n",
    "print(\"\\n--- Packages count by Origin Country ---\")\n",
    "print(origin_country_volume.head(15))\n",
    "\n",
    "# Create origin_country × destination matrix\n",
    "flow_matrix = pd.crosstab(parsed_packages_df['origin_country'], \n",
    "                           parsed_packages_df['destination_country'])\n",
    "\n",
    "# Keep only top 10 origin countries and top 10 destination countries for readability\n",
    "top_origins = parsed_packages_df['origin_country'].value_counts().head(10).index\n",
    "top_arrivals = parsed_packages_df['destination_country'].value_counts().head(10).index\n",
    "\n",
    "flow_matrix_top = flow_matrix.loc[top_origins, top_arrivals]\n",
    "\n",
    "print(\"\\n--- Origin Country × destination Country (Top 10 × Top 10) ---\")\n",
    "print(flow_matrix_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize origin × destination country flow\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(flow_matrix_top, cmap='YlOrRd', annot=True, fmt='d', \n",
    "            cbar_kws={'label': 'packages Count'}, linewidths=0.5)\n",
    "plt.title('packages Flow: Origin Country × destination country (Top 10 × Top 10)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Arrival Hub', fontsize=12)\n",
    "plt.ylabel('Origin Country', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "### 2. parsed_receptacle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# receptacle count by origin country\n",
    "origin_country_receptacle_volume = parsed_receptacle_df['origin_country'].value_counts()\n",
    "print(\"\\n--- Receptacle count by Origin Country ---\")\n",
    "print(origin_country_receptacle_volume.head(15))\n",
    "\n",
    "destination__receptacle_volume=parsed_receptacle_df['destination_country'].value_counts()\n",
    "print(\"\\n--- Receptacle count by destination ---\")\n",
    "print(destination__receptacle_volume.head(15))\n",
    "\n",
    "# Create origin_country × destination matrix\n",
    "receptacle_flow_matrix = pd.crosstab(parsed_receptacle_df['origin_country'], \n",
    "                           parsed_receptacle_df['destination_country'])\n",
    "\n",
    "# Keep only top 10 origin countries and top 10 destination countries for readability\n",
    "receptacle_top_origins = parsed_receptacle_df['origin_country'].value_counts().head(10).index\n",
    "receptacle_top_arrivals = parsed_receptacle_df['destination_country'].value_counts().head(10).index\n",
    "\n",
    "receptacle_flow_matrix_top = receptacle_flow_matrix.loc[receptacle_top_origins, receptacle_top_arrivals]\n",
    "\n",
    "print(\"\\n--- Origin Country × destination Country (Top 10 × Top 10) ---\")\n",
    "print(receptacle_flow_matrix_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize origin × destination country flow\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(receptacle_flow_matrix_top, cmap='YlOrRd', annot=True, fmt='d', \n",
    "            cbar_kws={'label': 'receptacles Count'}, linewidths=0.5)\n",
    "plt.title('receptacles Flow: Origin Country × destination country (Top 10 × Top 10)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Arrival Hub', fontsize=12)\n",
    "plt.ylabel('Origin Country', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "### create pairs (origin, destination) for more detailed analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_packages_df['origin_destination'] = parsed_packages_df['origin_country'] + '_' + parsed_packages_df['destination_country']\n",
    "parsed_receptacle_df['origin_destination'] = parsed_receptacle_df['origin_country'] + '_' + parsed_receptacle_df['destination_country']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "listing the obtained values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('(origin_destination) pairs obtained for ')\n",
    "print(\"\\nfor parsed_packages_df :\")\n",
    "print(\"\\nNumber of unique values\", parsed_packages_df['origin_destination'].nunique())\n",
    "print(\"\\nValues : \")\n",
    "print(parsed_packages_df['origin_destination'].unique())\n",
    "print(\"\\nfor parsed_receptacles_df :\")\n",
    "print(\"\\nNumber of unique values\", parsed_receptacle_df['origin_destination'].nunique())\n",
    "print(\"\\nValues : \")\n",
    "print(parsed_receptacle_df['origin_destination'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "### visualization of obtained results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "### 1. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_dest_counts = parsed_packages_df['origin_destination'].value_counts()\n",
    "top_origin_dest_counts= origin_dest_counts.head(15)\n",
    "plt.figure(figsize=(14, 7))\n",
    "top_origin_dest_counts.plot(kind='bar')\n",
    "plt.title('Histogram of (origin, destination) Pairs Based on Package Counts')\n",
    "plt.xlabel('Origin_Destination')\n",
    "plt.ylabel('Number of Packages')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "\n",
    "### 2. parsed_receptacle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_dest_counts = parsed_receptacle_df['origin_destination'].value_counts()\n",
    "top_origin_dest_counts = origin_dest_counts.head(15)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "top_origin_dest_counts.plot(kind='bar')\n",
    "plt.title('Histogram of (origin, destination) Pairs Based on receptacle Counts')\n",
    "plt.xlabel('Origin_Destination')\n",
    "plt.ylabel('Number of receptacles')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "## origin_destination X etablissments analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {},
   "source": [
    "## 1. current etablissment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "### a. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_packages_df: visualize the histogram of counts by (origin_destination, etablissement_postal) pairs\n",
    "\n",
    "# First, select top N most common (origin_destination, etablissement_postal) pairs for clarity\n",
    "pair_counts_current = parsed_packages_df.groupby(['origin_destination', 'etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_current = pair_counts_current.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_current,\n",
    "    x='count',\n",
    "    y=top_pairs_current.apply(lambda x: f\"{x['origin_destination']} | {x['etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin-Destination, Current Etablissement) Pairs by Package Counts')\n",
    "plt.xlabel('Number of Packages')\n",
    "plt.ylabel('(Origin_Destination | Current Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "### b. parsed_receptacle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_receptacle_df: visualize the histogram of counts by (origin_destination, etablissement_postal) pairs\n",
    "\n",
    "# First, select top N most common (origin_destination, etablissement_postal) pairs for clarity\n",
    "pair_counts_current_receptacle = parsed_receptacle_df.groupby(['origin_destination', 'etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_current_receptacle = pair_counts_current_receptacle.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_current_receptacle,\n",
    "    x='count',\n",
    "    y=top_pairs_current_receptacle.apply(lambda x: f\"{x['origin_destination']} | {x['etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin-Destination, Current Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Origin_Destination | Current Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104",
   "metadata": {},
   "source": [
    "we can see that the ETAB0002 is dominating and we remark that when the destination is DZ\n",
    "we'll try to confirm that by taking into consideration the destination only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_receptacle_df: visualize the histogram of counts by (destination_country, etablissement_postal) pairs\n",
    "\n",
    "# First, select top N most common (destination_country, etablissement_postal) pairs for clarity\n",
    "pair_counts_dest_receptacle = parsed_receptacle_df.groupby(['destination_country', 'etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_dest_receptacle = pair_counts_dest_receptacle.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_dest_receptacle,\n",
    "    x='count',\n",
    "    y=top_pairs_dest_receptacle.apply(lambda x: f\"{x['destination_country']} | {x['etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Destination Country, Current Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Destination Country | Current Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "### This is to test the origin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_receptacle_df: visualize the histogram of counts by (origin_country, etablissement_postal) pairs\n",
    "\n",
    "# First, select top N most common (origin_country, etablissement_postal) pairs for clarity\n",
    "pair_counts_origin_receptacle = parsed_receptacle_df.groupby(['origin_country', 'etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_origin_receptacle = pair_counts_origin_receptacle.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_origin_receptacle,\n",
    "    x='count',\n",
    "    y=top_pairs_origin_receptacle.apply(lambda x: f\"{x['origin_country']} | {x['etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin Country, Current Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Origin Country | Current Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {},
   "source": [
    "most of the values with ETAB0002 values are european countries in addition to AE and China(CN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {},
   "source": [
    "## 2. Next etablissement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110",
   "metadata": {},
   "source": [
    "### a. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_packages_df: visualize the histogram of counts by (origin_destination, next_etablissement_postal)\n",
    "\n",
    "# First, select top N most common (origin_destination, next_etablissement_postal) pairs for clarity\n",
    "pair_counts = parsed_packages_df.groupby(['origin_destination', 'next_etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs = pair_counts.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs,\n",
    "    x='count',\n",
    "    y=top_pairs.apply(lambda x: f\"{x['origin_destination']} | {x['next_etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin-Destination, Next Etablissement) Pairs by Package Counts')\n",
    "plt.xlabel('Number of Packages')\n",
    "plt.ylabel('(Origin_Destination | Next Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112",
   "metadata": {},
   "source": [
    "### b. parsed_receptacle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_receptacle_df: visualize the histogram of counts by (origin_destination, next_etablissement_postal)\n",
    "\n",
    "# First, select top N most common (origin_destination, next_etablissement_postal) pairs for clarity\n",
    "pair_counts_recept = parsed_receptacle_df.groupby(['origin_destination', 'next_etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_recept = pair_counts_recept.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_recept,\n",
    "    x='count',\n",
    "    y=top_pairs_recept.apply(lambda x: f\"{x['origin_destination']} | {x['next_etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin-Destination, Next Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Origin_Destination | Next Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114",
   "metadata": {},
   "source": [
    "Do for origin and for destination separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For parsed_receptacle_df: visualize the histogram of counts by destination_country and next_etablissement_postal\n",
    "\n",
    "# First, select top N most common (destination_country, next_etablissement_postal) pairs for clarity\n",
    "pair_counts_dest = parsed_receptacle_df.groupby(['destination_country', 'next_etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_dest = pair_counts_dest.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_dest,\n",
    "    x='count',\n",
    "    y=top_pairs_dest.apply(lambda x: f\"{x['destination_country']} | {x['next_etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Destination Country, Next Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Destination_Country | Next Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_receptacle_df: visualize the histogram of counts by origin_country and next_etablissement_postal\n",
    "\n",
    "# First, select top N most common (origin_country, next_etablissement_postal) pairs for clarity\n",
    "pair_counts_origin = parsed_receptacle_df.groupby(['origin_country', 'next_etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_origin = pair_counts_origin.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_origin,\n",
    "    x='count',\n",
    "    y=top_pairs_origin.apply(lambda x: f\"{x['origin_country']} | {x['next_etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin Country, Next Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Origin_Country | Next Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117",
   "metadata": {},
   "source": [
    "from the analysis we can see that there are some etablissments that get congested forming a sort of loop (ETAB0030, ETAB0002, ETAB0006)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118",
   "metadata": {},
   "source": [
    "### Time analysis regarding origin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {},
   "source": [
    "### 1. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the parsed_packages_df by date and origin_country to count packages per month per origin country\n",
    "parsed_packages_df['date'] = pd.to_datetime(parsed_packages_df['date'])\n",
    "\n",
    "# Group by origin_country and resample by month\n",
    "ts_by_origin = parsed_packages_df.groupby(['origin_country', pd.Grouper(key='date', freq='ME')])['MAILITM_FID'].count().reset_index()\n",
    "\n",
    "# Choose top 5 origin countries by total count for plotting\n",
    "top_origin = parsed_packages_df['origin_country'].value_counts().head(5).index\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "for origin in top_origin:\n",
    "    ts = ts_by_origin[ts_by_origin['origin_country'] == origin]\n",
    "    plt.plot(ts['date'], ts['MAILITM_FID'], marker='o', label=origin)\n",
    "\n",
    "plt.title('Monthly Package count by Origin Country')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Packages')\n",
    "plt.legend(title='Origin Country')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121",
   "metadata": {},
   "source": [
    "### 2. parsed_receptacle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the parsed_receptacle_df by date and origin_country to count receptacles per month per origin country\n",
    "parsed_receptacle_df['date'] = pd.to_datetime(parsed_receptacle_df['date'])\n",
    "\n",
    "# Group by origin_country and resample by month\n",
    "ts_by_origin_recept = parsed_receptacle_df.groupby(['origin_country', pd.Grouper(key='date', freq='ME')])['RECPTCL_FID'].count().reset_index()\n",
    "\n",
    "# Choose top 5 origin countries by total count for plotting\n",
    "top_origin_recept = parsed_receptacle_df['origin_country'].value_counts().head(5).index\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "for origin in top_origin_recept:\n",
    "    ts = ts_by_origin_recept[ts_by_origin_recept['origin_country'] == origin]\n",
    "    plt.plot(ts['date'], ts['RECPTCL_FID'], marker='o', label=origin)\n",
    "\n",
    "plt.title('Monthly Receptacle count by Origin Country')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Receptacles')\n",
    "plt.legend(title='Origin Country')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {},
   "source": [
    "### Time analysis by destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the parsed_packages_df by date and destination_country to count packages per month per destination country\n",
    "parsed_packages_df['date'] = pd.to_datetime(parsed_packages_df['date'])\n",
    "\n",
    "# Group by destination_country and resample by month\n",
    "ts_by_dest = parsed_packages_df.groupby(['destination_country', pd.Grouper(key='date', freq='ME')])['MAILITM_FID'].count().reset_index()\n",
    "\n",
    "# Choose top 5 destination countries by total count for plotting\n",
    "top_dest = parsed_packages_df['destination_country'].value_counts().head(5).index\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "for dest in top_dest:\n",
    "    ts = ts_by_dest[ts_by_dest['destination_country'] == dest]\n",
    "    plt.plot(ts['date'], ts['MAILITM_FID'], marker='o', label=dest)\n",
    "\n",
    "plt.title('Monthly Package count by Destination Country')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Packages')\n",
    "plt.legend(title='Destination Country')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125",
   "metadata": {},
   "source": [
    "### 2. parsed_receptacle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the parsed_receptacle_df by date and destination_country to count receptacles per month per destination country\n",
    "parsed_receptacle_df['date'] = pd.to_datetime(parsed_receptacle_df['date'])\n",
    "\n",
    "# Group by destination_country and resample by month \n",
    "ts_by_dest_recept = parsed_receptacle_df.groupby(['destination_country', pd.Grouper(key='date', freq='ME')])['RECPTCL_FID'].count().reset_index()\n",
    "\n",
    "# Choose top 5 destination countries by total count for plotting\n",
    "top_dest_recept = parsed_receptacle_df['destination_country'].value_counts().head(5).index\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "for dest in top_dest_recept:\n",
    "    ts = ts_by_dest_recept[ts_by_dest_recept['destination_country'] == dest]\n",
    "    plt.plot(ts['date'], ts['RECPTCL_FID'], marker='o', label=dest)\n",
    "\n",
    "plt.title('Monthly Receptacle count by Destination Country')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Receptacles')\n",
    "plt.legend(title='Destination Country')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {},
   "source": [
    "## Creating additional features \n",
    " 1. 'flow_type' column with values: 'inbound' (to DZ), 'outbound' (from DZ), 'local' (DZ to DZ), otherwise 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_flow_type(df):\n",
    "    # Define the conditions\n",
    "    conditions = [\n",
    "        (df['destination_country'] == 'DZ') & (df['origin_country'] == 'DZ'), # local\n",
    "        (df['destination_country'] == 'DZ'),                                # inbound\n",
    "        (df['origin_country'] == 'DZ')                                     # outbound\n",
    "    ]\n",
    "    \n",
    "    # Define the results for each condition\n",
    "    choices = ['local', 'inbound', 'outbound']\n",
    "    \n",
    "    # Apply logic with 'other' as the default\n",
    "    return np.select(conditions, choices, default='other')\n",
    "\n",
    "# Apply to both DataFrames instantly\n",
    "parsed_packages_df['flow_type'] = get_flow_type(parsed_packages_df)\n",
    "parsed_receptacle_df['flow_type'] = get_flow_type(parsed_receptacle_df)\n",
    "\n",
    "# Print counts\n",
    "print(\"Flow type counts in parsed_packages_df:\\n\", parsed_packages_df['flow_type'].value_counts())\n",
    "print(\"\\nFlow type counts in parsed_receptacle_df:\\n\", parsed_receptacle_df['flow_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129",
   "metadata": {},
   "source": [
    "we can see that there are some values of flow type with the type \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 rows of origin_country and destination_country for flow_type 'other' in parsed_receptacle_df\n",
    "print(parsed_receptacle_df.loc[parsed_receptacle_df['flow_type'] == 'other', ['origin_country', 'destination_country']].head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131",
   "metadata": {},
   "source": [
    "## Analysis of the relation between the flow_type and the event_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132",
   "metadata": {},
   "source": [
    "## 1. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Analyse relation between flow_type and EVENT_TYPE_CD in parsed_packages_df\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "ax = sns.countplot(data=parsed_packages_df, x=\"EVENT_TYPE_CD\", hue=\"flow_type\", palette='Set2')\n",
    "plt.title(\"Distribution of EVENT_TYPE_CD by flow_type in parsed_packages_df\")\n",
    "plt.xlabel(\"EVENT_TYPE_CD\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"Flow Type\")\n",
    "plt.tight_layout()\n",
    "for c in ax.containers:\n",
    "    ax.bar_label(c, label_type='edge', fontsize=8, padding=2)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134",
   "metadata": {},
   "source": [
    "## 2. parsed_receptacle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each flow_type, list the unique EVENT_TYPE_CD values and the most frequent EVENT_TYPE_CD value\n",
    "for flow in parsed_receptacle_df['flow_type'].unique():\n",
    "    event_types = parsed_receptacle_df.loc[parsed_receptacle_df['flow_type'] == flow, 'EVENT_TYPE_CD'].unique()\n",
    "    most_common_event_type = parsed_receptacle_df.loc[parsed_receptacle_df['flow_type'] == flow, 'EVENT_TYPE_CD'].mode()\n",
    "    print(f\"Flow type: {flow}\")\n",
    "    print(f\"EVENT_TYPE_CD values: {sorted(event_types)}\")\n",
    "    if not most_common_event_type.empty:\n",
    "        print(f\"Most frequent EVENT_TYPE_CD: {most_common_event_type.iloc[0]}\")\n",
    "    else:\n",
    "        print(\"No EVENT_TYPE_CD available\")\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "ax = sns.countplot(data=parsed_receptacle_df, x=\"EVENT_TYPE_CD\", hue=\"flow_type\", palette='Set1')\n",
    "plt.title(\"Distribution of EVENT_TYPE_CD by flow_type in parsed_receptacle_df\")\n",
    "plt.xlabel(\"EVENT_TYPE_CD\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"Flow Type\")\n",
    "plt.tight_layout()\n",
    "\n",
    "for c in ax.containers:\n",
    "    ax.bar_label(c, label_type='edge', fontsize=8, padding=2)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137",
   "metadata": {},
   "source": [
    "we can see that there are major event types related to the inbound flow type( coming to DZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138",
   "metadata": {},
   "source": [
    "# Track multiple receptacles just to see the flow of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track multiple receptacles: visualize all events for 5 different RECPTCL_FID in parsed_receptacle_df\n",
    "# Pick 5 unique RECPTCL_FID values to demonstrate\n",
    "num_examples = 5\n",
    "example_receptacle_ids = parsed_receptacle_df['RECPTCL_FID'].drop_duplicates().iloc[:num_examples]\n",
    "for rid in example_receptacle_ids:\n",
    "    print(\"\\n--- Events for RECPTCL_FID:\", rid, \"---\")\n",
    "    display(parsed_receptacle_df[parsed_receptacle_df['RECPTCL_FID'] == rid][['RECPTCL_FID', 'date', 'EVENT_TYPE_CD', 'etablissement_postal', 'next_etablissement_postal']].sort_values('date').reset_index(drop=True))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "for i, receptacle_id in enumerate(example_receptacle_ids):\n",
    "    ex_df = parsed_receptacle_df[parsed_receptacle_df['RECPTCL_FID'] == receptacle_id].sort_values('date')\n",
    "    plt.plot(\n",
    "        ex_df['date'],\n",
    "        ex_df['EVENT_TYPE_CD'],\n",
    "        marker='o',\n",
    "        label=f\"RECPTCL_FID: {receptacle_id}\"\n",
    "    )\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(f\"Event Timeline (EVENT_TYPE_CD) for {num_examples} Receptacles\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"EVENT_TYPE_CD\")\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140",
   "metadata": {},
   "source": [
    "# calculating the time of existence for each receptacle/ package \n",
    "   ## processing_duration \n",
    "   Note : this duration is calculated between min and make date and doesn't give the exact duration since it is not suming the duration between each event\n",
    "   further analysis could be conducted to take that ninto consideration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141",
   "metadata": {},
   "source": [
    "### 1. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate processing_duration (time from first to last event per MAILITM_FID)\n",
    "# For each mail item, sort events by date, get min and max date per mail item\n",
    "pkg_duration_df = (\n",
    "    parsed_packages_df\n",
    "    .sort_values(['MAILITM_FID', 'date'])\n",
    "    .groupby('MAILITM_FID')['date']\n",
    "    .agg(['min', 'max'])\n",
    "    .reset_index()\n",
    ")\n",
    "pkg_duration_df.columns = ['MAILITM_FID', 'min_date', 'max_date']\n",
    "pkg_duration_df['processing_duration'] = pkg_duration_df['max_date'] - pkg_duration_df['min_date']\n",
    "\n",
    "# Merge 'processing_duration' back to parsed_packages_df\n",
    "parsed_packages_df = parsed_packages_df.merge(\n",
    "    pkg_duration_df[['MAILITM_FID', 'processing_duration']],\n",
    "    on='MAILITM_FID',\n",
    "    how='left'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143",
   "metadata": {},
   "source": [
    "display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display processing_duration statistics\n",
    "print(parsed_packages_df[['MAILITM_FID', 'date', 'processing_duration']].drop_duplicates(subset=['processing_duration']).head(10))\n",
    "#print the max value of processing_duration\n",
    "max_duration = parsed_packages_df['processing_duration'].max()\n",
    "#print the mail item(s) with the max processing_duration\n",
    "max_duration_items = parsed_packages_df[parsed_packages_df['processing_duration'] == max_duration]['MAILITM_FID'].unique()\n",
    "print(f\"\\nMaximum processing duration: {max_duration}\")\n",
    "print(f\"Mail item(s) with maximum processing duration: {max_duration_items}\")\n",
    "\n",
    "#print the min value of processing_duration\n",
    "min_duration = parsed_packages_df['processing_duration'].min()\n",
    "#print the mail item(s) with the min processing_duration\n",
    "min_duration_items = parsed_packages_df[parsed_packages_df['processing_duration'] == min_duration]['MAILITM_FID'].unique()\n",
    "print(f\"\\nMinimum processing duration: {min_duration}\")\n",
    "print(f\"Mail item(s) with minimum processing duration: {min_duration_items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145",
   "metadata": {},
   "source": [
    "visualization of the distribution of the packages according to the processing duration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of packages by processing_duration\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Convert processing_duration to total days for better visualization\n",
    "parsed_packages_df['processing_duration_days'] = parsed_packages_df['processing_duration'].dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "# Create histogram\n",
    "plt.hist(parsed_packages_df['processing_duration_days'], bins=100, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Processing Duration (days)', fontsize=11)\n",
    "plt.ylabel('Number of Packages', fontsize=11)\n",
    "plt.title('Distribution of Packages by Processing Duration', fontsize=12, fontweight='bold')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Processing Duration Statistics:\")\n",
    "print(f\"Mean: {parsed_packages_df['processing_duration_days'].mean():.2f} days\")\n",
    "print(f\"Median: {parsed_packages_df['processing_duration_days'].median():.2f} days\")\n",
    "print(f\"Std Dev: {parsed_packages_df['processing_duration_days'].std():.2f} days\")\n",
    "print(f\"Min: {parsed_packages_df['processing_duration_days'].min():.2f} days\")\n",
    "print(f\"Max: {parsed_packages_df['processing_duration_days'].max():.2f} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147",
   "metadata": {},
   "source": [
    "- This doesn't give a lot of insights, we try to analyse the event type and the duration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for processing_duration by EVENT_TYPE_CD\n",
    "duration_by_event = (\n",
    "    parsed_packages_df\n",
    "    .groupby(\"EVENT_TYPE_CD\")['processing_duration_days']\n",
    "    .describe()\n",
    ")\n",
    "print(\"Processing Duration Statistics by EVENT_TYPE_CD:\")\n",
    "print(duration_by_event)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Visualize boxplot of processing_duration by EVENT_TYPE_CD\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.boxplot(x='EVENT_TYPE_CD', y='processing_duration_days', data=parsed_packages_df, showfliers=False)\n",
    "plt.title('Processing Duration by EVENT_TYPE_CD', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('EVENT_TYPE_CD', fontsize=12)\n",
    "plt.ylabel('Processing Duration (days)', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149",
   "metadata": {},
   "source": [
    "- Bar chart showing mean processing_duration by EVENT_TYPE_CD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_duration_by_event = parsed_packages_df.groupby('EVENT_TYPE_CD')['processing_duration_days'].mean().sort_values(ascending=False)\n",
    "plt.figure(figsize=(14, 7))\n",
    "mean_duration_by_event.plot(kind='barh', color='steelblue')\n",
    "plt.xlabel('Mean Processing Duration (days)', fontsize=11)\n",
    "plt.ylabel('EVENT_TYPE_CD', fontsize=11)\n",
    "plt.title('Mean Processing Duration by EVENT_TYPE_CD', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151",
   "metadata": {},
   "source": [
    "now we can see that some events type is a kind of groups of similar event types that have approximatly the same average number of processing duration "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152",
   "metadata": {},
   "source": [
    "## 2. parsed_receptacle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate processing_duration (time from first to last event per RECPTCL_FID)\n",
    "# For each receptacle, sort events by date, get min and max date per receptacle\n",
    "rec_duration_df = (\n",
    "    parsed_receptacle_df\n",
    "    .sort_values(['RECPTCL_FID', 'date'])\n",
    "    .groupby('RECPTCL_FID')['date']\n",
    "    .agg(['min', 'max'])\n",
    "    .reset_index()\n",
    ")\n",
    "rec_duration_df['processing_duration'] = rec_duration_df['max'] - rec_duration_df['min']\n",
    "\n",
    "# Merge 'processing_duration' back to parsed_receptacle_df\n",
    "parsed_receptacle_df = parsed_receptacle_df.merge(\n",
    "    rec_duration_df[['RECPTCL_FID', 'processing_duration']],\n",
    "    on='RECPTCL_FID',\n",
    "    how='left'\n",
    ")\n",
    "# Display the first few rows to show the obtained processing_duration values\n",
    "print(parsed_receptacle_df[['RECPTCL_FID', 'date', 'processing_duration']].head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154",
   "metadata": {},
   "source": [
    "### Analysis of the relation between the processing duration and the event type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of processing_duration by EVENT_TYPE_CD\n",
    "parsed_receptacle_df['processing_duration_hours'] = parsed_receptacle_df['processing_duration'].dt.total_seconds() / 3600\n",
    "# Summary statistics for processing_duration by EVENT_TYPE_CD\n",
    "duration_by_event = (\n",
    "    parsed_receptacle_df\n",
    "    .groupby(\"EVENT_TYPE_CD\")['processing_duration_hours']\n",
    "    .describe()\n",
    ")\n",
    "print(\"Processing Duration Statistics by EVENT_TYPE_CD:\")\n",
    "print(duration_by_event)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Visualize boxplot of processing_duration by EVENT_TYPE_CD\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.boxplot(x='EVENT_TYPE_CD', y='processing_duration_hours', data=parsed_receptacle_df, showfliers=False)\n",
    "plt.title('Processing Duration by EVENT_TYPE_CD', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('EVENT_TYPE_CD', fontsize=12)\n",
    "plt.ylabel('Processing Duration (hours)', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156",
   "metadata": {},
   "source": [
    "-  Bar chart showing mean processing_duration by EVENT_TYPE_CD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_duration_by_event = parsed_receptacle_df.groupby('EVENT_TYPE_CD')['processing_duration_hours'].mean().sort_values(ascending=False)\n",
    "plt.figure(figsize=(14, 7))\n",
    "mean_duration_by_event.plot(kind='barh', color='steelblue')\n",
    "plt.xlabel('Mean Processing Duration (hours)', fontsize=11)\n",
    "plt.ylabel('EVENT_TYPE_CD', fontsize=11)\n",
    "plt.title('Mean Processing Duration by EVENT_TYPE_CD', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158",
   "metadata": {},
   "source": [
    "-  Heatmap: EVENT_TYPE_CD vs Flow Type with mean duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pivot_table = parsed_receptacle_df.pivot_table(\n",
    "    values='processing_duration_hours',\n",
    "    index='EVENT_TYPE_CD',\n",
    "    columns='flow_type',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(pivot_table, annot=True, fmt='.1f', cmap='RdYlGn_r', cbar_kws={'label': 'Mean Duration (hours)'})\n",
    "plt.title('Mean Processing Duration: EVENT_TYPE_CD vs Flow Type', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Flow Type', fontsize=12)\n",
    "plt.ylabel('EVENT_TYPE_CD', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160",
   "metadata": {},
   "source": [
    "- Correlation: Count events per receptacle and duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_count_per_receptacle = parsed_receptacle_df.groupby('RECPTCL_FID').size().reset_index(name='event_count')\n",
    "receptacle_duration = parsed_receptacle_df[['RECPTCL_FID', 'processing_duration_hours']].drop_duplicates()\n",
    "event_duration_corr = event_count_per_receptacle.merge(receptacle_duration, on='RECPTCL_FID')\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.scatter(event_duration_corr['event_count'], event_duration_corr['processing_duration_hours'], alpha=0.5, s=30)\n",
    "plt.xlabel('Number of Events per Receptacle', fontsize=11)\n",
    "plt.ylabel('Processing Duration (hours)', fontsize=11)\n",
    "plt.title('Relationship between Event Count and Processing Duration', fontsize=12, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlation coefficient\n",
    "correlation = event_duration_corr['event_count'].corr(event_duration_corr['processing_duration_hours'])\n",
    "print(f\"\\nCorrelation between event count and processing duration: {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162",
   "metadata": {},
   "source": [
    "we can see that having more events reduces the duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163",
   "metadata": {},
   "source": [
    "### Analysis of the relation between the processing_duration and flow_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll analyze the relationship between processing_duration and flow_type .\n",
    "\n",
    "duration_by_flow = (\n",
    "    parsed_receptacle_df\n",
    "    .groupby(\"flow_type\")['processing_duration']\n",
    "    .describe()\n",
    ")\n",
    "print(duration_by_flow)\n",
    "\n",
    "# Visualize the distribution: boxplot of processing_duration by flow_type\n",
    "\n",
    "# Convert processing_duration to total hours for visualization\n",
    "#parsed_receptacle_df['processing_duration_hours'] = parsed_receptacle_df['processing_duration'].dt.total_seconds() / 3600\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(x='flow_type', y='processing_duration_hours', data=parsed_receptacle_df, showfliers=False)\n",
    "\n",
    "plt.title('Processing Duration by Flow Type')\n",
    "plt.xlabel('Flow Type')\n",
    "plt.ylabel('Processing Duration (hours)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165",
   "metadata": {},
   "source": [
    "From this boxplot of Processing Duration by Flow Type, we can extract several clear results and insights:\n",
    "\n",
    "1. Inbound flow\n",
    "\n",
    "Highest processing times overall\n",
    "\n",
    "Median is relatively high (around ~110–120 hours)\n",
    "\n",
    "Very wide spread (high variability) → large IQR and long whiskers\n",
    "\n",
    "Extreme delays exist (outliers reaching ~450+ hours)\n",
    "\n",
    "2. Outbound flow\n",
    "\n",
    "Moderate processing time\n",
    "\n",
    "Median lower than inbound (~80–90 hours)\n",
    "\n",
    "Still variable, but less extreme than inbound\n",
    "\n",
    "Some long delays, but fewer and smaller than inbound\n",
    "\n",
    "\n",
    "3. Local flow\n",
    "\n",
    "Very low processing duration\n",
    "\n",
    "Tight distribution (low variance)\n",
    "\n",
    "Almost no outliers\n",
    "\n",
    "Local receptacles are processed quickly and consistently.\n",
    "\n",
    "4. Other flow\n",
    "\n",
    "Near-zero processing duration\n",
    "\n",
    "Almost no variability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166",
   "metadata": {},
   "source": [
    "### Show the row with the max value of processing_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_row = rec_duration_df.loc[rec_duration_df['processing_duration'].idxmax()]\n",
    "print(max_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = parsed_receptacle_df.loc[parsed_receptacle_df['RECPTCL_FID'] == 'FRCDGADZALGAAEN99999999990102', ['RECPTCL_FID', 'date']]\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169",
   "metadata": {},
   "source": [
    "this is probably an error in the typing since they are the only values existing of the receptacle and the three are from march the diff is in the year of the third"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170",
   "metadata": {},
   "source": [
    "# extracting more features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171",
   "metadata": {},
   "source": [
    "## receptacle dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172",
   "metadata": {},
   "source": [
    "intuitively we think that the number of etablissement crossed by the receptacle can affect the processing duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the number of unique etablissement crossed by each receptacle\n",
    "etab_counts = (\n",
    "    parsed_receptacle_df\n",
    "    .groupby('RECPTCL_FID')['etablissement_postal']\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    "    .rename(columns={'etablissement_postal': 'num_etablissements'})\n",
    ")\n",
    "# Merge back to parsed_receptacle_df\n",
    "parsed_receptacle_df = parsed_receptacle_df.merge(\n",
    "    etab_counts,\n",
    "    on='RECPTCL_FID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "parsed_receptacle_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the correlation between num_etablissements and processing_duration\n",
    "correlation = parsed_receptacle_df[['num_etablissements', 'processing_duration_hours']].corr().iloc[0,1]\n",
    "print(f\"Correlation between number of etablissments and processing duration (hours): {correlation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175",
   "metadata": {},
   "source": [
    "considering the size of the dataset this correlation is not bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between num_etablissements and processing_duration\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.lineplot(\n",
    "    data=parsed_receptacle_df,\n",
    "    x='num_etablissements',\n",
    "    y='processing_duration_hours',\n",
    "    marker='o'\n",
    ")\n",
    "plt.title('Processing Duration vs. Number of Etablissements Crossed')\n",
    "plt.xlabel('Number of Etablissements Crossed')\n",
    "plt.ylabel('Processing Duration (hours)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177",
   "metadata": {},
   "source": [
    "the line plot shows a clear increasing trend indicating that as the number of etablissments crossed increases the processing duration also increases, but the correlation value shows that the relation is weak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='num_etablissements', y='processing_duration_hours', data=parsed_receptacle_df, showfliers=False)\n",
    "plt.xlabel(\"Number of Etablissements Crossed\")\n",
    "plt.ylabel(\"Processing Duration (hours)\")\n",
    "plt.title(\"Boxplot showing the variation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179",
   "metadata": {},
   "source": [
    "by looking at the boxplot we can see that the median processing duration increases with the number of etablissments crossed but the variance also increases significantly,this explains the weak correlation value obtained earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180",
   "metadata": {},
   "source": [
    "## packages dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181",
   "metadata": {},
   "source": [
    "we test the same hypothesis for the packages dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the number of unique etablissement crossed by each package\n",
    "etab_counts_pkg = (\n",
    "    parsed_packages_df\n",
    "    .groupby('MAILITM_FID')['etablissement_postal']\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    "    .rename(columns={'etablissement_postal': 'num_etablissements'})\n",
    ")\n",
    "# Merge back to parsed_packages_df\n",
    "parsed_packages_df = parsed_packages_df.merge(\n",
    "    etab_counts_pkg,\n",
    "    on='MAILITM_FID',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the correlation between num_etablissements and processing_duration\n",
    "correlation_pkg = parsed_packages_df[['num_etablissements', 'processing_duration_days']].corr().iloc[0,1]\n",
    "print(f\"Correlation between number of etablissments and processing duration (days): {correlation_pkg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between num_etablissements and processing_duration\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.lineplot(\n",
    "    data=parsed_packages_df,\n",
    "    x='num_etablissements',\n",
    "    y='processing_duration_days',\n",
    "    marker='o'\n",
    ")\n",
    "plt.title('Processing Duration vs. Number of Etablissements Crossed')\n",
    "plt.xlabel('Number of Etablissements Crossed')\n",
    "plt.ylabel('Processing Duration (days)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185",
   "metadata": {},
   "source": [
    "the correlation is weaker than that of the receptacle dataset this is probably because of variation in packages types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of processing_duration by num_etablissements\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='num_etablissements', y='processing_duration_days', data=parsed_packages_df, showfliers=False)\n",
    "plt.xlabel(\"Number of Etablissements Crossed\")\n",
    "plt.ylabel(\"Processing Duration (days)\")\n",
    "plt.title(\"Boxplot showing the variation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187",
   "metadata": {},
   "source": [
    "relation starts to appear more when the number of etablissments crossed is more than 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets on RECPTCL_FID to analyze relationship\n",
    "# merge on left so that all packages are kept, even if their receptacle information is missing\n",
    "#load the interim files\n",
    "# parsed_packages_df = pd.read_csv(output_dir / 'interim_parsed_packages_df.csv')\n",
    "# parsed_receptacle_df = pd.read_csv(output_dir / 'interim_parsed_receptacle_df.csv')\n",
    "\n",
    "#ensure both are ordered by date using with relation with the RECPTCL_FID\n",
    "parsed_packages_df = parsed_packages_df.sort_values(by=['RECPTCL_FID', 'date'])\n",
    "parsed_receptacle_df = parsed_receptacle_df.sort_values(by=['RECPTCL_FID', 'date'])\n",
    "\n",
    "merged_df = pd.merge(\n",
    "    parsed_packages_df,\n",
    "    parsed_receptacle_df,\n",
    "    on='RECPTCL_FID',\n",
    "    how='inner',\n",
    "    suffixes=('_package', '_receptacle')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define output directory for saving processed data\n",
    "output_dir = Path('../data/interim')\n",
    "\n",
    "# Save merged dataframe\n",
    "merged_df.to_csv(output_dir / 'interim_merged_packages_receptacle_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190",
   "metadata": {},
   "source": [
    "### Feature Engineering for Clustering & Anomaly Detection\n",
    "\n",
    "Before splitting the data, we create new features that will be useful for clustering and anomaly detection. These features are derived from the merged dataset and include:\n",
    "\n",
    "**Temporal Features:**\n",
    "- `hour`: Hour of day when package was processed\n",
    "- `day_of_week`: Day of the week (Sunday to Thursday )\n",
    "- `is_weekend`: Binary flag for weekend (Friday/Saturday based on our local calendar)\n",
    "\n",
    "**Package-Level Features:**\n",
    "- `delay_flag`: Binary flag for packages exceeding 15-day processing threshold\n",
    "- `delay_per_etab`: Normalized delay accounting for number of establishments\n",
    "- `pkg_route_step`: Route identifier (current → next establishment)\n",
    "- `pkg_route_freq`: How common each package route is\n",
    "- `current_etab_freq`, `next_etab_freq`: Establishment frequency in routing\n",
    "\n",
    "**Receptacle-Level Aggregated Features:**\n",
    "- `num_packages`: Number of packages per receptacle\n",
    "- `avg_processing_days`, `std_processing_days`: Processing time statistics\n",
    "- `avg_delay_per_etab`: Average normalized delay\n",
    "- `avg_pkg_route_rarity`: Average rarity of package routes\n",
    "- `rec_route_freq`: Receptacle route frequency\n",
    "- `flow_type_freq`: Flow type frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure date column is datetime\n",
    "merged_df['date_package'] = pd.to_datetime(merged_df['date_package'])\n",
    "\n",
    "# Extract temporal features\n",
    "merged_df['hour'] = merged_df['date_package'].dt.hour\n",
    "merged_df['day_of_week'] = merged_df['date_package'].dt.dayofweek\n",
    "merged_df['is_weekend'] = merged_df['day_of_week'].isin([4, 5]).astype(int)  # Friday=4, Saturday=5\n",
    "\n",
    "print(\"Temporal features created:\")\n",
    "print(f\"  - hour: {merged_df['hour'].min()} to {merged_df['hour'].max()}\")\n",
    "print(f\"  - day_of_week: {merged_df['day_of_week'].value_counts().to_dict()}\")\n",
    "print(f\"  - is_weekend: {merged_df['is_weekend'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Binary flag for packages exceeding 15-day processing threshold\n",
    "merged_df['delay_flag'] = (merged_df['processing_duration_days'] > 15).astype(int)\n",
    "\n",
    "# Normalize processing delay by number of establishments \n",
    "# (accounts for packages taking longer routes)\n",
    "merged_df['delay_per_etab'] = (\n",
    "    merged_df['processing_duration_days'] / \n",
    "    (merged_df['num_etablissements_package'] + 1)\n",
    ")\n",
    "\n",
    "print(\"Delay features created:\")\n",
    "print(f\"  - delay_flag: {merged_df['delay_flag'].value_counts().to_dict()}\")\n",
    "print(f\"  - delay_per_etab: mean={merged_df['delay_per_etab'].mean():.2f}, std={merged_df['delay_per_etab'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create package route identifier (current → next establishment)\n",
    "merged_df['pkg_route_step'] = (\n",
    "    merged_df['etablissement_postal_package'] + '→' + \n",
    "    merged_df['next_etablissement_postal_package']\n",
    ")\n",
    "\n",
    "# Calculate package route frequency (how common each route is)\n",
    "pkg_route_freq = merged_df['pkg_route_step'].value_counts(normalize=True)\n",
    "merged_df['pkg_route_freq'] = merged_df['pkg_route_step'].map(pkg_route_freq)\n",
    "\n",
    "# Calculate establishment frequency (how often each establishment appears)\n",
    "etab_freq = pd.concat([\n",
    "    merged_df['etablissement_postal_package'],\n",
    "    merged_df['next_etablissement_postal_package']\n",
    "]).value_counts()\n",
    "\n",
    "merged_df['current_etab_freq'] = merged_df['etablissement_postal_package'].map(etab_freq)\n",
    "merged_df['next_etab_freq'] = merged_df['next_etablissement_postal_package'].map(etab_freq)\n",
    "\n",
    "print(\"Route and frequency features created:\")\n",
    "print(f\"  - Unique package routes: {merged_df['pkg_route_step'].nunique()}\")\n",
    "print(f\"  - pkg_route_freq: min={merged_df['pkg_route_freq'].min():.6f}, max={merged_df['pkg_route_freq'].max():.4f}\")\n",
    "print(f\"  - current_etab_freq: mean={merged_df['current_etab_freq'].mean():.0f}\")\n",
    "print(f\"  - next_etab_freq: mean={merged_df['next_etab_freq'].mean():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create receptacle route identifier\n",
    "merged_df['rec_route_step'] = (\n",
    "    merged_df['etablissement_postal_receptacle'] + '→' + \n",
    "    merged_df['next_etablissement_postal_receptacle']\n",
    ")\n",
    "\n",
    "# Aggregate package statistics at receptacle level\n",
    "receptacle_route_stats = (\n",
    "    merged_df\n",
    "    .groupby('RECPTCL_FID')\n",
    "    .agg(\n",
    "        rec_route=('rec_route_step', 'first'),\n",
    "        num_packages=('MAILITM_FID', 'count'),\n",
    "        avg_processing_days=('processing_duration_days', 'mean'),\n",
    "        std_processing_days=('processing_duration_days', 'std'),\n",
    "        avg_delay_per_etab=('delay_per_etab', 'mean'),\n",
    "        avg_pkg_route_rarity=('pkg_route_freq', 'mean')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Fill NaN standard deviations (for receptacles with single packages)\n",
    "receptacle_route_stats['std_processing_days'] = receptacle_route_stats['std_processing_days'].fillna(0)\n",
    "\n",
    "print(\"Receptacle aggregated features created:\")\n",
    "print(f\"  - Total receptacles: {len(receptacle_route_stats)}\")\n",
    "print(f\"  - num_packages: mean={receptacle_route_stats['num_packages'].mean():.2f}\")\n",
    "print(f\"  - avg_processing_days: mean={receptacle_route_stats['avg_processing_days'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate receptacle route frequency\n",
    "rec_route_freq = receptacle_route_stats['rec_route'].value_counts(normalize=True)\n",
    "receptacle_route_stats['rec_route_freq'] = receptacle_route_stats['rec_route'].map(rec_route_freq)\n",
    "\n",
    "# Map flow type frequencies to each receptacle\n",
    "rec_flow_type = merged_df.groupby('RECPTCL_FID')['flow_type_receptacle'].first().reset_index()\n",
    "flow_type_freq = rec_flow_type['flow_type_receptacle'].value_counts(normalize=True)\n",
    "rec_flow_type['flow_type_freq'] = rec_flow_type['flow_type_receptacle'].map(flow_type_freq)\n",
    "\n",
    "# Merge flow type frequency back to receptacle stats\n",
    "receptacle_route_stats = receptacle_route_stats.merge(\n",
    "    rec_flow_type[['RECPTCL_FID', 'flow_type_freq']],\n",
    "    on='RECPTCL_FID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"Receptacle frequency features created:\")\n",
    "print(f\"  - Unique receptacle routes: {receptacle_route_stats['rec_route'].nunique()}\")\n",
    "print(f\"  - rec_route_freq: min={receptacle_route_stats['rec_route_freq'].min():.6f}, max={receptacle_route_stats['rec_route_freq'].max():.4f}\")\n",
    "print(f\"  - flow_type_freq distribution: {receptacle_route_stats['flow_type_freq'].describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge receptacle-level aggregated features back to the main dataframe\n",
    "merged_df = merged_df.merge(\n",
    "    receptacle_route_stats[[\n",
    "        'RECPTCL_FID', 'num_packages', 'avg_processing_days', \n",
    "        'std_processing_days', 'avg_delay_per_etab', 'avg_pkg_route_rarity',\n",
    "        'rec_route_freq', 'flow_type_freq'\n",
    "    ]],\n",
    "    on='RECPTCL_FID',\n",
    "    how='left',\n",
    "    suffixes=('', '_rec')\n",
    ")\n",
    "\n",
    "print(\"Merged receptacle features back to main dataframe\")\n",
    "print(f\"Total columns after feature engineering: {len(merged_df.columns)}\")\n",
    "print(f\"\\nNew features added:\")\n",
    "new_features = ['hour', 'day_of_week', 'is_weekend', 'delay_flag', 'delay_per_etab',\n",
    "                'pkg_route_step', 'pkg_route_freq', 'current_etab_freq', 'next_etab_freq',\n",
    "                'rec_route_step', 'num_packages', 'avg_processing_days', 'std_processing_days',\n",
    "                'avg_delay_per_etab', 'avg_pkg_route_rarity', 'rec_route_freq', 'flow_type_freq']\n",
    "for feat in new_features:\n",
    "    if feat in merged_df.columns:\n",
    "        print(f\"   {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Package-level features for clustering\n",
    "pkg_features = [\n",
    "    'processing_duration_days',\n",
    "    'delay_flag',\n",
    "    'delay_per_etab',\n",
    "    'num_etablissements_package',\n",
    "    'pkg_route_freq',\n",
    "    'current_etab_freq',\n",
    "    'next_etab_freq',\n",
    "    'hour',\n",
    "    'is_weekend'\n",
    "]\n",
    "\n",
    "# Receptacle-level features for clustering\n",
    "rec_features = [\n",
    "    'num_packages',\n",
    "    'avg_processing_days',\n",
    "    'std_processing_days',\n",
    "    'avg_delay_per_etab',\n",
    "    'avg_pkg_route_rarity',\n",
    "    'rec_route_freq',\n",
    "    'flow_type_freq'\n",
    "]\n",
    "\n",
    "# All numeric features for clustering (combined)\n",
    "numeric_clustering_features = pkg_features + rec_features\n",
    "\n",
    "# Check for NaN values in clustering features\n",
    "print(\"Checking for NaN values in clustering features:\")\n",
    "nan_counts = merged_df[numeric_clustering_features].isna().sum()\n",
    "nan_counts = nan_counts[nan_counts > 0]\n",
    "if len(nan_counts) > 0:\n",
    "    print(\"Features with NaN values:\")\n",
    "    print(nan_counts)\n",
    "else:\n",
    "    print(\"   No NaN values found in clustering features\")\n",
    "\n",
    "print(f\"\\nPackage-level features: {len(pkg_features)}\")\n",
    "print(f\"Receptacle-level features: {len(rec_features)}\")\n",
    "print(f\"Total numeric features for clustering: {len(numeric_clustering_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198",
   "metadata": {},
   "source": [
    "## Data Splitting to training and testing sets\n",
    "- We propose to split datasets (which are already sorted by date) using `TimeSeriesSplit` from `sklearn.model_selection`\n",
    "- because we need to keep all rows for a specific `MAILITM_FID` as a single block and not separated.\n",
    "- older rows are kept for the `training_set` and newer ones are kept for the `testing_set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Sort by date to ensure proper time series splitting\n",
    "merged_df_sorted = merged_df.sort_values(by='date_package').reset_index(drop=True)\n",
    "\n",
    "# Identify feature types for the preprocessor\n",
    "# Categorical features to one-hot encode\n",
    "cat_features_for_split = [\n",
    "    'origin_country_package', 'destination_country_package',\n",
    "    'origin_country_receptacle', 'destination_country_receptacle',\n",
    "    'flow_type_package', 'flow_type_receptacle'\n",
    "]\n",
    "# Filter to only include features that exist\n",
    "cat_features = [col for col in cat_features_for_split if col in merged_df_sorted.columns]\n",
    "\n",
    "# Numeric features to scale (including our engineered features)\n",
    "numeric_features_for_clustering = numeric_clustering_features.copy()\n",
    "\n",
    "print(f\"Categorical features for encoding: {len(cat_features)}\")\n",
    "print(f\"Numeric features for scaling: {len(numeric_features_for_clustering)}\")\n",
    "print(f\"Total features: {len(merged_df_sorted.columns)}\")\n",
    "\n",
    "# Create preprocessor for the merged dataset with engineered features\n",
    "merged_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_features),\n",
    "        ('num', StandardScaler(), numeric_features_for_clustering)\n",
    "    ],\n",
    "    remainder='drop'  # Drop columns not specified (like dates, IDs, route strings)\n",
    ")\n",
    "\n",
    "# Initialize TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit()\n",
    "\n",
    "# Display split information\n",
    "print(f\"\\nTimeSeriesSplit configuration:\")\n",
    "for fold, (train_idx, test_idx) in enumerate(tscv.split(merged_df_sorted)):\n",
    "    print(f\"Fold {fold + 1}: Train size={len(train_idx)}, Test size={len(test_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_etab_mapping_from_merged(df, source='package'):\n",
    " \n",
    "    # Select appropriate column suffixes based on source\n",
    "    suffix = f'_{source}'\n",
    "    \n",
    "    # Get the columns we need\n",
    "    etab_col = f'etablissement_postal{suffix}'\n",
    "    next_etab_col = f'next_etablissement_postal{suffix}'\n",
    "    \n",
    "    # Verify columns exist\n",
    "    if etab_col not in df.columns or next_etab_col not in df.columns:\n",
    "        print(f\"Warning: Columns {etab_col} or {next_etab_col} not found in dataframe\")\n",
    "        print(f\"Available columns: {[c for c in df.columns if 'etablissement' in c.lower()]}\")\n",
    "        return {}\n",
    "    \n",
    "    # 1. Calculate the global mode once (to use as a safe fallback)\n",
    "    # Filter out NaN values for mode calculation\n",
    "    mode_values = df[next_etab_col].dropna()\n",
    "    if len(mode_values) > 0:\n",
    "        global_mode = mode_values.mode().iat[0] if not mode_values.mode().empty else None\n",
    "    else:\n",
    "        global_mode = None\n",
    "    \n",
    "    # 2. Get the mode for every group at once\n",
    "    # This creates a Series where index = etablissement, value = most frequent next\n",
    "    modes_per_group = df.groupby(etab_col)[next_etab_col].agg(\n",
    "        lambda x: x.mode().iat[0] if not x.mode().empty else global_mode\n",
    "    )\n",
    "    \n",
    "    # 3. Convert to dictionary\n",
    "    etablissement_dict = modes_per_group.to_dict()\n",
    "    \n",
    "    return etablissement_dict\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201",
   "metadata": {},
   "source": [
    "* After splitting, for each pair `(X_train, X_test)` of `packages_splits`, we'll fill the null values of `next_etablissement_postal` based on the most frequent value of `next_etablissement_postal` appearing with the value of `etablissement_postal` of each specific row having a null value in `next_etablissement_postal`\n",
    "* Example:\n",
    "say that a row has a null `next_etablissement_postal`, we look at it's `etablissement_postal` value (say `VAL`), we iterate through the training set, we count how many times each `ETAB_XXXX` in ` next_etablissement_postal` appears with `VAL` being in `etablissement_postal`, we take the mode, and we use it fill all rows having null value at `next_etablissement_postal` where their `etablissement_postal` is `VAL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_splits(preprocessor, df, id_col, date_col='date_package'):\n",
    "\n",
    "    # Sort by date\n",
    "    df = df.sort_values(date_col).reset_index(drop=True)\n",
    "    \n",
    "    # Get unique IDs for splitting\n",
    "    unique_ids = df[id_col].unique()\n",
    "    \n",
    "    # Initialize TimeSeriesSplit\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    splits = []\n",
    "    \n",
    "    # Define both sources to process\n",
    "    sources = ['package', 'receptacle']\n",
    " \n",
    "    for fold_num, (train_id_idx, test_id_idx) in enumerate(tscv.split(unique_ids)):\n",
    "        # 1. Get the actual IDs for this fold\n",
    "        train_ids = unique_ids[train_id_idx]\n",
    "        test_ids = unique_ids[test_id_idx]\n",
    "        \n",
    "        # 2. Filter the main dataframe to get rows for this fold\n",
    "        train_df = df[df[id_col].isin(train_ids)].copy()\n",
    "        test_df = df[df[id_col].isin(test_ids)].copy()\n",
    "        \n",
    "        # 3. Handle NULL VALUES for BOTH package and receptacle columns\n",
    "        for source in sources:\n",
    "            etab_col = f'etablissement_postal_{source}'\n",
    "            next_etab_col = f'next_etablissement_postal_{source}'\n",
    "            \n",
    "            # Only process if these columns exist\n",
    "            if etab_col in train_df.columns and next_etab_col in train_df.columns:\n",
    "                # Create mapping from training data\n",
    "                etablissement_dict = get_etab_mapping_from_merged(train_df, source=source)\n",
    "                \n",
    "                # Fill NULLs in both train and test sets\n",
    "                for fold_data in [train_df, test_df]:\n",
    "                    # Find rows with missing next_etablissement_postal\n",
    "                    mask = fold_data[next_etab_col].isna()\n",
    "                    \n",
    "                    if mask.any():\n",
    "                        # Map using the dictionary\n",
    "                        fold_data.loc[mask, next_etab_col] = fold_data.loc[mask, etab_col].map(etablissement_dict)\n",
    "                        \n",
    "                        # Fill any remaining NaNs with global mode from training data\n",
    "                        if fold_data[next_etab_col].isna().any():\n",
    "                            global_mode = train_df[next_etab_col].dropna().mode()\n",
    "                            if len(global_mode) > 0:\n",
    "                                fold_data[next_etab_col].fillna(global_mode.iat[0], inplace=True)\n",
    "                        \n",
    "        \n",
    "        # 4. RECALCULATE FEATURES that depend on next_etablissement_postal for BOTH sides\n",
    "        for fold_data in [train_df, test_df]:\n",
    "            # ==================== PACKAGE-side features ====================\n",
    "            if 'etablissement_postal_package' in fold_data.columns:\n",
    "                etab_pkg = 'etablissement_postal_package'\n",
    "                next_etab_pkg = 'next_etablissement_postal_package'\n",
    "                \n",
    "                # Recalculate pkg_route_step and pkg_route_freq\n",
    "                fold_data['pkg_route_step'] = fold_data[etab_pkg] + '→' + fold_data[next_etab_pkg]\n",
    "                \n",
    "                # Recalculate next_etab_freq (PACKAGE side) using training data frequencies\n",
    "                next_etab_freq_pkg = train_df[next_etab_pkg].value_counts(normalize=True)\n",
    "                fold_data['next_etab_freq'] = fold_data[next_etab_pkg].map(next_etab_freq_pkg)\n",
    "                fold_data['next_etab_freq'].fillna(next_etab_freq_pkg.min() if len(next_etab_freq_pkg) > 0 else 0.0001, inplace=True)\n",
    "                \n",
    "                # Recalculate pkg_route_freq using training data frequencies\n",
    "                pkg_route_freq = train_df['pkg_route_step'].value_counts(normalize=True)\n",
    "                fold_data['pkg_route_freq'] = fold_data['pkg_route_step'].map(pkg_route_freq)\n",
    "                fold_data['pkg_route_freq'].fillna(pkg_route_freq.min() if len(pkg_route_freq) > 0 else 0.0001, inplace=True)\n",
    "            \n",
    "            # ==================== RECEPTACLE-side features ====================\n",
    "            if 'etablissement_postal_receptacle' in fold_data.columns:\n",
    "                etab_rec = 'etablissement_postal_receptacle'\n",
    "                next_etab_rec = 'next_etablissement_postal_receptacle'\n",
    "                \n",
    "                # Recalculate rec_route_step and rec_route_freq\n",
    "                fold_data['rec_route_step'] = fold_data[etab_rec] + '→' + fold_data[next_etab_rec]\n",
    "                \n",
    "                # # Recalculate next_etab_freq_rec (RECEPTACLE side) using training data frequencies\n",
    "                # next_etab_freq_rec = train_df[next_etab_rec].value_counts(normalize=True)\n",
    "                # fold_data['next_etab_freq_rec'] = fold_data[next_etab_rec].map(next_etab_freq_rec)\n",
    "                # fold_data['next_etab_freq_rec'].fillna(next_etab_freq_rec.min() if len(next_etab_freq_rec) > 0 else 0.0001, inplace=True)\n",
    "                \n",
    "                # Recalculate rec_route_freq using training data frequencies\n",
    "                rec_route_freq = train_df['rec_route_step'].value_counts(normalize=True)\n",
    "                fold_data['rec_route_freq'] = fold_data['rec_route_step'].map(rec_route_freq)\n",
    "                fold_data['rec_route_freq'].fillna(rec_route_freq.min() if len(rec_route_freq) > 0 else 0.0001, inplace=True)\n",
    "    \n",
    "        \n",
    "        # 5. Finally, Transform to Numeric using the preprocessor\n",
    "        X_train = preprocessor.fit_transform(train_df)\n",
    "        X_test = preprocessor.transform(test_df)\n",
    "        \n",
    "        splits.append((X_train, X_test))\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build splits using the merged dataframe - handles BOTH package and receptacle\n",
    "print(\"Building splits for MERGED DATA\")\n",
    "merged_splits = build_splits(\n",
    "    merged_preprocessor, \n",
    "    merged_df_sorted, \n",
    "    id_col='RECPTCL_FID',\n",
    "    date_col='date_package'\n",
    ")\n",
    "# Display summary of merged splits\n",
    "print(\"Summary of merged splits:\")\n",
    "print(f\"Total folds: {len(merged_splits)}\")\n",
    "for i, (X_train, X_test) in enumerate(merged_splits):\n",
    "    print(f\"Fold {i + 1}: X_train shape = {X_train.shape}, X_test shape = {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204",
   "metadata": {},
   "source": [
    "**We still need to deal with the high dimensionality we get from `OneHotEncoder`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205",
   "metadata": {},
   "source": [
    "* now each of `packages_splits` and `receptacles_splits` have 5 pairs of `(X_train, X_test)` following the cross-validation method\n",
    "* for each pair, we'll apply a classification algorithm to predict the clustering and anomaly detection targets for the testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206",
   "metadata": {},
   "source": [
    "# 1. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207",
   "metadata": {},
   "source": [
    "## 1.1 KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Use the first fold from merged_splits for initial clustering\n",
    "X_train, X_test = merged_splits[0]\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "print(\"Finding optimal K\")\n",
    "\n",
    "\n",
    "k_range = range(2, 11)\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_train)\n",
    "   \n",
    "    if k > 1:\n",
    "        sil_score = silhouette_score(X_train, kmeans.labels_, sample_size=min(10000, len(X_train)))\n",
    "        silhouette_scores.append(sil_score)\n",
    "        print(f\"K={k}: Silhouette={sil_score:.4f}\")\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(7, 5))\n",
    "\n",
    "\n",
    "# Silhouette score plot\n",
    "axes.plot(k_range, silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
    "axes.set_xlabel('Number of Clusters (K)', fontsize=12)\n",
    "axes.set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes.set_title('Silhouette Score vs Number of Clusters', fontsize=14)\n",
    "axes.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best K based on silhouette score\n",
    "best_k = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"\\n Best K based on silhouette score: {best_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "chosen_k = 10\n",
    "print(f\" CLUSTERING WITH K={chosen_k}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Fit final KMeans model\n",
    "final_kmeans = KMeans(n_clusters=chosen_k, random_state=42, n_init=10)\n",
    "train_labels = final_kmeans.fit_predict(X_train)\n",
    "test_labels = final_kmeans.predict(X_test)\n",
    "\n",
    "print(f\"\\nTraining set cluster distribution:\")\n",
    "unique, counts = np.unique(train_labels, return_counts=True)\n",
    "for cluster, count in zip(unique, counts):\n",
    "    print(f\"  Cluster {cluster}: {count} samples ({count/len(train_labels)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTest set cluster distribution:\")\n",
    "unique, counts = np.unique(test_labels, return_counts=True)\n",
    "for cluster, count in zip(unique, counts):\n",
    "    print(f\"  Cluster {cluster}: {count} samples ({count/len(test_labels)*100:.1f}%)\")\n",
    "\n",
    "# Reduce to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_train_2d = pca.fit_transform(X_train)\n",
    "X_test_2d = pca.transform(X_test)\n",
    "\n",
    "print(f\"\\nPCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained: {sum(pca.explained_variance_ratio_)*100:.2f}%\")\n",
    "\n",
    "# Plot clusters\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Training set\n",
    "scatter1 = axes[0].scatter(X_train_2d[:, 0], X_train_2d[:, 1], \n",
    "                           c=train_labels, cmap='viridis', alpha=0.7, s=50)\n",
    "axes[0].set_xlabel('PC1', fontsize=12)\n",
    "axes[0].set_ylabel('PC2', fontsize=12)\n",
    "axes[0].set_title(f'Training Set Clusters (K={chosen_k})', fontsize=14)\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "\n",
    "# Test set\n",
    "scatter2 = axes[1].scatter(X_test_2d[:, 0], X_test_2d[:, 1], \n",
    "                           c=test_labels, cmap='viridis', alpha=0.7, s=50)\n",
    "axes[1].set_xlabel('PC1', fontsize=12)\n",
    "axes[1].set_ylabel('PC2', fontsize=12)\n",
    "axes[1].set_title(f'Test Set Clusters (K={chosen_k})', fontsize=14)\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"CLUSTER QUALITY METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Silhouette Score\n",
    "sil_train = silhouette_score(X_train, train_labels)\n",
    "sil_test = silhouette_score(X_test, test_labels)\n",
    "print(f\"Silhouette Score (Train): {sil_train:.4f}\")\n",
    "print(f\"Silhouette Score (Test):  {sil_test:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210",
   "metadata": {},
   "source": [
    "## 1.2 DBSCAN \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 2. TRY DIFFERENT EPS VALUES\n",
    "print(\"DBSCAN WITH DIFFERENT EPS VALUES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try multiple eps values around the suggested one\n",
    "eps_values = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\n",
    "min_samples = 5\n",
    "\n",
    "results = []\n",
    "for eps in eps_values:\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(X_train)\n",
    "    \n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "    noise_ratio = n_noise / len(labels) * 100\n",
    "    \n",
    "    # Calculate silhouette score only if there are at least 2 clusters and not all noise\n",
    "    if n_clusters >= 2 and n_noise < len(labels):\n",
    "        # Exclude noise points for silhouette calculation\n",
    "        mask = labels != -1\n",
    "        if mask.sum() > 1:\n",
    "            sil = silhouette_score(X_train[mask], labels[mask])\n",
    "        else:\n",
    "            sil = -1\n",
    "    else:\n",
    "        sil = -1\n",
    "    \n",
    "    results.append({\n",
    "        'eps': eps,\n",
    "        'n_clusters': n_clusters,\n",
    "        'n_noise': n_noise,\n",
    "        'noise_ratio': noise_ratio,\n",
    "        'silhouette': sil\n",
    "    })\n",
    "    \n",
    "    print(f\"eps={eps:.1f}: {n_clusters} clusters, {n_noise} noise points ({noise_ratio:.1f}%), Silhouette={sil:.4f}\" if sil != -1 else f\"eps={eps:.1f}: {n_clusters} clusters, {n_noise} noise points ({noise_ratio:.1f}%), Silhouette=N/A\")\n",
    "\n",
    "# Find best eps based on silhouette score (among valid ones)\n",
    "valid_results = [r for r in results if r['silhouette'] > 0]\n",
    "if valid_results:\n",
    "    best_result = max(valid_results, key=lambda x: x['silhouette'])\n",
    "    best_eps = best_result['eps']\n",
    "    print(f\"\\n Best eps based on silhouette: {best_eps}\")\n",
    "else:\n",
    "    best_eps = 1.5  # Default\n",
    "    print(f\"\\n No valid clustering found, using default eps: {best_eps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN MODEL AND VISUALIZATION\n",
    "\n",
    "print(f\"FINAL DBSCAN CLUSTERING (eps={best_eps}, min_samples={min_samples})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Fit final DBSCAN model\n",
    "final_dbscan = DBSCAN(eps=best_eps, min_samples=min_samples)\n",
    "dbscan_train_labels = final_dbscan.fit_predict(X_train)\n",
    "\n",
    "# For test set, we need to assign labels based on nearest core points\n",
    "# DBSCAN doesn't have a predict method, so we use a workaround\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Find core samples from training\n",
    "core_samples_mask = np.zeros_like(dbscan_train_labels, dtype=bool)\n",
    "core_samples_mask[final_dbscan.core_sample_indices_] = True\n",
    "\n",
    "# Use nearest neighbors to assign test points to clusters\n",
    "nn = NearestNeighbors(n_neighbors=1)\n",
    "nn.fit(X_train[core_samples_mask])\n",
    "distances, indices = nn.kneighbors(X_test)\n",
    "\n",
    "# Assign test labels based on nearest core point\n",
    "dbscan_test_labels = np.array([\n",
    "    dbscan_train_labels[final_dbscan.core_sample_indices_[idx[0]]] \n",
    "    if dist[0] <= best_eps else -1 \n",
    "    for dist, idx in zip(distances, indices)\n",
    "])\n",
    "\n",
    "# Results summary\n",
    "n_clusters_train = len(set(dbscan_train_labels)) - (1 if -1 in dbscan_train_labels else 0)\n",
    "n_noise_train = list(dbscan_train_labels).count(-1)\n",
    "\n",
    "n_clusters_test = len(set(dbscan_test_labels)) - (1 if -1 in dbscan_test_labels else 0)\n",
    "n_noise_test = list(dbscan_test_labels).count(-1)\n",
    "\n",
    "print(f\"\\nTraining Set Results:\")\n",
    "print(f\"  Number of clusters: {n_clusters_train}\")\n",
    "print(f\"  Noise points: {n_noise_train} ({n_noise_train/len(dbscan_train_labels)*100:.1f}%)\")\n",
    "print(f\"  Core samples: {len(final_dbscan.core_sample_indices_)}\")\n",
    "\n",
    "print(f\"\\nTest Set Results:\")\n",
    "print(f\"  Number of clusters: {n_clusters_test}\")\n",
    "print(f\"  Noise points: {n_noise_test} ({n_noise_test/len(dbscan_test_labels)*100:.1f}%)\")\n",
    "\n",
    "# Cluster distribution\n",
    "print(f\"\\nTraining cluster distribution:\")\n",
    "unique, counts = np.unique(dbscan_train_labels, return_counts=True)\n",
    "for cluster, count in zip(unique, counts):\n",
    "    label = \"Noise\" if cluster == -1 else f\"Cluster {cluster}\"\n",
    "    print(f\"  {label}: {count} samples ({count/len(dbscan_train_labels)*100:.1f}%)\")\n",
    "\n",
    " \n",
    "# Use PCA from previous KMeans analysis\n",
    "X_train_2d = pca.transform(X_train)\n",
    "X_test_2d = pca.transform(X_test)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Training set - color noise points differently\n",
    "colors_train = ['gray' if l == -1 else plt.cm.viridis(l / max(dbscan_train_labels.max(), 1)) \n",
    "                for l in dbscan_train_labels]\n",
    "axes[0].scatter(X_train_2d[:, 0], X_train_2d[:, 1], c=colors_train, alpha=0.7, s=50)\n",
    "axes[0].scatter(X_train_2d[core_samples_mask, 0], X_train_2d[core_samples_mask, 1], \n",
    "                c='red', marker='x', s=30, alpha=0.5, label='Core samples')\n",
    "axes[0].set_xlabel('PC1', fontsize=12)\n",
    "axes[0].set_ylabel('PC2', fontsize=12)\n",
    "axes[0].set_title(f'DBSCAN Training Set (eps={best_eps})\\n{n_clusters_train} clusters, {n_noise_train} noise', fontsize=14)\n",
    "axes[0].legend()\n",
    "\n",
    "# Test set\n",
    "colors_test = ['gray' if l == -1 else plt.cm.viridis(l / max(dbscan_test_labels.max(), 1)) \n",
    "               for l in dbscan_test_labels]\n",
    "axes[1].scatter(X_test_2d[:, 0], X_test_2d[:, 1], c=colors_test, alpha=0.7, s=50)\n",
    "axes[1].set_xlabel('PC1', fontsize=12)\n",
    "axes[1].set_ylabel('PC2', fontsize=12)\n",
    "axes[1].set_title(f'DBSCAN Test Set\\n{n_clusters_test} clusters, {n_noise_test} noise', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# DBSCAN VS KMEANS\n",
    "\n",
    "print(\"COMPARISON: DBSCAN vs KMeans\")\n",
    "\n",
    "\n",
    "# Calculate metrics for non-noise points\n",
    "mask_train = dbscan_train_labels != -1\n",
    "mask_test = dbscan_test_labels != -1\n",
    "\n",
    "if mask_train.sum() > 1 and len(set(dbscan_train_labels[mask_train])) > 1:\n",
    "    dbscan_sil_train = silhouette_score(X_train[mask_train], dbscan_train_labels[mask_train])\n",
    "else:\n",
    "    dbscan_sil_train = None\n",
    "\n",
    "if mask_test.sum() > 1 and len(set(dbscan_test_labels[mask_test])) > 1:\n",
    "    dbscan_sil_test = silhouette_score(X_test[mask_test], dbscan_test_labels[mask_test])\n",
    "else:\n",
    "    dbscan_sil_test = None\n",
    "\n",
    "print(f\"\\n{'Metric':<30} {'KMeans':>15} {'DBSCAN':>15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Number of Clusters (Train)':<30} {chosen_k:>15} {n_clusters_train:>15}\")\n",
    "print(f\"{'Noise Points (Train)':<30} {'0':>15} {n_noise_train:>15}\")\n",
    "print(f\"{'Silhouette Score (Train)':<30} {sil_train:>15.4f} {dbscan_sil_train:>15.4f}\" if dbscan_sil_train else f\"{'Silhouette Score (Train)':<30} {sil_train:>15.4f} {'N/A':>15}\")\n",
    "print(f\"{'Silhouette Score (Test)':<30} {sil_test:>15.4f} {dbscan_sil_test:>15.4f}\" if dbscan_sil_test else f\"{'Silhouette Score (Test)':<30} {sil_test:>15.4f} {'N/A':>15}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
