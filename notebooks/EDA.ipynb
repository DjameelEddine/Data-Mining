{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "make sure to rename the columns by removing é\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) for Packages and Receptacle Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "packages_df = pd.read_csv('../data/raw/packages_data_2023_2025.csv',delimiter=';', encoding='latin-1')\n",
    "receptacles_df = pd.read_csv('../data/raw/receptacle_data_2023_2025.csv',delimiter=';', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "* Columns' names adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df = packages_df.rename(columns={'établissement_postal': 'etablissement_postal', 'next_établissement_postal': 'next_etablissement_postal'})\n",
    "receptacles_df = receptacles_df.rename(columns={'ï»¿RECPTCL_FID': 'RECPTCL_FID', 'EVENT_TYPECD': 'EVENT_TYPE_CD', 'nextetablissement_postal': 'next_etablissement_postal'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df.shape, receptacles_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "* Columns' types adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df['date'] = pd.to_datetime(packages_df['date'])\n",
    "receptacles_df['date'] = pd.to_datetime(receptacles_df['date'])\n",
    "packages_df['RECPTCL_FID'] = packages_df['RECPTCL_FID'].str.strip()\n",
    "packages_df['MAILITM_FID'] = packages_df['MAILITM_FID'].str.strip()\n",
    "packages_df['etablissement_postal'] = packages_df['etablissement_postal'].str.strip()\n",
    "packages_df['next_etablissement_postal'] = packages_df['next_etablissement_postal'].str.strip()\n",
    "receptacles_df['etablissement_postal'] = receptacles_df['etablissement_postal'].str.strip()\n",
    "receptacles_df['next_etablissement_postal'] = receptacles_df['next_etablissement_postal'].str.strip()\n",
    "receptacles_df['RECPTCL_FID'] = receptacles_df['RECPTCL_FID'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    " Initial Observations\n",
    "- both datasets cover the period from 2023 to 2025\n",
    "- we have no target variable in either dataset\n",
    "- for packages dataset:\n",
    "    - 6 features in total with 5 categorical and 1 numerical\n",
    "    - MAILITM_FID is unique identifier for each package\n",
    "    - RECPTCL_FID is foreign key linking to receptacle dataset\n",
    "    - etablissement_postal and next_etablissement_postal have some null values\n",
    "- for receptacle dataset:\n",
    "    - 5 features in total with 4 categorical and 1 numerical\n",
    "    - RECPTCL_FID is unique identifier for each receptacle\n",
    "    - EVENT_TYPE_CD has some null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in packages_df.columns:\n",
    "    print(f'{column} has {packages_df[column].nunique()} unique values.')\n",
    "    print(f'{column} has {packages_df[column].isnull().sum()} null values.')\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in receptacles_df.columns:\n",
    "    print(f'{column} has {receptacles_df[column].nunique()} unique values.')\n",
    "    print(f'{column} has {receptacles_df[column].isnull().sum()} null values.')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "we notice the following:<br>\n",
    "- receptacle dataset has more unique values for RECPTCL_FID than packages dataset, indicating one-to-many relationship<br>\n",
    "- MAILITM_FID is unique in packages dataset.<br>\n",
    "- packages dataset have more unique date values than receptacle dataset.<br>\n",
    "- both datasets have null values in etablissement_postal and next_etablissement_postal columns. This requires processing later on<br>\n",
    "- packages dataset has more unique values in the next_etablissement_postal column compared to receptacle dataset but also more null values. **further investigation is needed to understand why**<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "for EVENT_TYPE_CD we notice different range of values for packages and receptacle datasets indicating different types of events.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### Visualizations\n",
    "for now we will visualize the distribution of EVENT_TYPE_CD in both datasets.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of EVENT_TYPE_CD in packages dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=packages_df,x='EVENT_TYPE_CD')\n",
    "plt.title('distribution of EVENT_TYPE_CD in packages dataset')\n",
    "plt.xlabel('EVENT_TYPE_CD')\n",
    "plt.ylabel('count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of EVENT_TYPE_CD in receptacle dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=receptacles_df,x='EVENT_TYPE_CD')\n",
    "plt.title('distribution of EVENT_TYPE_CD in Receptacle dataset')\n",
    "plt.xlabel('EVENT_TYPE_CD')\n",
    "plt.ylabel('count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "for etablissement_postal and next_etablissement_postal I will start with visualizing the receptacle dataset since the packages dataset has a lot of unique values<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of etablissement_postal in receptacle dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=receptacles_df,x='etablissement_postal')\n",
    "plt.title('distribution of etablissement_postal in Receptacle dataset')\n",
    "plt.xlabel('etablissement_postal')\n",
    "plt.ylabel('count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of next_etablissement_postal in receptacle dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=receptacles_df,x='next_etablissement_postal')\n",
    "plt.title('distribution of next_etablissement_postal in Receptacle dataset')\n",
    "plt.xlabel('next_etablissement_postal')\n",
    "plt.ylabel('count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.  Top 10 locations to keep the map small\n",
    "top_locs = receptacles_df['etablissement_postal'].value_counts().head(10).index\n",
    "top_next = receptacles_df['next_etablissement_postal'].value_counts().head(10).index\n",
    "\n",
    "# 2. include only these top locations\n",
    "filtered_df = receptacles_df[\n",
    "    (receptacles_df['etablissement_postal'].isin(top_locs)) &\n",
    "    (receptacles_df['next_etablissement_postal'].isin(top_next))\n",
    "]\n",
    "\n",
    "# 3. Create a Matrix (Cross-tabulation)\n",
    "matrix = pd.crosstab(filtered_df['etablissement_postal'], filtered_df['next_etablissement_postal'])\n",
    "\n",
    "# 4. Plot Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(matrix, cmap='Reds', annot=True, fmt='d', linewidths=.5)\n",
    "plt.title(\"Most Frequent Routes (Origin vs Destination) - Receptacle Dataset\")\n",
    "plt.xlabel(\"Next Destination\")\n",
    "plt.ylabel(\"Current Location\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "we notice that some etablissements have significantly higher traffic compared to others, indicating  major distribution centers.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "for etablissement_postal and next_etablissement_postal we will create a heatmap to visualize the flow between current location and next destination.<br>\n",
    "Count of parcels moving from A to B to see the density of connections between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.  Top 10 locations to keep the map small\n",
    "top_locs = packages_df['etablissement_postal'].value_counts().head(10).index\n",
    "top_next = packages_df['next_etablissement_postal'].value_counts().head(10).index\n",
    "\n",
    "# 2. include only these top locations\n",
    "filtered_df = packages_df[\n",
    "    (packages_df['etablissement_postal'].isin(top_locs)) &\n",
    "    (packages_df['next_etablissement_postal'].isin(top_next))\n",
    "]\n",
    "\n",
    "# 3. Create a Matrix (Cross-tabulation)\n",
    "matrix = pd.crosstab(filtered_df['etablissement_postal'], filtered_df['next_etablissement_postal'])\n",
    "\n",
    "# 4. Plot Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(matrix, cmap='Reds', annot=True, fmt='d', linewidths=.5)\n",
    "plt.title(\"Most Frequent Routes (Origin vs Destination) - Packages Dataset\")\n",
    "plt.xlabel(\"Next Destination\")\n",
    "plt.ylabel(\"Current Location\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count packages per location\n",
    "location_counts = packages_df['etablissement_postal'].value_counts().reset_index()\n",
    "location_counts.columns = ['Location', 'Volume']\n",
    "\n",
    "# keep only top 20 busiest centers\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Volume', y='Location', data=location_counts.head(20), palette='viridis')\n",
    "plt.title(\"Top 20 Busiest Postal Centers\")\n",
    "plt.xlabel(\"Number of Packages\")\n",
    "plt.ylabel(\"Center ID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "we notice the same pattern as before with some etablissements having significantly higher trafic compared to others.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Extract Time Features\n",
    "packages_df['hour'] = packages_df['date'].dt.hour\n",
    "packages_df['day_of_week'] = packages_df['date'].dt.day_name()\n",
    "\n",
    "# 2. Create a Pivot Table (Cross-tabulation)\n",
    "# Rows = Day, Cols = Hour, Values = Count of Scans\n",
    "heatmap_data = pd.crosstab(\n",
    "    packages_df['day_of_week'],\n",
    "    packages_df['hour']\n",
    ")\n",
    "\n",
    "days_order = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "heatmap_data = heatmap_data.reindex(days_order)\n",
    "\n",
    "# 3. Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(heatmap_data, cmap='YlOrRd', linewidths=.5, annot=False)\n",
    "plt.title(\"Package Scan Activity by Day and Hour\")\n",
    "plt.xlabel(\"Hour of Day (0-23)\")\n",
    "plt.ylabel(\"Day of Week\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "we notice that the busiest times for package scans are during weekdays, particularly from mid-morning to late afternoon.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of packages per receptacle\n",
    "packages_per_receptacle = packages_df.groupby('RECPTCL_FID')['MAILITM_FID'].nunique()\n",
    "packages_per_receptacle.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "- Drop the packages starting from 2020 and keep only the ones starting from 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in packages df Drop the values that are from 2020 and start only from 2023\n",
    "packages_df = packages_df[packages_df['date'].dt.year >= 2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df['etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "* `etablissement_postal` have 26772 null values (2.7% of the whole dataset)\n",
    "* As its null values are less than 5% of the dataset (2.7%), we drop these null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df = packages_df[~packages_df['etablissement_postal'].isna()]\n",
    "packages_df['etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "* We propose to consider the packages having null `next_etablissement_postal`\n",
    "as having issue during transfer, we'll try to validate that using\n",
    "`EVENT_TYPE_CD` also\n",
    "* Let's check if `EVENT_TYPE_CD` can indicate whether the `next_etablissement_postal` is null or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_unknown_next_etablissement = packages_df[packages_df['next_etablissement_postal'].isna()]\n",
    "# keep only top EVENT_TYPES_ID\n",
    "packages_unknown_next_etablissement = packages_unknown_next_etablissement['EVENT_TYPE_CD'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "packages_unknown_next_etablissement.head(10).plot(kind='bar')\n",
    "plt.xlabel('EVENT TYPE CD')\n",
    "plt.ylabel('Null Next Etablissement Postal')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "* `EVENT_TYPE_CD` doesn't actually indicate null values of `next_etablissement_postal`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "* the function `fill_NaN_next_etab` cell fills the `next_etablissement_postal` using the next `etablissement_postal` for the same package.\n",
    "* if the last route for a specific package is null, then it keeps it null because there's no next `etablissement_postal` for that package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_NaN_next_etab(df, id_col):\n",
    "    # 1. Ensure the dataframe is sorted (same as before)\n",
    "    df = df.sort_values([id_col, 'date'])\n",
    "\n",
    "    # 2. Look ahead to the next row's postal code and ID\n",
    "    shifted_postal = df['etablissement_postal'].shift(-1)\n",
    "    shifted_id = df[id_col].shift(-1)\n",
    "# 3. Identify the \"boundaries\" where the postal code changes within the same package\n",
    "# This marks the last row of a block with the value of the start of the next block\n",
    "    is_boundary = (df['etablissement_postal'] != shifted_postal) & \\\n",
    "              (df[id_col] == shifted_id)\n",
    "# 4. Use grouped backfill to broadcast those values to all preceding rows in the block\n",
    "# This replaces your 'blocks.map' logic with a single vectorized pass\n",
    "    fill_values = shifted_postal.where(is_boundary).groupby(df[id_col]).bfill()\n",
    "\n",
    "# 5. Fill only the NaNs in the existing column to match your original logic\n",
    "    df['next_etablissement_postal'] = df['next_etablissement_postal'].fillna(fill_values)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to fill NaN values in next_etablissement_postal\n",
    "packages_df = fill_NaN_next_etab(packages_df, 'MAILITM_FID')\n",
    "# Check remaining NaNs\n",
    "packages_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "* Like this, we've handled a good part of null values and inconsitencies for `packages` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "* **We'll be doing the same steps for `receptacle` dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df['etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "* Dropping rows having null `etablissement_postal`, as they're just 0.1% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df = receptacles_df[~receptacles_df['etablissement_postal'].isna()]\n",
    "receptacles_df['etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "* apply the function that fills null values of `next_etablissement_postal` using `etablissement_postal` to `receptacles_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df = fill_NaN_next_etab(receptacles_df, 'RECPTCL_FID')\n",
    "# Check remaining NaNs\n",
    "receptacles_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "* Null values are mostly gone, but there are still some illogical packages' and receptacles' routes between `etablissements`\n",
    "* We'll treat these logical routes now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each package (group of rows), check whether there's any illogical route\n",
    "# between 'etablissement_postal' and 'next_etablissement_postal'\n",
    "def isPackageIllogical(group):\n",
    "    return (\n",
    "        group['next_etablissement_postal']\n",
    "        .iloc[:-1]\n",
    "        .ne(group['etablissement_postal'].shift(-1).iloc[:-1])\n",
    "        .any()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "illogical_packages = packages_df.groupby('MAILITM_FID').apply(isPackageIllogical)\n",
    "illogical_packages.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "103376 / packages_df['MAILITM_FID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "* 103376 Packages have illogical routes (98%) of all packages, so it's impossible to drop them, but instead, we plan to ignore the `MAILITM_FID` and `RECPTCL_FID` in the training and testing sets that will come next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each receptacle (group of rows), check whether there's any illogical route\n",
    "# between 'etablissement_postal' and 'next_etablissement_postal'\n",
    "def isReceptacleIllogical(group):\n",
    "    return (\n",
    "        group['next_etablissement_postal']\n",
    "        .iloc[:-1]\n",
    "        .ne(group['etablissement_postal'].shift(-1).iloc[:-1])\n",
    "        .any()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "illogical_receptacles = receptacles_df.groupby('RECPTCL_FID').apply(isReceptacleIllogical)\n",
    "illogical_receptacles.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "205519 / receptacles_df['RECPTCL_FID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "205519 receptacles have illogical routes (95%) of all receptacles, so it's also impossible to drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep copies for backup (en cas ou)\n",
    "packages_df_copy = packages_df.copy()\n",
    "receptacles_df_copy = receptacles_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "# Feature Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "### Check RECPTCL_FID and MAILITM_FID having same length formats\n",
    "if yes then we can split them into meaningfull parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "same=1\n",
    "print(\"\\n=== RECPTCL_FID  ===\")\n",
    "print(f\"testing if the lengths of RECPTCL_FID values are all the same:\")\n",
    "for val in packages_df['RECPTCL_FID'].values:\n",
    "    if len(str(val)) != 29 :\n",
    "        print(f\"  {val} (length: {len(str(val))})\")\n",
    "        same=0\n",
    "        break\n",
    "if same==1:\n",
    "    print('all same length' )\n",
    "\n",
    "same=1\n",
    "print(\"\\n=== MAILITM_FID  ===\")\n",
    "print(f\"testing if the lengths of MAILITM_FID values are all the same:\")\n",
    "for val in packages_df['MAILITM_FID'].values:\n",
    "    if len(str(val)) != 13 :\n",
    "        print(f\"  {val} (length: {len(str(val))})\")\n",
    "        same=0\n",
    "        break\n",
    "if same==1:\n",
    "    print ('all same length' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "### RECPTCL_FID Analysis\n",
    "- **Format:** 29-character string (e.g., `USORDADZALGDAUN30050001900005`)\n",
    "- **Data Quality:** No null values (1,000,000) | 215,867 unique values in receptacle dataset and 45306 unique values in packages dataset\n",
    "- **Extractable Features:**\n",
    "  - Origin Country (2 chars): US, FR, AE, etc.\n",
    "  - Destination Country (2 chars): DZ, AI, AA, etc.\n",
    "\n",
    "### MAILITM_FID Analysis\n",
    "- **Format:** 13-character string according to the S10-12 patern (e.g., `CA000132868US`, `CA000340856PK`)\n",
    "- **Data Quality:** No null values (1,000,000 packages)\n",
    "- **Extractable Features:**\n",
    "  - Service Indicator (2 chars): CA, etc.\n",
    "  - Serial Number (8 chars): 00013286, 00034085, etc.\n",
    "  - Check Digit (1 char): 8, 6, etc.\n",
    "  - Country Code (3 chars, right-stripped): US, PK.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "## Definition of the parser funtions\n",
    "These functions are responsible for spliting the IDs into parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_recptcl_fid(id_str):\n",
    "    origin_country = id_str[0:2]\n",
    "    destination_country = id_str[6:8]\n",
    "    return origin_country, destination_country\n",
    "\n",
    "def parse_mailitm_fid(id_str):\n",
    "    service_indicator = id_str[0:2]\n",
    "    serial_number = id_str[2:11]\n",
    "    country_code = id_str[11:14].strip()\n",
    "    return service_indicator, serial_number, country_code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "### Apply parsing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_receptacles_df = receptacles_df.copy()\n",
    "# parsed_receptacles_df[['origin_country', 'destination_country']] = parsed_receptacles_df['RECPTCL_FID'].apply(lambda x: pd.Series(parse_recptcl_fid(x)))\n",
    "\n",
    "# Faster method:\n",
    "parsed_data = list(receptacles_df['RECPTCL_FID'].apply(parse_recptcl_fid))\n",
    "\n",
    "# Assign to new columns by creating a temporary DataFrame\n",
    "parsed_receptacles_df = receptacles_df.copy()\n",
    "parsed_receptacles_df[['origin_country', 'destination_country']] = pd.DataFrame(\n",
    "    parsed_data, index=receptacles_df.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_packages_df = packages_df.copy()\n",
    "# parsed_packages_df[['service_indicator', 'serial_number', 'country_code']] = parsed_packages_df['MAILITM_FID'].apply(lambda x: pd.Series(parse_mailitm_fid(x)))\n",
    "# parsed_packages_df[['origin_country','destination_country']] = parsed_packages_df['RECPTCL_FID'].apply(lambda x: pd.Series(parse_recptcl_fid(x)))\n",
    "\n",
    "# Faster method:\n",
    "parsed_packages_df = packages_df.copy()\n",
    "\n",
    "# 1. Optimize MAILITM_FID parsing\n",
    "mailitm_data = list(parsed_packages_df['MAILITM_FID'].apply(parse_mailitm_fid))\n",
    "parsed_packages_df[['service_indicator', 'serial_number', 'country_code']] = pd.DataFrame(\n",
    "    mailitm_data, index=parsed_packages_df.index\n",
    ")\n",
    "\n",
    "# 2. Optimize RECPTCL_FID parsing\n",
    "recptcl_data = list(parsed_packages_df['RECPTCL_FID'].apply(parse_recptcl_fid))\n",
    "parsed_packages_df[['origin_country', 'destination_country']] = pd.DataFrame(\n",
    "    recptcl_data, index=parsed_packages_df.index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "### show samples of new parsed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=== packages_df sample with new parsed columns ===\")\n",
    "parsed_packages_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== receptacles_df sample with new parsed columns ===\")\n",
    "parsed_receptacles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "# Analysis of the extrcted features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "## 1. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n--- Unique Value Counts for parsed_packages_df ---\")\n",
    "print(\"\\nFor receptacle FID parsing:\")\n",
    "print(f\"Unique origin_country values: {parsed_packages_df['origin_country'].nunique()}\")\n",
    "print(f\"Unique destination_country values: {parsed_packages_df['destination_country'].nunique()}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"\\nFor mail item FID parsing:\")\n",
    "print(f\"Unique service_indicator values: {parsed_packages_df['service_indicator'].nunique()}\")\n",
    "print(f\"Unique country_code values: {parsed_packages_df['country_code'].nunique()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "## 2. parsed_receptacles_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Unique Value Counts for parsed_receptacles_df ---\")\n",
    "print(f\"Unique origin_country values: {parsed_receptacles_df['origin_country'].nunique()}\")\n",
    "print(f\"Unique destination_country values: {parsed_receptacles_df['destination_country'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "## List values of the new columns obtained from receptacle FID parsing for both parsed dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "### 1. for parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#listing the values \n",
    "print(\"\\n--- Values of origin_country ---\")\n",
    "print(parsed_packages_df['origin_country'].unique())\n",
    "\n",
    "print(\"\\n--- Values of destination_country ---\")\n",
    "print(parsed_packages_df['destination_country'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "### 2. for parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#listing the values \n",
    "print(\"\\n--- Values of origin_country ---\")\n",
    "print(parsed_receptacles_df['origin_country'].unique())\n",
    "\n",
    "print(\"\\n--- Values of destination_country ---\")\n",
    "print(parsed_receptacles_df['destination_country'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "### Do the intersection of origin_country of both dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the intersection of origin_country values in both parsed datasets\n",
    "packages_origin_countries = set(parsed_packages_df['origin_country'].unique())\n",
    "receptacle_origin_countries = set(parsed_receptacles_df['origin_country'].unique())\n",
    "common_origin_countries = packages_origin_countries.intersection(receptacle_origin_countries)\n",
    "print(\"number of common origin_country values in both parsed datasets:\", len(common_origin_countries))\n",
    "print(f\"\\nCommon origin_country values in both paesed datasets: \")\n",
    "print(common_origin_countries)\n",
    "# remaining ones \n",
    "remaining_in_packages = packages_origin_countries - common_origin_countries\n",
    "remaining_in_receptacle = receptacle_origin_countries - common_origin_countries\n",
    "print(f\"Remaining origin_country values only in parsed_packages_df:\")\n",
    "print(remaining_in_packages)\n",
    "print(f\"Remaining origin_country values only in parsed_receptacles_df:\")\n",
    "print(remaining_in_receptacle )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "### list the values of both service indicators and country code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "### 1. service indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Values of service_indicator ---\")\n",
    "print(parsed_packages_df['service_indicator'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    " we can see that there are values that don't follow the standards in the S10-12 format so we need to handle that correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform country_code to uppercase for consistency\n",
    "parsed_packages_df['service_indicator'] = parsed_packages_df['service_indicator'].str.upper()\n",
    "print(\"values of service_indicator after transformation to uppercase:\")\n",
    "print(parsed_packages_df['service_indicator'].unique())\n",
    "print(\"number of unique service indicators after transformation:\", parsed_packages_df['service_indicator'].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "### 2. country code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Values of country codes ---\")\n",
    "\n",
    "print(parsed_packages_df['country_code'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "we can see that many values for the country codes are numbers instead of ISO 3166-1 format these values should be replaced by the values of origin country gotten from the receptacle when doing the preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "### replace them with the correct origin country code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Vectorized string capitalization\n",
    "parsed_packages_df['country_code'] = parsed_packages_df['country_code'].str.upper()\n",
    "\n",
    "# 2. Vectorized comparison to find mismatches\n",
    "mismatch_mask = parsed_packages_df['origin_country'] != parsed_packages_df['country_code']\n",
    "\n",
    "# 3. Count the Trues\n",
    "count = mismatch_mask.sum()\n",
    "\n",
    "print(f\"Number of rows where origin_country does not match country_code: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "### replace them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .loc to find rows where they don't match, and update only the 'country_code' column\n",
    "parsed_packages_df.loc[parsed_packages_df['origin_country'] != parsed_packages_df['country_code'], 'country_code'] = parsed_packages_df['origin_country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the unique values again\n",
    "print(\"\\n--- Values of country codes after correction ---\")\n",
    "print(parsed_packages_df['country_code'].unique())\n",
    "print(\"number of unique country codes after correction:\", parsed_packages_df['country_code'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "## visualization of Origin Country distribution according to number of packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104",
   "metadata": {},
   "source": [
    "### 1. for the parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_country_counts = parsed_packages_df['origin_country'].value_counts().head(20)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.barh(origin_country_counts.index, origin_country_counts.values, color='steelblue')\n",
    "plt.xlabel('Count', fontsize=11)\n",
    "plt.ylabel('Origin Country', fontsize=11)\n",
    "plt.title('Top 20 Origin Countries by packages count', fontsize=12, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "### 2. for the parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_country_counts = parsed_receptacles_df['origin_country'].value_counts().head(20)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.barh(origin_country_counts.index, origin_country_counts.values, color='steelblue')\n",
    "plt.xlabel('Count', fontsize=11)\n",
    "plt.ylabel('Origin Country', fontsize=11)\n",
    "plt.title('Top 20 Origin Countries by receptacle count', fontsize=12, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {},
   "source": [
    "## Visualiation of the service indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_indicator_count = parsed_packages_df['service_indicator'].value_counts().head(20)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.barh(service_indicator_count.index, service_indicator_count.values, color='mediumseagreen')\n",
    "plt.xlabel('Count', fontsize=11)\n",
    "plt.ylabel('Service Indicator', fontsize=11)\n",
    "plt.title('Top 20 Service Indicators by packages count', fontsize=12, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110",
   "metadata": {},
   "source": [
    "# Origin–Destination Flow Analysis\n",
    "\n",
    "This section investigates the flow of receptacles and packages from origin countries to destination. We examine:\n",
    "- packages count by origin country\n",
    "- Top origin countries delivering to each destination\n",
    "- Visual representation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {},
   "source": [
    "### 1. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages count by origin country\n",
    "origin_country_volume = parsed_packages_df['origin_country'].value_counts()\n",
    "print(\"\\n--- Packages count by Origin Country ---\")\n",
    "print(origin_country_volume.head(15))\n",
    "\n",
    "# Create origin_country × destination matrix\n",
    "flow_matrix = pd.crosstab(parsed_packages_df['origin_country'], \n",
    "                           parsed_packages_df['destination_country'])\n",
    "\n",
    "# Keep only top 10 origin countries and top 10 destination countries for readability\n",
    "top_origins = parsed_packages_df['origin_country'].value_counts().head(10).index\n",
    "top_arrivals = parsed_packages_df['destination_country'].value_counts().head(10).index\n",
    "\n",
    "flow_matrix_top = flow_matrix.loc[top_origins, top_arrivals]\n",
    "\n",
    "print(\"\\n--- Origin Country × destination Country (Top 10 × Top 10) ---\")\n",
    "print(flow_matrix_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize origin × destination country flow\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(flow_matrix_top, cmap='YlOrRd', annot=True, fmt='d', \n",
    "            cbar_kws={'label': 'packages Count'}, linewidths=0.5)\n",
    "plt.title('packages Flow: Origin Country × destination country (Top 10 × Top 10)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Arrival Hub', fontsize=12)\n",
    "plt.ylabel('Origin Country', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114",
   "metadata": {},
   "source": [
    "### 2. parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# receptacle count by origin country\n",
    "origin_country_receptacle_volume = parsed_receptacles_df['origin_country'].value_counts()\n",
    "print(\"\\n--- Receptacle count by Origin Country ---\")\n",
    "print(origin_country_receptacle_volume.head(15))\n",
    "\n",
    "destination__receptacle_volume=parsed_receptacles_df['destination_country'].value_counts()\n",
    "print(\"\\n--- Receptacle count by destination ---\")\n",
    "print(destination__receptacle_volume.head(15))\n",
    "\n",
    "# Create origin_country × destination matrix\n",
    "receptacle_flow_matrix = pd.crosstab(parsed_receptacles_df['origin_country'], \n",
    "                           parsed_receptacles_df['destination_country'])\n",
    "\n",
    "# Keep only top 10 origin countries and top 10 destination countries for readability\n",
    "receptacle_top_origins = parsed_receptacles_df['origin_country'].value_counts().head(10).index\n",
    "receptacle_top_arrivals = parsed_receptacles_df['destination_country'].value_counts().head(10).index\n",
    "\n",
    "receptacle_flow_matrix_top = receptacle_flow_matrix.loc[receptacle_top_origins, receptacle_top_arrivals]\n",
    "\n",
    "print(\"\\n--- Origin Country × destination Country (Top 10 × Top 10) ---\")\n",
    "print(receptacle_flow_matrix_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize origin × destination country flow\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(receptacle_flow_matrix_top, cmap='YlOrRd', annot=True, fmt='d', \n",
    "            cbar_kws={'label': 'receptacles Count'}, linewidths=0.5)\n",
    "plt.title('receptacles Flow: Origin Country × destination country (Top 10 × Top 10)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Arrival Hub', fontsize=12)\n",
    "plt.ylabel('Origin Country', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117",
   "metadata": {},
   "source": [
    "### create pairs (origin, destination) for more detailed analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_packages_df['origin_destination'] = parsed_packages_df['origin_country'] + '_' + parsed_packages_df['destination_country']\n",
    "parsed_receptacles_df['origin_destination'] = parsed_receptacles_df['origin_country'] + '_' + parsed_receptacles_df['destination_country']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {},
   "source": [
    "listing the obtained values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('(origin_destination) pairs obtained for ')\n",
    "print(\"\\nfor parsed_packages_df :\")\n",
    "print(\"\\nNumber of unique values\", parsed_packages_df['origin_destination'].nunique())\n",
    "print(\"\\nValues : \")\n",
    "print(parsed_packages_df['origin_destination'].unique())\n",
    "print(\"\\nfor parsed_receptacles_df :\")\n",
    "print(\"\\nNumber of unique values\", parsed_receptacles_df['origin_destination'].nunique())\n",
    "print(\"\\nValues : \")\n",
    "print(parsed_receptacles_df['origin_destination'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121",
   "metadata": {},
   "source": [
    "### visualization of obtained results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122",
   "metadata": {},
   "source": [
    "### 1. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_dest_counts = parsed_packages_df['origin_destination'].value_counts()\n",
    "top_origin_dest_counts= origin_dest_counts.head(15)\n",
    "plt.figure(figsize=(14, 7))\n",
    "top_origin_dest_counts.plot(kind='bar')\n",
    "plt.title('Histogram of (origin, destination) Pairs Based on Package Counts')\n",
    "plt.xlabel('Origin_Destination')\n",
    "plt.ylabel('Number of Packages')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124",
   "metadata": {},
   "source": [
    "\n",
    "### 2. parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_dest_counts = parsed_receptacles_df['origin_destination'].value_counts()\n",
    "top_origin_dest_counts = origin_dest_counts.head(15)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "top_origin_dest_counts.plot(kind='bar')\n",
    "plt.title('Histogram of (origin, destination) Pairs Based on receptacle Counts')\n",
    "plt.xlabel('Origin_Destination')\n",
    "plt.ylabel('Number of receptacles')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126",
   "metadata": {},
   "source": [
    "## origin_destination X etablissments analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {},
   "source": [
    "## 1. current etablissment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128",
   "metadata": {},
   "source": [
    "### a. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_packages_df: visualize the histogram of counts by (origin_destination, etablissement_postal) pairs\n",
    "\n",
    "# First, select top N most common (origin_destination, etablissement_postal) pairs for clarity\n",
    "pair_counts_current = parsed_packages_df.groupby(['origin_destination', 'etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_current = pair_counts_current.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_current,\n",
    "    x='count',\n",
    "    y=top_pairs_current.apply(lambda x: f\"{x['origin_destination']} | {x['etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin-Destination, Current Etablissement) Pairs by Package Counts')\n",
    "plt.xlabel('Number of Packages')\n",
    "plt.ylabel('(Origin_Destination | Current Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130",
   "metadata": {},
   "source": [
    "### b. parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_receptacles_dffffff: visualize the histogram of counts by (origin_destination, etablissement_postal) pairs\n",
    "\n",
    "# First, select top N most common (origin_destination, etablissement_postal) pairs for clarity\n",
    "pair_counts_current_receptacle = parsed_receptacles_df.groupby(['origin_destination', 'etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_current_receptacle = pair_counts_current_receptacle.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_current_receptacle,\n",
    "    x='count',\n",
    "    y=top_pairs_current_receptacle.apply(lambda x: f\"{x['origin_destination']} | {x['etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin-Destination, Current Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Origin_Destination | Current Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132",
   "metadata": {},
   "source": [
    "we can see that the ETAB0002 is dominating and we remark that when the destination is DZ\n",
    "we'll try to confirm that by taking into consideration the destination only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_receptacles_dffff: visualize the histogram of counts by (destination_country, etablissement_postal) pairs\n",
    "\n",
    "# First, select top N most common (destination_country, etablissement_postal) pairs for clarity\n",
    "pair_counts_dest_receptacle = parsed_receptacles_df.groupby(['destination_country', 'etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_dest_receptacle = pair_counts_dest_receptacle.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_dest_receptacle,\n",
    "    x='count',\n",
    "    y=top_pairs_dest_receptacle.apply(lambda x: f\"{x['destination_country']} | {x['etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Destination Country, Current Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Destination Country | Current Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134",
   "metadata": {},
   "source": [
    "### This is to test the origin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_receptacles_dffffff: visualize the histogram of counts by (origin_country, etablissement_postal) pairs\n",
    "\n",
    "# First, select top N most common (origin_country, etablissement_postal) pairs for clarity\n",
    "pair_counts_origin_receptacle = parsed_receptacles_df.groupby(['origin_country', 'etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_origin_receptacle = pair_counts_origin_receptacle.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_origin_receptacle,\n",
    "    x='count',\n",
    "    y=top_pairs_origin_receptacle.apply(lambda x: f\"{x['origin_country']} | {x['etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin Country, Current Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Origin Country | Current Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136",
   "metadata": {},
   "source": [
    "most of the values with ETAB0002 values are european countries in addition to AE and China(CN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137",
   "metadata": {},
   "source": [
    "## 2. Next etablissement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138",
   "metadata": {},
   "source": [
    "### a. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_packages_df: visualize the histogram of counts by (origin_destination, next_etablissement_postal)\n",
    "\n",
    "# First, select top N most common (origin_destination, next_etablissement_postal) pairs for clarity\n",
    "pair_counts = parsed_packages_df.groupby(['origin_destination', 'next_etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs = pair_counts.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs,\n",
    "    x='count',\n",
    "    y=top_pairs.apply(lambda x: f\"{x['origin_destination']} | {x['next_etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin-Destination, Next Etablissement) Pairs by Package Counts')\n",
    "plt.xlabel('Number of Packages')\n",
    "plt.ylabel('(Origin_Destination | Next Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140",
   "metadata": {},
   "source": [
    "### b. parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_receptacles_dffff: visualize the histogram of counts by (origin_destination, next_etablissement_postal)\n",
    "\n",
    "# First, select top N most common (origin_destination, next_etablissement_postal) pairs for clarity\n",
    "pair_counts_recept = parsed_receptacles_df.groupby(['origin_destination', 'next_etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_recept = pair_counts_recept.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_recept,\n",
    "    x='count',\n",
    "    y=top_pairs_recept.apply(lambda x: f\"{x['origin_destination']} | {x['next_etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin-Destination, Next Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Origin_Destination | Next Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142",
   "metadata": {},
   "source": [
    "Do for origin and for destination separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For parsed_receptacles_df: visualize the histogram of counts by destination_country and next_etablissement_postal\n",
    "\n",
    "# First, select top N most common (destination_country, next_etablissement_postal) pairs for clarity\n",
    "pair_counts_dest = parsed_receptacles_df.groupby(['destination_country', 'next_etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_dest = pair_counts_dest.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_dest,\n",
    "    x='count',\n",
    "    y=top_pairs_dest.apply(lambda x: f\"{x['destination_country']} | {x['next_etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Destination Country, Next Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Destination_Country | Next Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_receptacles_df: visualize the histogram of counts by origin_country and next_etablissement_postal\n",
    "\n",
    "# First, select top N most common (origin_country, next_etablissement_postal) pairs for clarity\n",
    "pair_counts_origin = parsed_receptacles_df.groupby(['origin_country', 'next_etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_origin = pair_counts_origin.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_origin,\n",
    "    x='count',\n",
    "    y=top_pairs_origin.apply(lambda x: f\"{x['origin_country']} | {x['next_etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin Country, Next Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Origin_Country | Next Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145",
   "metadata": {},
   "source": [
    "from the analysis we can see that there are some etablissments that get congested forming a sort of loop (ETAB0030, ETAB0002, ETAB0006)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146",
   "metadata": {},
   "source": [
    "### Time analysis regarding origin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147",
   "metadata": {},
   "source": [
    "### 1. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the parsed_packages_df by date and origin_country to count packages per month per origin country\n",
    "parsed_packages_df['date'] = pd.to_datetime(parsed_packages_df['date'])\n",
    "\n",
    "# Group by origin_country and resample by month\n",
    "ts_by_origin = parsed_packages_df.groupby(['origin_country', pd.Grouper(key='date', freq='ME')])['MAILITM_FID'].count().reset_index()\n",
    "\n",
    "# Choose top 5 origin countries by total count for plotting\n",
    "top_origin = parsed_packages_df['origin_country'].value_counts().head(5).index\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "for origin in top_origin:\n",
    "    ts = ts_by_origin[ts_by_origin['origin_country'] == origin]\n",
    "    plt.plot(ts['date'], ts['MAILITM_FID'], marker='o', label=origin)\n",
    "\n",
    "plt.title('Monthly Package count by Origin Country')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Packages')\n",
    "plt.legend(title='Origin Country')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149",
   "metadata": {},
   "source": [
    "### 2. parsed_receptacles_dfffff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the parsed_receptacles_df by date and origin_country to count receptacles per month per origin country\n",
    "parsed_receptacles_df['date'] = pd.to_datetime(parsed_receptacles_df['date'])\n",
    "\n",
    "# Group by origin_country and resample by month\n",
    "ts_by_origin_recept = parsed_receptacles_df.groupby(['origin_country', pd.Grouper(key='date', freq='ME')])['RECPTCL_FID'].count().reset_index()\n",
    "\n",
    "# Choose top 5 origin countries by total count for plotting\n",
    "top_origin_recept = parsed_receptacles_df['origin_country'].value_counts().head(5).index\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "for origin in top_origin_recept:\n",
    "    ts = ts_by_origin_recept[ts_by_origin_recept['origin_country'] == origin]\n",
    "    plt.plot(ts['date'], ts['RECPTCL_FID'], marker='o', label=origin)\n",
    "\n",
    "plt.title('Monthly Receptacle count by Origin Country')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Receptacles')\n",
    "plt.legend(title='Origin Country')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151",
   "metadata": {},
   "source": [
    "### Time analysis by destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the parsed_packages_df by date and destination_country to count packages per month per destination country\n",
    "parsed_packages_df['date'] = pd.to_datetime(parsed_packages_df['date'])\n",
    "\n",
    "# Group by destination_country and resample by month\n",
    "ts_by_dest = parsed_packages_df.groupby(['destination_country', pd.Grouper(key='date', freq='ME')])['MAILITM_FID'].count().reset_index()\n",
    "\n",
    "# Choose top 5 destination countries by total count for plotting\n",
    "top_dest = parsed_packages_df['destination_country'].value_counts().head(5).index\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "for dest in top_dest:\n",
    "    ts = ts_by_dest[ts_by_dest['destination_country'] == dest]\n",
    "    plt.plot(ts['date'], ts['MAILITM_FID'], marker='o', label=dest)\n",
    "\n",
    "plt.title('Monthly Package count by Destination Country')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Packages')\n",
    "plt.legend(title='Destination Country')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153",
   "metadata": {},
   "source": [
    "### 2. parsed_receptacles_dffff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the parsed_receptacles_df by date and destination_country to count receptacles per month per destination country\n",
    "parsed_receptacles_df['date'] = pd.to_datetime(parsed_receptacles_df['date'])\n",
    "\n",
    "# Group by destination_country and resample by month \n",
    "ts_by_dest_recept = parsed_receptacles_df.groupby(['destination_country', pd.Grouper(key='date', freq='ME')])['RECPTCL_FID'].count().reset_index()\n",
    "\n",
    "# Choose top 5 destination countries by total count for plotting\n",
    "top_dest_recept = parsed_receptacles_df['destination_country'].value_counts().head(5).index\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "for dest in top_dest_recept:\n",
    "    ts = ts_by_dest_recept[ts_by_dest_recept['destination_country'] == dest]\n",
    "    plt.plot(ts['date'], ts['RECPTCL_FID'], marker='o', label=dest)\n",
    "\n",
    "plt.title('Monthly Receptacle count by Destination Country')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Receptacles')\n",
    "plt.legend(title='Destination Country')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155",
   "metadata": {},
   "source": [
    "## Creating additional features \n",
    " 1. 'flow_type' column with values: 'inbound' (to DZ), 'outbound' (from DZ), 'local' (DZ to DZ), otherwise 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_flow_type(df):\n",
    "    # Define the conditions\n",
    "    conditions = [\n",
    "        (df['destination_country'] == 'DZ') & (df['origin_country'] == 'DZ'), # local\n",
    "        (df['destination_country'] == 'DZ'),                                # inbound\n",
    "        (df['origin_country'] == 'DZ')                                     # outbound\n",
    "    ]\n",
    "    \n",
    "    # Define the results for each condition\n",
    "    choices = ['local', 'inbound', 'outbound']\n",
    "    \n",
    "    # Apply logic with 'other' as the default\n",
    "    return np.select(conditions, choices, default='other')\n",
    "\n",
    "# Apply to both DataFrames instantly\n",
    "parsed_packages_df['flow_type'] = get_flow_type(parsed_packages_df)\n",
    "parsed_receptacles_df['flow_type'] = get_flow_type(parsed_receptacles_df)\n",
    "\n",
    "# Print counts\n",
    "print(\"Flow type counts in parsed_packages_df:\\n\", parsed_packages_df['flow_type'].value_counts())\n",
    "print(\"\\nFlow type counts in parsed_receptacles_df:\\n\", parsed_receptacles_df['flow_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157",
   "metadata": {},
   "source": [
    "we can see that there are some values of flow type with the type \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 rows of origin_country and destination_country for flow_type 'other' in parsed_receptacles_df\n",
    "print(parsed_receptacles_df.loc[parsed_receptacles_df['flow_type'] == 'other', ['origin_country', 'destination_country']].head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159",
   "metadata": {},
   "source": [
    "## Analysis of the relation between the flow_type and the event_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160",
   "metadata": {},
   "source": [
    "## 1. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Analyse relation between flow_type and EVENT_TYPE_CD in parsed_packages_df\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "ax = sns.countplot(data=parsed_packages_df, x=\"EVENT_TYPE_CD\", hue=\"flow_type\", palette='Set2')\n",
    "plt.title(\"Distribution of EVENT_TYPE_CD by flow_type in parsed_packages_df\")\n",
    "plt.xlabel(\"EVENT_TYPE_CD\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"Flow Type\")\n",
    "plt.tight_layout()\n",
    "for c in ax.containers:\n",
    "    ax.bar_label(c, label_type='edge', fontsize=8, padding=2)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162",
   "metadata": {},
   "source": [
    "## 2. parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each flow_type, list the unique EVENT_TYPE_CD values and the most frequent EVENT_TYPE_CD value\n",
    "for flow in parsed_receptacles_df['flow_type'].unique():\n",
    "    event_types = parsed_receptacles_df.loc[parsed_receptacles_df['flow_type'] == flow, 'EVENT_TYPE_CD'].unique()\n",
    "    most_common_event_type = parsed_receptacles_df.loc[parsed_receptacles_df['flow_type'] == flow, 'EVENT_TYPE_CD'].mode()\n",
    "    print(f\"Flow type: {flow}\")\n",
    "    print(f\"EVENT_TYPE_CD values: {sorted(event_types)}\")\n",
    "    if not most_common_event_type.empty:\n",
    "        print(f\"Most frequent EVENT_TYPE_CD: {most_common_event_type.iloc[0]}\")\n",
    "    else:\n",
    "        print(\"No EVENT_TYPE_CD available\")\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "ax = sns.countplot(data=parsed_receptacles_df, x=\"EVENT_TYPE_CD\", hue=\"flow_type\", palette='Set1')\n",
    "plt.title(\"Distribution of EVENT_TYPE_CD by flow_type in parsed_receptacles_df\")\n",
    "plt.xlabel(\"EVENT_TYPE_CD\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"Flow Type\")\n",
    "plt.tight_layout()\n",
    "\n",
    "for c in ax.containers:\n",
    "    ax.bar_label(c, label_type='edge', fontsize=8, padding=2)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165",
   "metadata": {},
   "source": [
    "we can see that there are major event types related to the inbound flow type( coming to DZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166",
   "metadata": {},
   "source": [
    "# Track multiple receptacles just to see the flow of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track multiple receptacles: visualize all events for 5 different RECPTCL_FID in parsed_receptacles_df\n",
    "# Pick 5 unique RECPTCL_FID values to demonstrate\n",
    "num_examples = 5\n",
    "example_receptacle_ids = parsed_receptacles_df['RECPTCL_FID'].drop_duplicates().iloc[:num_examples]\n",
    "for rid in example_receptacle_ids:\n",
    "    print(\"\\n--- Events for RECPTCL_FID:\", rid, \"---\")\n",
    "    display(parsed_receptacles_df[parsed_receptacles_df['RECPTCL_FID'] == rid][['RECPTCL_FID', 'date', 'EVENT_TYPE_CD', 'etablissement_postal', 'next_etablissement_postal']].sort_values('date').reset_index(drop=True))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "for i, receptacle_id in enumerate(example_receptacle_ids):\n",
    "    ex_df = parsed_receptacles_df[parsed_receptacles_df['RECPTCL_FID'] == receptacle_id].sort_values('date')\n",
    "    plt.plot(\n",
    "        ex_df['date'],\n",
    "        ex_df['EVENT_TYPE_CD'],\n",
    "        marker='o',\n",
    "        label=f\"RECPTCL_FID: {receptacle_id}\"\n",
    "    )\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(f\"Event Timeline (EVENT_TYPE_CD) for {num_examples} Receptacles\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"EVENT_TYPE_CD\")\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df=parsed_packages_df.copy()\n",
    "receptacles_df=parsed_receptacles_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169",
   "metadata": {},
   "source": [
    "- we need to delete the first record of each package to not take into consideration the time it was inside its receptacle since this time would be calculated separatly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the first row for each package \n",
    "packages_df = packages_df.sort_values(['MAILITM_FID', 'date'])\n",
    "packages_df = packages_df.groupby('MAILITM_FID').apply(lambda x: x.iloc[1:]).reset_index(drop=True)\n",
    "packages_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the last row for each receptacle based on date\n",
    "last_rows = receptacles_df.loc[receptacles_df.groupby('RECPTCL_FID')['date'].idxmax()]\n",
    "# print unique last rows based on origin_destination\n",
    "last_rows = last_rows[last_rows['origin_destination']=='FR_DZ']\n",
    "last_rows.head(20)\n",
    "last_rows.value_counts('etablissement_postal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172",
   "metadata": {},
   "source": [
    "# **ATTENTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173",
   "metadata": {},
   "source": [
    "* ... **Add feature engineering starting from here, as the next part is the splitting part, and it needs to be done to the whole datasets** ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175",
   "metadata": {},
   "source": [
    "### Adding the target `delay`\n",
    "* In the following cell, we're adding the target `delay` for each row.\n",
    "* for rows (of the same package/receptacle) having `next_etablissement_postal` different than `etablissement_postal` of their next row, we keep the value of `delay` NaN.\n",
    "* otherwise, we get the difference of `date` of the row and its next (of the same package/receptacle) in `hours` and store it in `delay`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_target(df):\n",
    "\n",
    "    # 1. Sort the entire dataset chronologically\n",
    "    df = df.sort_values(by=['MAILITM_FID', 'date'], ascending=True)\n",
    "\n",
    "    # ... ADD THE DELAY TARGET ...\n",
    "\n",
    "    df['next_event_date'] = df.groupby('MAILITM_FID')['date'].shift(-1)\n",
    "    df['delay'] = (df['next_event_date'] - df['date']).dt.total_seconds() / 3600\n",
    "    # Create logical consistency mask\n",
    "    df['next_row_etab'] = df.groupby('MAILITM_FID')['etablissement_postal'].shift(-1)\n",
    "    valid_delay_mask = (df['next_etablissement_postal'] == df['next_row_etab'])\n",
    "\n",
    "    df.loc[~valid_delay_mask, 'delay'] = np.nan\n",
    "    df = df.drop(columns=['next_event_date', 'next_row_etab']) # to keep only to the original features.\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df = add_target(packages_df)\n",
    "#receptacles_df = add_target(receptacles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_delay_pkg = packages_df[packages_df['delay'] > 1000]\n",
    "large_delay_pkg.sort_values('delay', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179",
   "metadata": {},
   "source": [
    "# Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180",
   "metadata": {},
   "source": [
    "* Splitting `packages` and dataset into training and testing sets using `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "packages_df = packages_df.sort_values(by=['date'])\n",
    "pkg_X_train, pkg_X_test, = train_test_split(\n",
    "    packages_df,\n",
    "    test_size=0.2, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print (f\"Training set size: {pkg_X_train.shape[0]} rows\")\n",
    "print (f\"Testing set size: {pkg_X_test.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182",
   "metadata": {},
   "source": [
    "### Handling NaN values of `delay`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg_X_train['delay'].isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg_X_test['delay'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185",
   "metadata": {},
   "source": [
    "* Splitting `receptacles` and dataset into training and testing sets using `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Step 1: Train/Test split (80/20)\n",
    "# rcp_X_train, rcp_X_test, = train_test_split(\n",
    "#     receptacles_df,\n",
    "#     test_size=0.2, \n",
    "#     random_state=42,\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# print (f\"Training set size: {rcp_X_train.shape[0]} rows\")\n",
    "# print (f\"Testing set size: {rcp_X_test.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_etab_mapping(df):\n",
    "    # 1. Calculate the global mode once (to use as a safe fallback)\n",
    "    global_mode = df['next_etablissement_postal'].mode().iat[0]\n",
    "\n",
    "    # 2. Get the mode for every group at once\n",
    "    # This creates a Series where index = etablissement, value = most frequent next\n",
    "    modes_per_group = df.groupby('etablissement_postal')['next_etablissement_postal'].agg(\n",
    "        lambda x: x.mode().iat[0] if not x.mode().empty else global_mode\n",
    "    )\n",
    "\n",
    "    # 3. Convert to dictionary\n",
    "    etablissement_dict = modes_per_group.to_dict()\n",
    "    return etablissement_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188",
   "metadata": {},
   "source": [
    "* After splitting, for each pair `(X_train, X_test)` of `packages_splits`, we'll fill the null values of `next_etablissement_postal` based on the most frequent value of `next_etablissement_postal` appearing with the value of `etablissement_postal` of each specific row having a null value in `next_etablissement_postal`\n",
    "* Example:\n",
    "say that a row has a null `next_etablissement_postal`, we look at it's `etablissement_postal` value (say `v`), we iterate through the training set, we count how many times each `ETAB_XXXX` in ` next_etablissement_postal` appears with `v` being in `etablissement_postal`, we take the mode, and we use it fill all rows having null value at `next_etablissement_postal` where their `etablissement_postal` is `v`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg_etablissement_dict = get_etab_mapping(pkg_X_train)\n",
    "# rcp_etablissement_dict = get_etab_mapping(rcp_X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190",
   "metadata": {},
   "source": [
    "* fill remaining null values of `next_etablissement_postal` with most frequent values of the training set only, avoiding *data leakage*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_mask = pkg_X_train['next_etablissement_postal'].isna()\n",
    "\n",
    "# Apply to original dataframe directly\n",
    "pkg_X_train.loc[null_mask, 'next_etablissement_postal'] = (\n",
    "    pkg_X_train.loc[null_mask, 'etablissement_postal'].map(pkg_etablissement_dict)\n",
    ")\n",
    "\n",
    "pkg_X_train['next_etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_mask = pkg_X_test['next_etablissement_postal'].isna()\n",
    "\n",
    "# Apply to original dataframe directly\n",
    "pkg_X_test.loc[null_mask, 'next_etablissement_postal'] = (\n",
    "    pkg_X_test.loc[null_mask, 'etablissement_postal'].map(pkg_etablissement_dict)\n",
    ")\n",
    "\n",
    "pkg_X_test['next_etablissement_postal'].isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193",
   "metadata": {},
   "source": [
    "* 11 rows still have NaN `next_etablissement_postal` because some values in the test set didn't exist in the training set, therefore they didn't find the right etablissement to map to\n",
    "* we'll fill them with `Unknown`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg_X_test['next_etablissement_postal'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195",
   "metadata": {},
   "source": [
    "- delete the rows with null delay(this will delete the rows with unknow next etablissment values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows with NaN delay\n",
    "pkg_X_train = pkg_X_train[~pkg_X_train['delay'].isna()]\n",
    "pkg_X_train['delay'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows with NaN delay\n",
    "pkg_X_test = pkg_X_test[~pkg_X_test['delay'].isna()]\n",
    "pkg_X_test['delay'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198",
   "metadata": {},
   "source": [
    "* apply to `receptacles` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# null_mask = rcp_X_train['next_etablissement_postal'].isna()\n",
    "\n",
    "# # Apply to original dataframe directly\n",
    "# rcp_X_train.loc[null_mask, 'next_etablissement_postal'] = (\n",
    "#     rcp_X_train.loc[null_mask, 'etablissement_postal'].map(rcp_etablissement_dict)\n",
    "# )\n",
    "\n",
    "# rcp_X_train['next_etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# null_mask = rcp_X_test['next_etablissement_postal'].isna()\n",
    "\n",
    "# # Apply to original dataframe directly\n",
    "# rcp_X_test.loc[null_mask, 'next_etablissement_postal'] = (\n",
    "#     rcp_X_test.loc[null_mask, 'etablissement_postal'].map(rcp_etablissement_dict)\n",
    "# )\n",
    "\n",
    "# rcp_X_test['next_etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201",
   "metadata": {},
   "source": [
    "# New features \n",
    "  - Added here to avoid data leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202",
   "metadata": {},
   "source": [
    "- This is the etablissement congestion durung the 4h hours window(subject to modification to see performance change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_etab_load_1h(df):\n",
    "    # 1. Store the original order/index\n",
    "    df = df.copy()\n",
    "    df['original_index'] = df.index \n",
    "    \n",
    "    # 2. Sort by etab and date for the rolling calculation\n",
    "    # We keep the unique original_index to map values back correctly\n",
    "    df_sorted = df.sort_values(['etablissement_postal', 'date'])\n",
    "    \n",
    "    # 3. Calculate rolling count\n",
    "    # We use 'date' as the window, but we keep it in the index alongside the original_index\n",
    "    rolling_series = (\n",
    "        df_sorted.set_index('date')\n",
    "        .groupby('etablissement_postal')['MAILITM_FID']\n",
    "        .rolling('1h', closed='left')\n",
    "        .count()\n",
    "    )\n",
    "    \n",
    "    # 4. Map it back safely\n",
    "    # We reset the index of rolling_series to get a flat dataframe\n",
    "    # Then we align it back to the original dataframe\n",
    "    rolling_df = rolling_series.reset_index()\n",
    "    \n",
    "    # Since rolling and groupby can reorder rows, we merge on \n",
    "    # the specific columns to ensure every package gets its correct count\n",
    "    # To handle duplicates, we add a temporary 'sequence' within each millisecond\n",
    "    df_sorted['temp_seq'] = df_sorted.groupby(['etablissement_postal', 'date']).cumcount()\n",
    "    rolling_df['temp_seq'] = rolling_df.groupby(['etablissement_postal', 'date']).cumcount()\n",
    "    \n",
    "    df_final = pd.merge(\n",
    "        df_sorted, \n",
    "        rolling_df.rename(columns={'MAILITM_FID': 'etab_load_1h'}),\n",
    "        on=['etablissement_postal', 'date', 'temp_seq'],\n",
    "        how='left'\n",
    "    )\n",
    "    df_final['etab_load_1h'] = df_final['etab_load_1h'].fillna(0)\n",
    "    \n",
    "    return df_final.drop(columns=['temp_seq', 'original_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg_X_train = calculate_etab_load_1h(pkg_X_train)\n",
    "pkg_X_test = calculate_etab_load_1h(pkg_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205",
   "metadata": {},
   "source": [
    "- This is the route load in past 4h (we can modify the time window to see if there would be improvments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_route_load_1h(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Sort by route and date\n",
    "    # We use next_etablissement_postal which you've already filled\n",
    "    df_sorted = df.sort_values(['etablissement_postal', 'next_etablissement_postal', 'date'])\n",
    "    \n",
    "    # 2. Calculate rolling count for the specific LANE\n",
    "    rolling_series = (\n",
    "        df_sorted.set_index('date')\n",
    "        .groupby(['etablissement_postal', 'next_etablissement_postal'])['MAILITM_FID']\n",
    "        .rolling('1h', closed='left')\n",
    "        .count()\n",
    "    )\n",
    "    \n",
    "    # 3. Flatten and prepare for merge\n",
    "    rolling_df = rolling_series.reset_index()\n",
    "    \n",
    "    # Handle duplicates with sequence counts\n",
    "    df_sorted['temp_seq'] = df_sorted.groupby(['etablissement_postal', 'next_etablissement_postal', 'date']).cumcount()\n",
    "    rolling_df['temp_seq'] = rolling_df.groupby(['etablissement_postal', 'next_etablissement_postal', 'date']).cumcount()\n",
    "    \n",
    "    # 4. Merge back\n",
    "    df_final = pd.merge(\n",
    "        df_sorted, \n",
    "        rolling_df.rename(columns={'MAILITM_FID': 'route_load_1h'}),\n",
    "        on=['etablissement_postal', 'next_etablissement_postal', 'date', 'temp_seq'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Fill NaNs with 0 (No other packages on this lane in the last 1h)\n",
    "    df_final['route_load_1h'] = df_final['route_load_1h'].fillna(0)\n",
    "    \n",
    "    return df_final.drop(columns=['temp_seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg_X_train= calculate_route_load_1h(pkg_X_train)\n",
    "pkg_X_test= calculate_route_load_1h(pkg_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208",
   "metadata": {},
   "source": [
    "- Adding the time since last scan feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_since_last_scan(df):\n",
    "    # Ensure we are looking at the same package in the right order\n",
    "    df = df.sort_values(['MAILITM_FID', 'date'])\n",
    "    \n",
    "    # 1. TIME SINCE LAST SCAN \n",
    "    # This is NOT cumulative. \n",
    "    df['time_since_last_scan'] = df.groupby('MAILITM_FID')['date'].diff().dt.total_seconds() / 3600\n",
    "    \n",
    "    df['time_since_last_scan'] = df['time_since_last_scan'].fillna(0)\n",
    "    return df\n",
    "\n",
    "# Apply\n",
    "pkg_X_train = add_time_since_last_scan(pkg_X_train)\n",
    "pkg_X_test = add_time_since_last_scan(pkg_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg_X_train['month'] = pkg_X_train['date'].dt.month\n",
    "pkg_X_test['month'] = pkg_X_test['date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df in [pkg_X_train, pkg_X_test]:\n",
    "#     # 4 = Friday, 5 = Saturday\n",
    "#     df['is_weekend'] = df['date'].dt.dayofweek.isin([4, 5]).astype(int)\n",
    "#     df['first_last_week_day']= df['date'].dt.dayofweek.isin([3,6]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkg_X_train['country_service'] = pkg_X_train['origin_country'].astype(str) + \"_\" + pkg_X_train['service_indicator'].astype(str)\n",
    "# pkg_X_test['country_service'] = pkg_X_test['origin_country'].astype(str) + \"_\" + pkg_X_test['service_indicator'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213",
   "metadata": {},
   "source": [
    "- Adding features that represents events that could affect the flow of packages in algeria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "\n",
    "# 1. Setup Algeria Holidays (Keep this outside the function for speed)\n",
    "al_holidays = holidays.Algeria(years=[2023, 2024, 2025, 2026])\n",
    "holiday_dates = sorted(al_holidays.keys())\n",
    "holiday_df = pd.DataFrame({'holiday_date': pd.to_datetime(holiday_dates)})\n",
    "\n",
    "def add_holidays_features(df):\n",
    "    # Ensure Date is datetime and SORTED (merge_asof requires sorting)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').copy() # .copy() avoids SettingWithCopy warnings\n",
    "\n",
    "    # 3. Calculate \"Days Since Last Holiday\"\n",
    "    # direction='backward' looks for the last holiday <= current date\n",
    "    df = pd.merge_asof(df, holiday_df, left_on='date', right_on='holiday_date', direction='backward')\n",
    "    df['days_since_last_holiday'] = (df['date'] - df['holiday_date']).dt.days\n",
    "    df = df.drop(columns=['holiday_date']) # Drop it so the next merge doesn't conflict\n",
    "\n",
    "    # 4. Calculate \"Days Until Next Holiday\"\n",
    "    # direction='forward' looks for the next holiday >= current date\n",
    "    df = pd.merge_asof(df, holiday_df, left_on='date', right_on='holiday_date', direction='forward')\n",
    "    df['days_until_next_holiday'] = (df['holiday_date'] - df['date']).dt.days\n",
    "    df = df.drop(columns=['holiday_date'])\n",
    "\n",
    "    # 5. Clean up and Cap\n",
    "    df[['days_since_last_holiday', 'days_until_next_holiday']] = df[['days_since_last_holiday', 'days_until_next_holiday']].fillna(30)\n",
    "    \n",
    "    df['days_since_last_holiday'] = df['days_since_last_holiday'].clip(upper=30)\n",
    "    df['days_until_next_holiday'] = df['days_until_next_holiday'].clip(upper=30)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg_X_train = add_holidays_features(pkg_X_train)\n",
    "pkg_X_test = add_holidays_features(pkg_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg_X_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#large packages delays\n",
    "large_delay_pkg = pkg_X_train[pkg_X_train['delay'] > 100]\n",
    "large_delay_pkg.sort_values('delay', ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218",
   "metadata": {},
   "source": [
    "# Use CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219",
   "metadata": {},
   "source": [
    "### Using CatBoost with RMSE as the loss function\n",
    "- parameters are manually defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from catboost import CatBoostRegressor, Pool, cv\n",
    "pkg_X_train = pkg_X_train.sort_values('date').reset_index(drop=True)\n",
    "# --- Define Model Parameters and Features ---\n",
    "TARGET_COL = 'delay'\n",
    "\n",
    "# List of categorical features\n",
    "cat_features = [\n",
    "    'etablissement_postal',\n",
    "    'next_etablissement_postal',\n",
    "    'service_indicator',\n",
    "    'day_of_week',\n",
    "    'origin_destination',\n",
    "    'EVENT_TYPE_CD',\n",
    "    'hour',\n",
    "    'month',\n",
    "    'is_weekend',\n",
    "    'first_last_week_day',\n",
    "    'country_service',\n",
    "\n",
    "    # Add any other categorical columns here\n",
    "]\n",
    "\n",
    "# Columns to drop from features (IDs, date used for sorting, and the target)\n",
    "drop_cols = [TARGET_COL, 'MAILITM_FID', 'RECPTCL_FID', 'date', 'serial_number','flow_type','country_code','destination_country','origin_country']\n",
    "\n",
    "# Prepare the full 80% training set features and target\n",
    "X_train_full = pkg_X_train.drop(columns=[c for c in drop_cols if c in pkg_X_train.columns])\n",
    "y_train_full = pkg_X_train[TARGET_COL]\n",
    "\n",
    "# Identify categorical features by name\n",
    "categorical_feature_names = [c for c in cat_features if c in X_train_full.columns]\n",
    "\n",
    "# --- TimeSeries Cross-Validation to Find Optimal Iterations ---\n",
    "\n",
    "# Create the training Pool\n",
    "train_pool = Pool(\n",
    "    data=X_train_full,\n",
    "    label=y_train_full,\n",
    "    cat_features=categorical_feature_names,\n",
    ")\n",
    "\n",
    "cv_params = {\n",
    "    'loss_function': 'RMSE',\n",
    "    'iterations': 3000,         # Increase this significantly\n",
    "    'learning_rate': 0.015,      # Lower this (from 0.05)\n",
    "    'depth': 10,                 \n",
    "    'l2_leaf_reg': 3,           # Standard regularization\n",
    "    'random_seed': 42,\n",
    "    'verbose': 0,\n",
    "    'early_stopping_rounds': 100 # Stop if no improvement for 100 rounds\n",
    "}\n",
    "\n",
    "print(\"Starting TimeSeries Cross-Validation (5 Folds) to find optimal tree count...\")\n",
    "\n",
    "cv_results = cv(\n",
    "    params=cv_params,\n",
    "    pool=train_pool,\n",
    "    fold_count=5,\n",
    "    shuffle=False,               # CRITICAL: Ensures chronological order\n",
    "    type='TimeSeries',           # Uses the rolling window strategy\n",
    ")\n",
    "\n",
    "\n",
    "# Find the best iteration based on the minimum average RMSE\n",
    "best_iter = cv_results['test-RMSE-mean'].values.argmin()\n",
    "best_rmse = cv_results['test-RMSE-mean'].min()\n",
    "print(f\"Optimal number of CatBoost iterations: {best_iter + 1} (Best CV RMSE: {best_rmse:.4f})\")\n",
    "\n",
    "# --- Train Final Model on ENTIRE Training Set ---\n",
    "\n",
    "final_model = CatBoostRegressor(\n",
    "    iterations=best_iter + 1,  # Use the optimal number of trees found in CV\n",
    "    learning_rate=0.015,\n",
    "    depth=10,\n",
    "    loss_function='RMSE',\n",
    "    random_seed=42,\n",
    "    cat_features=categorical_feature_names,\n",
    "    verbose=100 # Show training progress for the final model\n",
    ")\n",
    "\n",
    "print(\"\\nTraining final model on the entire 80% training set...\")\n",
    "final_model.fit(train_pool)\n",
    "\n",
    "# --- Predict and Evaluate on the Future Test Set ---\n",
    "\n",
    "# Prepare the test features\n",
    "X_test_features = pkg_X_test.drop(columns=[c for c in drop_cols if c in pkg_X_test.columns])\n",
    "y_test_target = pkg_X_test[TARGET_COL] # The 'delay' column in pkg_X_test is the ground truth target\n",
    "\n",
    "predictions = final_model.predict(X_test_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221",
   "metadata": {},
   "source": [
    "### Use Catboost with Huber loss function\n",
    "- Huber with delta=20 hours act as MAE for the ones with less than 20 hours and act as RMSE for the ones greater than delta.\n",
    "- This will solve the obssession of RMSE toward predicting the outliers(large values)\n",
    "- parameters are set manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from catboost import CatBoostRegressor, Pool, cv\n",
    "\n",
    "# 1. Ensure chronological order\n",
    "pkg_X_train = pkg_X_train.sort_values('date').reset_index(drop=True)\n",
    "pkg_X_test = pkg_X_test.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# --- Define Model Parameters and Features ---\n",
    "TARGET_COL = 'delay'\n",
    "\n",
    "# List of categorical features (Kept country_service as it was helping!)\n",
    "cat_features = [\n",
    "    'etablissement_postal',\n",
    "    'next_etablissement_postal',\n",
    "    'day_of_week',\n",
    "    'service_indicator',\n",
    "    'origin_destination',\n",
    "    'EVENT_TYPE_CD',\n",
    "    'hour',\n",
    "    'month',\n",
    "    'is_weekend',\n",
    "    'first_last_week_day',\n",
    "    'country_service'\n",
    "]\n",
    "\n",
    "# Columns to drop\n",
    "drop_cols = [TARGET_COL, 'MAILITM_FID', 'RECPTCL_FID', 'date', 'serial_number', \n",
    "             'flow_type', 'country_code', 'destination_country', 'origin_country']\n",
    "\n",
    "# Prepare the full 80% training set\n",
    "X_train_full = pkg_X_train.drop(columns=[c for c in drop_cols if c in pkg_X_train.columns])\n",
    "y_train_full = pkg_X_train[TARGET_COL]\n",
    "\n",
    "# Identify categorical features by name\n",
    "categorical_feature_names = [c for c in cat_features if c in X_train_full.columns]\n",
    "\n",
    "# --- TimeSeries Cross-Validation to Find Optimal Iterations ---\n",
    "\n",
    "# Create the training Pool\n",
    "train_pool = Pool(\n",
    "    data=X_train_full,\n",
    "    label=y_train_full,\n",
    "    cat_features=categorical_feature_names,\n",
    ")\n",
    "\n",
    "cv_params = {\n",
    "    'loss_function': 'Huber:delta=20.0',          # <--- CHANGED to Hurber(acts as MAE for normal ones but acts as RMSE for outliers)\n",
    "    'eval_metric': 'MAE',            \n",
    "    'iterations': 1500, \n",
    "    'learning_rate': 0.03,\n",
    "    'depth': 8,                 \n",
    "    'random_seed': 42,\n",
    "    'verbose': 0,\n",
    "    # 'early_stopping_rounds': 100,\n",
    "    # 'l2_leaf_reg': 3,          # Standard regularization\n",
    "}\n",
    "\n",
    "print(\"Starting TimeSeries Cross-Validation (5 Folds) optimizing for MAE...\")\n",
    "\n",
    "cv_results = cv(\n",
    "    params=cv_params,\n",
    "    pool=train_pool,\n",
    "    fold_count=5,\n",
    "    shuffle=False, \n",
    "    type='TimeSeries', \n",
    ")\n",
    "\n",
    "# Find the best iteration based on the minimum average MAE\n",
    "# Note: cv_results keys change based on the loss_function used\n",
    "best_iter = cv_results['test-MAE-mean'].values.argmin()\n",
    "best_mae = cv_results['test-MAE-mean'].min()\n",
    "print(f\"Optimal number of CatBoost iterations: {best_iter + 1} (Best CV MAE: {best_mae:.4f})\")\n",
    "\n",
    "# --- Train Final Model on ENTIRE Training Set ---\n",
    "\n",
    "final_model = CatBoostRegressor(\n",
    "    iterations=best_iter + 1,\n",
    "    learning_rate=0.03,\n",
    "    depth=8,\n",
    "    loss_function='Huber:delta=20.0',            \n",
    "    eval_metric='MAE',              \n",
    "    random_seed=42,\n",
    "    cat_features=categorical_feature_names,\n",
    "    verbose=100 \n",
    ")\n",
    "\n",
    "print(\"\\nTraining final model on the entire 80% training set...\")\n",
    "final_model.fit(train_pool)\n",
    "\n",
    "# --- Predict and Evaluate on the Future Test Set ---\n",
    "\n",
    "X_test_features = pkg_X_test.drop(columns=[c for c in drop_cols if c in pkg_X_test.columns])\n",
    "y_test_target = pkg_X_test[TARGET_COL]\n",
    "\n",
    "predictions = final_model.predict(X_test_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223",
   "metadata": {},
   "source": [
    "# Using Optuna for hyperparameters tuning before passing them to CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224",
   "metadata": {},
   "source": [
    "### Preparing the data pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool\n",
    "\n",
    "# ---  Data Preparation ---\n",
    "pkg_X_train = pkg_X_train.sort_values('date').reset_index(drop=True)\n",
    "pkg_X_test = pkg_X_test.sort_values('date').reset_index(drop=True)\n",
    "TARGET_COL = 'delay'\n",
    "cat_features = [\n",
    "    'etablissement_postal', 'next_etablissement_postal', 'day_of_week',\n",
    "    'service_indicator', 'origin_destination', 'EVENT_TYPE_CD', 'hour', 'month',\n",
    "    'is_weekend', 'first_last_week_day', 'service_country'\n",
    "]\n",
    "drop_cols = [TARGET_COL, 'MAILITM_FID', 'RECPTCL_FID', 'date', 'serial_number', \n",
    "             'flow_type', 'country_code', 'destination_country', 'origin_country']\n",
    "\n",
    "X_train_full = pkg_X_train.drop(columns=[c for c in drop_cols if c in pkg_X_train.columns])\n",
    "y_train_full = pkg_X_train[TARGET_COL]\n",
    "categorical_feature_names = [c for c in cat_features if c in X_train_full.columns]\n",
    "\n",
    "train_pool = Pool(data=X_train_full, label=y_train_full, cat_features=categorical_feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226",
   "metadata": {},
   "source": [
    "### This will find the best parameters and saves them into a json file so they can be used later without the need to do the tuning again unless changes are made in the data or the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import json\n",
    "from catboost import cv\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'loss_function': 'Huber:delta=20.0',\n",
    "        'eval_metric': 'MAE',\n",
    "        'custom_metric': ['RMSE'], # Ensure RMSE is calculated\n",
    "        'random_seed': 42,\n",
    "        'verbose': 0,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'depth': trial.suggest_int('depth', 4, 8),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "        'iterations': 1000, \n",
    "        'early_stopping_rounds': 30\n",
    "    }\n",
    "    \n",
    "    cv_results = cv(\n",
    "        params=params, \n",
    "        pool=train_pool, \n",
    "        fold_count=3, \n",
    "        shuffle=False, \n",
    "        type='TimeSeries',\n",
    "        plot=False\n",
    "    )\n",
    "    \n",
    "    # Track both metrics\n",
    "    best_mae = cv_results['test-MAE-mean'].min()\n",
    "    best_rmse = cv_results['test-RMSE-mean'].min()\n",
    "    \n",
    "    return best_mae, best_rmse\n",
    "\n",
    "# --- 2. Run Multi-Objective Optimization ---\n",
    "print(\"Starting Hyperparameter Tuning (Tracking MAE & RMSE)...\")\n",
    "study = optuna.create_study(directions=['minimize', 'minimize'])\n",
    "study.optimize(objective, n_trials=20) \n",
    "\n",
    "# --- 3. Pick the best trial and find optimal iterations ---\n",
    "# We pick the trial with the lowest MAE \n",
    "best_trial = min(study.best_trials, key=lambda t: t.values[0])\n",
    "\n",
    "print(f\"\\nBest Trial selected (MAE: {best_trial.values[0]:.4f}, RMSE: {best_trial.values[1]:.4f})\")\n",
    "\n",
    "# Find best iterations for the winning set on 5 folds for stability\n",
    "final_params_temp = {\n",
    "    'loss_function': 'Huber:delta=20.0', \n",
    "    'eval_metric': 'MAE', \n",
    "    **best_trial.params\n",
    "}\n",
    "\n",
    "print(\"Calculating final optimal iterations...\")\n",
    "final_cv = cv(params=final_params_temp, pool=train_pool, fold_count=5, shuffle=False, type='TimeSeries', verbose=0)\n",
    "best_iteration = int(final_cv['test-MAE-mean'].values.argmin() + 1)\n",
    "\n",
    "# --- 4. Save to JSON ---\n",
    "best_config = {\n",
    "    **best_trial.params,\n",
    "    \"iterations\": best_iteration,\n",
    "    \"loss_function\": 'Huber:delta=20.0',\n",
    "    \"eval_metric\": 'MAE',\n",
    "    \"final_mae\": best_trial.values[0],\n",
    "    \"final_rmse\": best_trial.values[1]\n",
    "}\n",
    "\n",
    "with open(\"catboost_best_params.json\", \"w\") as f:\n",
    "    json.dump(best_config, f, indent=4)\n",
    "\n",
    "print(f\"Tuning complete. Best parameters saved to 'catboost_best_params.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228",
   "metadata": {},
   "source": [
    "### Load the parameters from the json file and use them for the final training and then test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# --- 1. Load Parameters ---\n",
    "with open(\"catboost_best_params.json\", \"r\") as f:\n",
    "    best_config = json.load(f)\n",
    "\n",
    "# Extract iterations and remove metrics/metadata so CatBoost doesn't crash\n",
    "iters = best_config.pop(\"iterations\")\n",
    "best_config.pop(\"final_mae\", None)   # Remove if present\n",
    "best_config.pop(\"final_rmse\", None)  # Remove if present\n",
    "\n",
    "# --- 2. Initialize and Train Model ---\n",
    "# Ensure categorical_feature_names and train_pool are defined in your environment\n",
    "final_model = CatBoostRegressor(\n",
    "    iterations=iters,\n",
    "    **best_config,\n",
    "    cat_features=categorical_feature_names,\n",
    "    random_seed=42,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "print(\"Training final model with saved parameters...\")\n",
    "final_model.fit(train_pool)\n",
    "\n",
    "# --- 3. Save the actual Model weights ---\n",
    "final_model.save_model(\"final_delay_model.cbm\")\n",
    "print(\"Model trained and saved as 'final_delay_model.cbm'\")\n",
    "\n",
    "# --- 4. Predict on Test Set ---\n",
    "# Ensure pkg_X_test and drop_cols are defined in your environment\n",
    "X_test_features = pkg_X_test.drop(columns=[c for c in drop_cols if c in pkg_X_test.columns])\n",
    "predictions = final_model.predict(X_test_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error,root_mean_squared_error\n",
    "y_test_true = pkg_X_test[TARGET_COL]\n",
    "y_test_pred_subset = predictions\n",
    "    \n",
    "final_rmse = root_mean_squared_error(y_test_true, y_test_pred_subset)\n",
    "final_mae = mean_absolute_error(y_test_true, y_test_pred_subset)\n",
    "\n",
    "print(f\"\\nFinal Model Evaluation on Test Set:\")\n",
    "print(f\"RMSE: {final_rmse:.4f} hours\")\n",
    "print(f\"MAE: {final_mae:.4f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Importance Analysis ---\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the feature importances from the trained model\n",
    "feature_importances = final_model.get_feature_importance()\n",
    "feature_names = X_train_full.columns\n",
    "\n",
    "# Create a Series for easy sorting and handling\n",
    "importance_series = pd.Series(feature_importances, index=feature_names)\n",
    "\n",
    "# Sort the features by importance (descending)\n",
    "sorted_importance = importance_series.sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n--- Feature Importances (Top 10) ---\")\n",
    "print(sorted_importance.head(10))\n",
    "\n",
    "# Optional: Plot the top 10 features\n",
    "plt.figure(figsize=(10, 6))\n",
    "sorted_importance.head(10).plot(kind='barh', color='skyblue')\n",
    "plt.title('Top 10 CatBoost Feature Importances')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = final_model.get_feature_importance(type='Interaction', prettified=True)\n",
    "print(\"\\nTop Feature Interactions:\")\n",
    "print(interactions.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Target Variable Comparison ---\")\n",
    "print(\"Training Delay Statistics:\")\n",
    "print(pkg_X_train['delay'].describe())\n",
    "\n",
    "print(\"\\nTesting Delay Statistics:\")\n",
    "print(pkg_X_test['delay'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235",
   "metadata": {},
   "source": [
    "The 7800-hour max in the training set and the 6024-hour max in the test set are still extremely high. These current outliers, combined with the high mean delay (~50 hours), mean that the target variable (delay) is still heavily right-skewed.\n",
    "The CatBoost model is predicting the average of the distribution, and the tail (the very long delays) is pulling the final RMSE score up dramatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236",
   "metadata": {},
   "source": [
    "### Analysis of the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_test_target- predictions\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=predictions, y=residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Residual Plot: Prediction vs. Error')\n",
    "plt.xlabel('Predicted Delay (Hours)')\n",
    "plt.ylabel('Residuals (Actual - Predicted)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate the raw error (not absolute)\n",
    "results = pd.DataFrame({\n",
    "    'Actual': y_test_target,\n",
    "    'Predicted': predictions\n",
    "})\n",
    "results['Residual'] = results['Predicted'] - results['Actual']\n",
    "\n",
    "# 2. Filter for large errors (greater than 50 hours)\n",
    "big_errors = results[np.abs(results['Residual']) > 50]\n",
    "\n",
    "# 3. Count which is more common\n",
    "missed_delays = big_errors[big_errors['Residual'] < 0].shape[0]\n",
    "model_hallucinations = big_errors[big_errors['Residual'] > 0].shape[0]\n",
    "\n",
    "print(f\"Total Large Errors (>50h): {len(big_errors)}\")\n",
    "print(f\"---\")\n",
    "print(f\"Missed Delays (Under-predicted): {missed_delays}\")\n",
    "print(f\"Model Hallucinations (Over-predicted): {model_hallucinations}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
