{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "make sure to rename the columns by removing é\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) for Packages and Receptacle Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "packages_df = pd.read_csv('../data/raw/packages_data_2023_2025.csv',delimiter=';', encoding='latin-1')\n",
    "receptacles_df = pd.read_csv('../data/raw/receptacle_data_2023_2025.csv',delimiter=';', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "* Columns' names adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df = packages_df.rename(columns={'établissement_postal': 'etablissement_postal', 'next_établissement_postal': 'next_etablissement_postal'})\n",
    "receptacles_df = receptacles_df.rename(columns={'ï»¿RECPTCL_FID': 'RECPTCL_FID', 'EVENT_TYPECD': 'EVENT_TYPE_CD', 'nextetablissement_postal': 'next_etablissement_postal'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df.shape, receptacles_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "* Columns' types adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df['date'] = pd.to_datetime(packages_df['date'])\n",
    "receptacles_df['date'] = pd.to_datetime(receptacles_df['date'])\n",
    "packages_df['RECPTCL_FID'] = packages_df['RECPTCL_FID'].str.strip()\n",
    "packages_df['MAILITM_FID'] = packages_df['MAILITM_FID'].str.strip()\n",
    "packages_df['etablissement_postal'] = packages_df['etablissement_postal'].str.strip()\n",
    "packages_df['next_etablissement_postal'] = packages_df['next_etablissement_postal'].str.strip()\n",
    "receptacles_df['etablissement_postal'] = receptacles_df['etablissement_postal'].str.strip()\n",
    "receptacles_df['next_etablissement_postal'] = receptacles_df['next_etablissement_postal'].str.strip()\n",
    "receptacles_df['RECPTCL_FID'] = receptacles_df['RECPTCL_FID'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    " Initial Observations\n",
    "- both datasets cover the period from 2023 to 2025\n",
    "- we have no target variable in either dataset\n",
    "- for packages dataset:\n",
    "    - 6 features in total with 5 categorical and 1 numerical\n",
    "    - MAILITM_FID is unique identifier for each package\n",
    "    - RECPTCL_FID is foreign key linking to receptacle dataset\n",
    "    - etablissement_postal and next_etablissement_postal have some null values\n",
    "- for receptacle dataset:\n",
    "    - 5 features in total with 4 categorical and 1 numerical\n",
    "    - RECPTCL_FID is unique identifier for each receptacle\n",
    "    - EVENT_TYPE_CD has some null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in packages_df.columns:\n",
    "    print(f'{column} has {packages_df[column].nunique()} unique values.')\n",
    "    print(f'{column} has {packages_df[column].isnull().sum()} null values.')\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in receptacles_df.columns:\n",
    "    print(f'{column} has {receptacles_df[column].nunique()} unique values.')\n",
    "    print(f'{column} has {receptacles_df[column].isnull().sum()} null values.')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "we notice the following:<br>\n",
    "- receptacle dataset has more unique values for RECPTCL_FID than packages dataset, indicating one-to-many relationship<br>\n",
    "- MAILITM_FID is unique in packages dataset.<br>\n",
    "- packages dataset have more unique date values than receptacle dataset.<br>\n",
    "- both datasets have null values in etablissement_postal and next_etablissement_postal columns. This requires processing later on<br>\n",
    "- packages dataset has more unique values in the next_etablissement_postal column compared to receptacle dataset but also more null values. **further investigation is needed to understand why**<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "for EVENT_TYPE_CD we notice different range of values for packages and receptacle datasets indicating different types of events.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### Visualizations\n",
    "for now we will visualize the distribution of EVENT_TYPE_CD in both datasets.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of EVENT_TYPE_CD in packages dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=packages_df,x='EVENT_TYPE_CD')\n",
    "plt.title('distribution of EVENT_TYPE_CD in packages dataset')\n",
    "plt.xlabel('EVENT_TYPE_CD')\n",
    "plt.ylabel('count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of EVENT_TYPE_CD in receptacle dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=receptacles_df,x='EVENT_TYPE_CD')\n",
    "plt.title('distribution of EVENT_TYPE_CD in Receptacle dataset')\n",
    "plt.xlabel('EVENT_TYPE_CD')\n",
    "plt.ylabel('count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "for etablissement_postal and next_etablissement_postal I will start with visualizing the receptacle dataset since the packages dataset has a lot of unique values<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of etablissement_postal in receptacle dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=receptacles_df,x='etablissement_postal')\n",
    "plt.title('distribution of etablissement_postal in Receptacle dataset')\n",
    "plt.xlabel('etablissement_postal')\n",
    "plt.ylabel('count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of next_etablissement_postal in receptacle dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=receptacles_df,x='next_etablissement_postal')\n",
    "plt.title('distribution of next_etablissement_postal in Receptacle dataset')\n",
    "plt.xlabel('next_etablissement_postal')\n",
    "plt.ylabel('count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.  Top 10 locations to keep the map small\n",
    "top_locs = receptacles_df['etablissement_postal'].value_counts().head(10).index\n",
    "top_next = receptacles_df['next_etablissement_postal'].value_counts().head(10).index\n",
    "\n",
    "# 2. include only these top locations\n",
    "filtered_df = receptacles_df[\n",
    "    (receptacles_df['etablissement_postal'].isin(top_locs)) &\n",
    "    (receptacles_df['next_etablissement_postal'].isin(top_next))\n",
    "]\n",
    "\n",
    "# 3. Create a Matrix (Cross-tabulation)\n",
    "matrix = pd.crosstab(filtered_df['etablissement_postal'], filtered_df['next_etablissement_postal'])\n",
    "\n",
    "# 4. Plot Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(matrix, cmap='Reds', annot=True, fmt='d', linewidths=.5)\n",
    "plt.title(\"Most Frequent Routes (Origin vs Destination) - Receptacle Dataset\")\n",
    "plt.xlabel(\"Next Destination\")\n",
    "plt.ylabel(\"Current Location\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "we notice that some etablissements have significantly higher traffic compared to others, indicating  major distribution centers.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "for etablissement_postal and next_etablissement_postal we will create a heatmap to visualize the flow between current location and next destination.<br>\n",
    "Count of parcels moving from A to B to see the density of connections between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.  Top 10 locations to keep the map small\n",
    "top_locs = packages_df['etablissement_postal'].value_counts().head(10).index\n",
    "top_next = packages_df['next_etablissement_postal'].value_counts().head(10).index\n",
    "\n",
    "# 2. include only these top locations\n",
    "filtered_df = packages_df[\n",
    "    (packages_df['etablissement_postal'].isin(top_locs)) &\n",
    "    (packages_df['next_etablissement_postal'].isin(top_next))\n",
    "]\n",
    "\n",
    "# 3. Create a Matrix (Cross-tabulation)\n",
    "matrix = pd.crosstab(filtered_df['etablissement_postal'], filtered_df['next_etablissement_postal'])\n",
    "\n",
    "# 4. Plot Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(matrix, cmap='Reds', annot=True, fmt='d', linewidths=.5)\n",
    "plt.title(\"Most Frequent Routes (Origin vs Destination) - Packages Dataset\")\n",
    "plt.xlabel(\"Next Destination\")\n",
    "plt.ylabel(\"Current Location\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count packages per location\n",
    "location_counts = packages_df['etablissement_postal'].value_counts().reset_index()\n",
    "location_counts.columns = ['Location', 'Volume']\n",
    "\n",
    "# keep only top 20 busiest centers\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Volume', y='Location', data=location_counts.head(20), palette='viridis')\n",
    "plt.title(\"Top 20 Busiest Postal Centers\")\n",
    "plt.xlabel(\"Number of Packages\")\n",
    "plt.ylabel(\"Center ID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "we notice the same pattern as before with some etablissements having significantly higher trafic compared to others.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Extract Time Features\n",
    "packages_df['hour'] = packages_df['date'].dt.hour\n",
    "packages_df['day_of_week'] = packages_df['date'].dt.day_name()\n",
    "\n",
    "# 2. Create a Pivot Table (Cross-tabulation)\n",
    "# Rows = Day, Cols = Hour, Values = Count of Scans\n",
    "heatmap_data = pd.crosstab(\n",
    "    packages_df['day_of_week'],\n",
    "    packages_df['hour']\n",
    ")\n",
    "\n",
    "days_order = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "heatmap_data = heatmap_data.reindex(days_order)\n",
    "\n",
    "# 3. Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(heatmap_data, cmap='YlOrRd', linewidths=.5, annot=False)\n",
    "plt.title(\"Package Scan Activity by Day and Hour\")\n",
    "plt.xlabel(\"Hour of Day (0-23)\")\n",
    "plt.ylabel(\"Day of Week\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "we notice that the busiest times for package scans are during weekdays, particularly from mid-morning to late afternoon.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of packages per receptacle\n",
    "packages_per_receptacle = packages_df.groupby('RECPTCL_FID')['MAILITM_FID'].nunique()\n",
    "packages_per_receptacle.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "- Drop the packages starting from 2020 and keep only the ones starting from 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in packages df Drop the values that are from 2020 and start only from 2023\n",
    "packages_df = packages_df[packages_df['date'].dt.year >= 2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df['etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "* `etablissement_postal` have 26772 null values (2.7% of the whole dataset)\n",
    "* As its null values are less than 5% of the dataset (2.7%), we drop these null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df = packages_df[~packages_df['etablissement_postal'].isna()]\n",
    "packages_df['etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "* We propose to consider the packages having null `next_etablissement_postal`\n",
    "as having issue during transfer, we'll try to validate that using\n",
    "`EVENT_TYPE_CD` also\n",
    "* Let's check if `EVENT_TYPE_CD` can indicate whether the `next_etablissement_postal` is null or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_unknown_next_etablissement = packages_df[packages_df['next_etablissement_postal'].isna()]\n",
    "# keep only top EVENT_TYPES_ID\n",
    "packages_unknown_next_etablissement = packages_unknown_next_etablissement['EVENT_TYPE_CD'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "packages_unknown_next_etablissement.head(10).plot(kind='bar')\n",
    "plt.xlabel('EVENT TYPE CD')\n",
    "plt.ylabel('Null Next Etablissement Postal')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "* `EVENT_TYPE_CD` doesn't actually indicate null values of `next_etablissement_postal`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "* the function `fill_NaN_next_etab` cell fills the `next_etablissement_postal` using the next `etablissement_postal` for the same package.\n",
    "* if the last route for a specific package is null, then it keeps it null because there's no next `etablissement_postal` for that package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_NaN_next_etab(df, id_col):\n",
    "    # 1. Ensure the dataframe is sorted (same as before)\n",
    "    df = df.sort_values([id_col, 'date'])\n",
    "\n",
    "    # 2. Look ahead to the next row's postal code and ID\n",
    "    shifted_postal = df['etablissement_postal'].shift(-1)\n",
    "    shifted_id = df[id_col].shift(-1)\n",
    "# 3. Identify the \"boundaries\" where the postal code changes within the same package\n",
    "# This marks the last row of a block with the value of the start of the next block\n",
    "    is_boundary = (df['etablissement_postal'] != shifted_postal) & \\\n",
    "              (df[id_col] == shifted_id)\n",
    "# 4. Use grouped backfill to broadcast those values to all preceding rows in the block\n",
    "# This replaces your 'blocks.map' logic with a single vectorized pass\n",
    "    fill_values = shifted_postal.where(is_boundary).groupby(df[id_col]).bfill()\n",
    "\n",
    "# 5. Fill only the NaNs in the existing column to match your original logic\n",
    "    df['next_etablissement_postal'] = df['next_etablissement_postal'].fillna(fill_values)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to fill NaN values in next_etablissement_postal\n",
    "packages_df = fill_NaN_next_etab(packages_df, 'MAILITM_FID')\n",
    "# Check remaining NaNs\n",
    "packages_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "* Like this, we've handled a good part of null values and inconsitencies for `packages` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "* **We'll be doing the same steps for `receptacle` dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df['etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "* Dropping rows having null `etablissement_postal`, as they're just 0.1% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df = receptacles_df[~receptacles_df['etablissement_postal'].isna()]\n",
    "receptacles_df['etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "* apply the function that fills null values of `next_etablissement_postal` using `etablissement_postal` to `receptacles_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df = fill_NaN_next_etab(receptacles_df, 'RECPTCL_FID')\n",
    "# Check remaining NaNs\n",
    "receptacles_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "* Null values are mostly gone, but there are still some illogical packages' and receptacles' routes between `etablissements`\n",
    "* We'll treat these logical routes now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each package (group of rows), check whether there's any illogical route\n",
    "# between 'etablissement_postal' and 'next_etablissement_postal'\n",
    "def isPackageIllogical(group):\n",
    "    return (\n",
    "        group['next_etablissement_postal']\n",
    "        .iloc[:-1]\n",
    "        .ne(group['etablissement_postal'].shift(-1).iloc[:-1])\n",
    "        .any()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "illogical_packages = packages_df.groupby('MAILITM_FID').apply(isPackageIllogical)\n",
    "illogical_packages.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "103376 / packages_df['MAILITM_FID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "* 103376 Packages have illogical routes (98%) of all packages, so it's impossible to drop them, but instead, we plan to ignore the `MAILITM_FID` and `RECPTCL_FID` in the training and testing sets that will come next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each receptacle (group of rows), check whether there's any illogical route\n",
    "# between 'etablissement_postal' and 'next_etablissement_postal'\n",
    "def isReceptacleIllogical(group):\n",
    "    return (\n",
    "        group['next_etablissement_postal']\n",
    "        .iloc[:-1]\n",
    "        .ne(group['etablissement_postal'].shift(-1).iloc[:-1])\n",
    "        .any()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "illogical_receptacles = receptacles_df.groupby('RECPTCL_FID').apply(isReceptacleIllogical)\n",
    "illogical_receptacles.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "205519 / receptacles_df['RECPTCL_FID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "205519 receptacles have illogical routes (95%) of all receptacles, so it's also impossible to drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep copies for backup (en cas ou)\n",
    "packages_df_copy = packages_df.copy()\n",
    "receptacles_df_copy = receptacles_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "# Feature Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "### Check RECPTCL_FID and MAILITM_FID having same length formats\n",
    "if yes then we can split them into meaningfull parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "same=1\n",
    "print(\"\\n=== RECPTCL_FID  ===\")\n",
    "print(f\"testing if the lengths of RECPTCL_FID values are all the same:\")\n",
    "for val in packages_df['RECPTCL_FID'].values:\n",
    "    if len(str(val)) != 29 :\n",
    "        print(f\"  {val} (length: {len(str(val))})\")\n",
    "        same=0\n",
    "        break\n",
    "if same==1:\n",
    "    print('all same length' )\n",
    "\n",
    "same=1\n",
    "print(\"\\n=== MAILITM_FID  ===\")\n",
    "print(f\"testing if the lengths of MAILITM_FID values are all the same:\")\n",
    "for val in packages_df['MAILITM_FID'].values:\n",
    "    if len(str(val)) != 13 :\n",
    "        print(f\"  {val} (length: {len(str(val))})\")\n",
    "        same=0\n",
    "        break\n",
    "if same==1:\n",
    "    print ('all same length' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "### RECPTCL_FID Analysis\n",
    "- **Format:** 29-character string (e.g., `USORDADZALGDAUN30050001900005`)\n",
    "- **Data Quality:** No null values (1,000,000) | 215,867 unique values in receptacle dataset and 45306 unique values in packages dataset\n",
    "- **Extractable Features:**\n",
    "  - Origin Country (2 chars): US, FR, AE, etc.\n",
    "  - Destination Country (2 chars): DZ, AI, AA, etc.\n",
    "\n",
    "### MAILITM_FID Analysis\n",
    "- **Format:** 13-character string according to the S10-12 patern (e.g., `CA000132868US`, `CA000340856PK`)\n",
    "- **Data Quality:** No null values (1,000,000 packages)\n",
    "- **Extractable Features:**\n",
    "  - Service Indicator (2 chars): CA, etc.\n",
    "  - Serial Number (8 chars): 00013286, 00034085, etc.\n",
    "  - Check Digit (1 char): 8, 6, etc.\n",
    "  - Country Code (3 chars, right-stripped): US, PK.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "## Definition of the parser funtions\n",
    "These functions are responsible for spliting the IDs into parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_recptcl_fid(id_str):\n",
    "    origin_country = id_str[0:2]\n",
    "    destination_country = id_str[6:8]\n",
    "    return origin_country, destination_country\n",
    "\n",
    "def parse_mailitm_fid(id_str):\n",
    "    service_indicator = id_str[0:2]\n",
    "    serial_number = id_str[2:11]\n",
    "    country_code = id_str[11:14].strip()\n",
    "    return service_indicator, serial_number, country_code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "### Apply parsing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_receptacles_df = receptacles_df.copy()\n",
    "# parsed_receptacles_df[['origin_country', 'destination_country']] = parsed_receptacles_df['RECPTCL_FID'].apply(lambda x: pd.Series(parse_recptcl_fid(x)))\n",
    "\n",
    "# Faster method:\n",
    "parsed_data = list(receptacles_df['RECPTCL_FID'].apply(parse_recptcl_fid))\n",
    "\n",
    "# Assign to new columns by creating a temporary DataFrame\n",
    "parsed_receptacles_df = receptacles_df.copy()\n",
    "parsed_receptacles_df[['origin_country', 'destination_country']] = pd.DataFrame(\n",
    "    parsed_data, index=receptacles_df.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_packages_df = packages_df.copy()\n",
    "# parsed_packages_df[['service_indicator', 'serial_number', 'country_code']] = parsed_packages_df['MAILITM_FID'].apply(lambda x: pd.Series(parse_mailitm_fid(x)))\n",
    "# parsed_packages_df[['origin_country','destination_country']] = parsed_packages_df['RECPTCL_FID'].apply(lambda x: pd.Series(parse_recptcl_fid(x)))\n",
    "\n",
    "# Faster method:\n",
    "parsed_packages_df = packages_df.copy()\n",
    "\n",
    "# 1. Optimize MAILITM_FID parsing\n",
    "mailitm_data = list(parsed_packages_df['MAILITM_FID'].apply(parse_mailitm_fid))\n",
    "parsed_packages_df[['service_indicator', 'serial_number', 'country_code']] = pd.DataFrame(\n",
    "    mailitm_data, index=parsed_packages_df.index\n",
    ")\n",
    "\n",
    "# 2. Optimize RECPTCL_FID parsing\n",
    "recptcl_data = list(parsed_packages_df['RECPTCL_FID'].apply(parse_recptcl_fid))\n",
    "parsed_packages_df[['origin_country', 'destination_country']] = pd.DataFrame(\n",
    "    recptcl_data, index=parsed_packages_df.index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "### show samples of new parsed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=== packages_df sample with new parsed columns ===\")\n",
    "parsed_packages_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== receptacles_df sample with new parsed columns ===\")\n",
    "parsed_receptacles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "# Analysis of the extrcted features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "## 1. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n--- Unique Value Counts for parsed_packages_df ---\")\n",
    "print(\"\\nFor receptacle FID parsing:\")\n",
    "print(f\"Unique origin_country values: {parsed_packages_df['origin_country'].nunique()}\")\n",
    "print(f\"Unique destination_country values: {parsed_packages_df['destination_country'].nunique()}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"\\nFor mail item FID parsing:\")\n",
    "print(f\"Unique service_indicator values: {parsed_packages_df['service_indicator'].nunique()}\")\n",
    "print(f\"Unique country_code values: {parsed_packages_df['country_code'].nunique()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "## 2. parsed_receptacles_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Unique Value Counts for parsed_receptacles_df ---\")\n",
    "print(f\"Unique origin_country values: {parsed_receptacles_df['origin_country'].nunique()}\")\n",
    "print(f\"Unique destination_country values: {parsed_receptacles_df['destination_country'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "## List values of the new columns obtained from receptacle FID parsing for both parsed dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "### 1. for parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#listing the values \n",
    "print(\"\\n--- Values of origin_country ---\")\n",
    "print(parsed_packages_df['origin_country'].unique())\n",
    "\n",
    "print(\"\\n--- Values of destination_country ---\")\n",
    "print(parsed_packages_df['destination_country'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "### 2. for parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#listing the values \n",
    "print(\"\\n--- Values of origin_country ---\")\n",
    "print(parsed_receptacles_df['origin_country'].unique())\n",
    "\n",
    "print(\"\\n--- Values of destination_country ---\")\n",
    "print(parsed_receptacles_df['destination_country'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "### Do the intersection of origin_country of both dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the intersection of origin_country values in both parsed datasets\n",
    "packages_origin_countries = set(parsed_packages_df['origin_country'].unique())\n",
    "receptacle_origin_countries = set(parsed_receptacles_df['origin_country'].unique())\n",
    "common_origin_countries = packages_origin_countries.intersection(receptacle_origin_countries)\n",
    "print(\"number of common origin_country values in both parsed datasets:\", len(common_origin_countries))\n",
    "print(f\"\\nCommon origin_country values in both paesed datasets: \")\n",
    "print(common_origin_countries)\n",
    "# remaining ones \n",
    "remaining_in_packages = packages_origin_countries - common_origin_countries\n",
    "remaining_in_receptacle = receptacle_origin_countries - common_origin_countries\n",
    "print(f\"Remaining origin_country values only in parsed_packages_df:\")\n",
    "print(remaining_in_packages)\n",
    "print(f\"Remaining origin_country values only in parsed_receptacles_df:\")\n",
    "print(remaining_in_receptacle )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "### list the values of both service indicators and country code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "### 1. service indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Values of service_indicator ---\")\n",
    "print(parsed_packages_df['service_indicator'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    " we can see that there are values that don't follow the standards in the S10-12 format so we need to handle that correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform country_code to uppercase for consistency\n",
    "parsed_packages_df['service_indicator'] = parsed_packages_df['service_indicator'].str.upper()\n",
    "print(\"values of service_indicator after transformation to uppercase:\")\n",
    "print(parsed_packages_df['service_indicator'].unique())\n",
    "print(\"number of unique service indicators after transformation:\", parsed_packages_df['service_indicator'].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "### 2. country code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Values of country codes ---\")\n",
    "\n",
    "print(parsed_packages_df['country_code'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "we can see that many values for the country codes are numbers instead of ISO 3166-1 format these values should be replaced by the values of origin country gotten from the receptacle when doing the preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "### replace them with the correct origin country code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Vectorized string capitalization\n",
    "parsed_packages_df['country_code'] = parsed_packages_df['country_code'].str.upper()\n",
    "\n",
    "# 2. Vectorized comparison to find mismatches\n",
    "mismatch_mask = parsed_packages_df['origin_country'] != parsed_packages_df['country_code']\n",
    "\n",
    "# 3. Count the Trues\n",
    "count = mismatch_mask.sum()\n",
    "\n",
    "print(f\"Number of rows where origin_country does not match country_code: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "### replace them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .loc to find rows where they don't match, and update only the 'country_code' column\n",
    "parsed_packages_df.loc[parsed_packages_df['origin_country'] != parsed_packages_df['country_code'], 'country_code'] = parsed_packages_df['origin_country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the unique values again\n",
    "print(\"\\n--- Values of country codes after correction ---\")\n",
    "print(parsed_packages_df['country_code'].unique())\n",
    "print(\"number of unique country codes after correction:\", parsed_packages_df['country_code'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "## visualization of Origin Country distribution according to number of packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104",
   "metadata": {},
   "source": [
    "### 1. for the parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_country_counts = parsed_packages_df['origin_country'].value_counts().head(20)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.barh(origin_country_counts.index, origin_country_counts.values, color='steelblue')\n",
    "plt.xlabel('Count', fontsize=11)\n",
    "plt.ylabel('Origin Country', fontsize=11)\n",
    "plt.title('Top 20 Origin Countries by packages count', fontsize=12, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "### 2. for the parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_country_counts = parsed_receptacles_df['origin_country'].value_counts().head(20)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.barh(origin_country_counts.index, origin_country_counts.values, color='steelblue')\n",
    "plt.xlabel('Count', fontsize=11)\n",
    "plt.ylabel('Origin Country', fontsize=11)\n",
    "plt.title('Top 20 Origin Countries by receptacle count', fontsize=12, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {},
   "source": [
    "## Visualiation of the service indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_indicator_count = parsed_packages_df['service_indicator'].value_counts().head(20)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.barh(service_indicator_count.index, service_indicator_count.values, color='mediumseagreen')\n",
    "plt.xlabel('Count', fontsize=11)\n",
    "plt.ylabel('Service Indicator', fontsize=11)\n",
    "plt.title('Top 20 Service Indicators by packages count', fontsize=12, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110",
   "metadata": {},
   "source": [
    "# Origin–Destination Flow Analysis\n",
    "\n",
    "This section investigates the flow of receptacles and packages from origin countries to destination. We examine:\n",
    "- packages count by origin country\n",
    "- Top origin countries delivering to each destination\n",
    "- Visual representation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {},
   "source": [
    "### 1. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages count by origin country\n",
    "origin_country_volume = parsed_packages_df['origin_country'].value_counts()\n",
    "print(\"\\n--- Packages count by Origin Country ---\")\n",
    "print(origin_country_volume.head(15))\n",
    "\n",
    "# Create origin_country × destination matrix\n",
    "flow_matrix = pd.crosstab(parsed_packages_df['origin_country'], \n",
    "                           parsed_packages_df['destination_country'])\n",
    "\n",
    "# Keep only top 10 origin countries and top 10 destination countries for readability\n",
    "top_origins = parsed_packages_df['origin_country'].value_counts().head(10).index\n",
    "top_arrivals = parsed_packages_df['destination_country'].value_counts().head(10).index\n",
    "\n",
    "flow_matrix_top = flow_matrix.loc[top_origins, top_arrivals]\n",
    "\n",
    "print(\"\\n--- Origin Country × destination Country (Top 10 × Top 10) ---\")\n",
    "print(flow_matrix_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize origin × destination country flow\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(flow_matrix_top, cmap='YlOrRd', annot=True, fmt='d', \n",
    "            cbar_kws={'label': 'packages Count'}, linewidths=0.5)\n",
    "plt.title('packages Flow: Origin Country × destination country (Top 10 × Top 10)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Arrival Hub', fontsize=12)\n",
    "plt.ylabel('Origin Country', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114",
   "metadata": {},
   "source": [
    "### 2. parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# receptacle count by origin country\n",
    "origin_country_receptacle_volume = parsed_receptacles_df['origin_country'].value_counts()\n",
    "print(\"\\n--- Receptacle count by Origin Country ---\")\n",
    "print(origin_country_receptacle_volume.head(15))\n",
    "\n",
    "destination__receptacle_volume=parsed_receptacles_df['destination_country'].value_counts()\n",
    "print(\"\\n--- Receptacle count by destination ---\")\n",
    "print(destination__receptacle_volume.head(15))\n",
    "\n",
    "# Create origin_country × destination matrix\n",
    "receptacle_flow_matrix = pd.crosstab(parsed_receptacles_df['origin_country'], \n",
    "                           parsed_receptacles_df['destination_country'])\n",
    "\n",
    "# Keep only top 10 origin countries and top 10 destination countries for readability\n",
    "receptacle_top_origins = parsed_receptacles_df['origin_country'].value_counts().head(10).index\n",
    "receptacle_top_arrivals = parsed_receptacles_df['destination_country'].value_counts().head(10).index\n",
    "\n",
    "receptacle_flow_matrix_top = receptacle_flow_matrix.loc[receptacle_top_origins, receptacle_top_arrivals]\n",
    "\n",
    "print(\"\\n--- Origin Country × destination Country (Top 10 × Top 10) ---\")\n",
    "print(receptacle_flow_matrix_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize origin × destination country flow\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(receptacle_flow_matrix_top, cmap='YlOrRd', annot=True, fmt='d', \n",
    "            cbar_kws={'label': 'receptacles Count'}, linewidths=0.5)\n",
    "plt.title('receptacles Flow: Origin Country × destination country (Top 10 × Top 10)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Arrival Hub', fontsize=12)\n",
    "plt.ylabel('Origin Country', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117",
   "metadata": {},
   "source": [
    "### create pairs (origin, destination) for more detailed analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_packages_df['origin_destination'] = parsed_packages_df['origin_country'] + '_' + parsed_packages_df['destination_country']\n",
    "parsed_receptacles_df['origin_destination'] = parsed_receptacles_df['origin_country'] + '_' + parsed_receptacles_df['destination_country']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {},
   "source": [
    "listing the obtained values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('(origin_destination) pairs obtained for ')\n",
    "print(\"\\nfor parsed_packages_df :\")\n",
    "print(\"\\nNumber of unique values\", parsed_packages_df['origin_destination'].nunique())\n",
    "print(\"\\nValues : \")\n",
    "print(parsed_packages_df['origin_destination'].unique())\n",
    "print(\"\\nfor parsed_receptacles_df :\")\n",
    "print(\"\\nNumber of unique values\", parsed_receptacles_df['origin_destination'].nunique())\n",
    "print(\"\\nValues : \")\n",
    "print(parsed_receptacles_df['origin_destination'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121",
   "metadata": {},
   "source": [
    "### visualization of obtained results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122",
   "metadata": {},
   "source": [
    "### 1. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_dest_counts = parsed_packages_df['origin_destination'].value_counts()\n",
    "top_origin_dest_counts= origin_dest_counts.head(15)\n",
    "plt.figure(figsize=(14, 7))\n",
    "top_origin_dest_counts.plot(kind='bar')\n",
    "plt.title('Histogram of (origin, destination) Pairs Based on Package Counts')\n",
    "plt.xlabel('Origin_Destination')\n",
    "plt.ylabel('Number of Packages')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124",
   "metadata": {},
   "source": [
    "\n",
    "### 2. parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_dest_counts = parsed_receptacles_df['origin_destination'].value_counts()\n",
    "top_origin_dest_counts = origin_dest_counts.head(15)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "top_origin_dest_counts.plot(kind='bar')\n",
    "plt.title('Histogram of (origin, destination) Pairs Based on receptacle Counts')\n",
    "plt.xlabel('Origin_Destination')\n",
    "plt.ylabel('Number of receptacles')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126",
   "metadata": {},
   "source": [
    "## origin_destination X etablissments analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {},
   "source": [
    "## 1. current etablissment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128",
   "metadata": {},
   "source": [
    "### a. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_packages_df: visualize the histogram of counts by (origin_destination, etablissement_postal) pairs\n",
    "\n",
    "# First, select top N most common (origin_destination, etablissement_postal) pairs for clarity\n",
    "pair_counts_current = parsed_packages_df.groupby(['origin_destination', 'etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_current = pair_counts_current.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_current,\n",
    "    x='count',\n",
    "    y=top_pairs_current.apply(lambda x: f\"{x['origin_destination']} | {x['etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin-Destination, Current Etablissement) Pairs by Package Counts')\n",
    "plt.xlabel('Number of Packages')\n",
    "plt.ylabel('(Origin_Destination | Current Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130",
   "metadata": {},
   "source": [
    "### b. parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_receptacles_dffffff: visualize the histogram of counts by (origin_destination, etablissement_postal) pairs\n",
    "\n",
    "# First, select top N most common (origin_destination, etablissement_postal) pairs for clarity\n",
    "pair_counts_current_receptacle = parsed_receptacles_df.groupby(['origin_destination', 'etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_current_receptacle = pair_counts_current_receptacle.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_current_receptacle,\n",
    "    x='count',\n",
    "    y=top_pairs_current_receptacle.apply(lambda x: f\"{x['origin_destination']} | {x['etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin-Destination, Current Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Origin_Destination | Current Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132",
   "metadata": {},
   "source": [
    "we can see that the ETAB0002 is dominating and we remark that when the destination is DZ\n",
    "we'll try to confirm that by taking into consideration the destination only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_receptacles_dffff: visualize the histogram of counts by (destination_country, etablissement_postal) pairs\n",
    "\n",
    "# First, select top N most common (destination_country, etablissement_postal) pairs for clarity\n",
    "pair_counts_dest_receptacle = parsed_receptacles_df.groupby(['destination_country', 'etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_dest_receptacle = pair_counts_dest_receptacle.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_dest_receptacle,\n",
    "    x='count',\n",
    "    y=top_pairs_dest_receptacle.apply(lambda x: f\"{x['destination_country']} | {x['etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Destination Country, Current Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Destination Country | Current Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134",
   "metadata": {},
   "source": [
    "### This is to test the origin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_receptacles_dffffff: visualize the histogram of counts by (origin_country, etablissement_postal) pairs\n",
    "\n",
    "# First, select top N most common (origin_country, etablissement_postal) pairs for clarity\n",
    "pair_counts_origin_receptacle = parsed_receptacles_df.groupby(['origin_country', 'etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_origin_receptacle = pair_counts_origin_receptacle.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_origin_receptacle,\n",
    "    x='count',\n",
    "    y=top_pairs_origin_receptacle.apply(lambda x: f\"{x['origin_country']} | {x['etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin Country, Current Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Origin Country | Current Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136",
   "metadata": {},
   "source": [
    "most of the values with ETAB0002 values are european countries in addition to AE and China(CN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137",
   "metadata": {},
   "source": [
    "## 2. Next etablissement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138",
   "metadata": {},
   "source": [
    "### a. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_packages_df: visualize the histogram of counts by (origin_destination, next_etablissement_postal)\n",
    "\n",
    "# First, select top N most common (origin_destination, next_etablissement_postal) pairs for clarity\n",
    "pair_counts = parsed_packages_df.groupby(['origin_destination', 'next_etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs = pair_counts.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs,\n",
    "    x='count',\n",
    "    y=top_pairs.apply(lambda x: f\"{x['origin_destination']} | {x['next_etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin-Destination, Next Etablissement) Pairs by Package Counts')\n",
    "plt.xlabel('Number of Packages')\n",
    "plt.ylabel('(Origin_Destination | Next Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140",
   "metadata": {},
   "source": [
    "### b. parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_receptacles_dffff: visualize the histogram of counts by (origin_destination, next_etablissement_postal)\n",
    "\n",
    "# First, select top N most common (origin_destination, next_etablissement_postal) pairs for clarity\n",
    "pair_counts_recept = parsed_receptacles_df.groupby(['origin_destination', 'next_etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_recept = pair_counts_recept.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_recept,\n",
    "    x='count',\n",
    "    y=top_pairs_recept.apply(lambda x: f\"{x['origin_destination']} | {x['next_etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin-Destination, Next Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Origin_Destination | Next Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142",
   "metadata": {},
   "source": [
    "Do for origin and for destination separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For parsed_receptacles_df: visualize the histogram of counts by destination_country and next_etablissement_postal\n",
    "\n",
    "# First, select top N most common (destination_country, next_etablissement_postal) pairs for clarity\n",
    "pair_counts_dest = parsed_receptacles_df.groupby(['destination_country', 'next_etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_dest = pair_counts_dest.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_dest,\n",
    "    x='count',\n",
    "    y=top_pairs_dest.apply(lambda x: f\"{x['destination_country']} | {x['next_etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Destination Country, Next Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Destination_Country | Next Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_receptacles_df: visualize the histogram of counts by origin_country and next_etablissement_postal\n",
    "\n",
    "# First, select top N most common (origin_country, next_etablissement_postal) pairs for clarity\n",
    "pair_counts_origin = parsed_receptacles_df.groupby(['origin_country', 'next_etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_origin = pair_counts_origin.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_origin,\n",
    "    x='count',\n",
    "    y=top_pairs_origin.apply(lambda x: f\"{x['origin_country']} | {x['next_etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin Country, Next Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Origin_Country | Next Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145",
   "metadata": {},
   "source": [
    "from the analysis we can see that there are some etablissments that get congested forming a sort of loop (ETAB0030, ETAB0002, ETAB0006)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146",
   "metadata": {},
   "source": [
    "### Time analysis regarding origin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147",
   "metadata": {},
   "source": [
    "### 1. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the parsed_packages_df by date and origin_country to count packages per month per origin country\n",
    "parsed_packages_df['date'] = pd.to_datetime(parsed_packages_df['date'])\n",
    "\n",
    "# Group by origin_country and resample by month\n",
    "ts_by_origin = parsed_packages_df.groupby(['origin_country', pd.Grouper(key='date', freq='ME')])['MAILITM_FID'].count().reset_index()\n",
    "\n",
    "# Choose top 5 origin countries by total count for plotting\n",
    "top_origin = parsed_packages_df['origin_country'].value_counts().head(5).index\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "for origin in top_origin:\n",
    "    ts = ts_by_origin[ts_by_origin['origin_country'] == origin]\n",
    "    plt.plot(ts['date'], ts['MAILITM_FID'], marker='o', label=origin)\n",
    "\n",
    "plt.title('Monthly Package count by Origin Country')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Packages')\n",
    "plt.legend(title='Origin Country')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149",
   "metadata": {},
   "source": [
    "### 2. parsed_receptacles_dfffff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the parsed_receptacles_df by date and origin_country to count receptacles per month per origin country\n",
    "parsed_receptacles_df['date'] = pd.to_datetime(parsed_receptacles_df['date'])\n",
    "\n",
    "# Group by origin_country and resample by month\n",
    "ts_by_origin_recept = parsed_receptacles_df.groupby(['origin_country', pd.Grouper(key='date', freq='ME')])['RECPTCL_FID'].count().reset_index()\n",
    "\n",
    "# Choose top 5 origin countries by total count for plotting\n",
    "top_origin_recept = parsed_receptacles_df['origin_country'].value_counts().head(5).index\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "for origin in top_origin_recept:\n",
    "    ts = ts_by_origin_recept[ts_by_origin_recept['origin_country'] == origin]\n",
    "    plt.plot(ts['date'], ts['RECPTCL_FID'], marker='o', label=origin)\n",
    "\n",
    "plt.title('Monthly Receptacle count by Origin Country')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Receptacles')\n",
    "plt.legend(title='Origin Country')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151",
   "metadata": {},
   "source": [
    "### Time analysis by destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the parsed_packages_df by date and destination_country to count packages per month per destination country\n",
    "parsed_packages_df['date'] = pd.to_datetime(parsed_packages_df['date'])\n",
    "\n",
    "# Group by destination_country and resample by month\n",
    "ts_by_dest = parsed_packages_df.groupby(['destination_country', pd.Grouper(key='date', freq='ME')])['MAILITM_FID'].count().reset_index()\n",
    "\n",
    "# Choose top 5 destination countries by total count for plotting\n",
    "top_dest = parsed_packages_df['destination_country'].value_counts().head(5).index\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "for dest in top_dest:\n",
    "    ts = ts_by_dest[ts_by_dest['destination_country'] == dest]\n",
    "    plt.plot(ts['date'], ts['MAILITM_FID'], marker='o', label=dest)\n",
    "\n",
    "plt.title('Monthly Package count by Destination Country')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Packages')\n",
    "plt.legend(title='Destination Country')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153",
   "metadata": {},
   "source": [
    "### 2. parsed_receptacles_dffff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the parsed_receptacles_df by date and destination_country to count receptacles per month per destination country\n",
    "parsed_receptacles_df['date'] = pd.to_datetime(parsed_receptacles_df['date'])\n",
    "\n",
    "# Group by destination_country and resample by month \n",
    "ts_by_dest_recept = parsed_receptacles_df.groupby(['destination_country', pd.Grouper(key='date', freq='ME')])['RECPTCL_FID'].count().reset_index()\n",
    "\n",
    "# Choose top 5 destination countries by total count for plotting\n",
    "top_dest_recept = parsed_receptacles_df['destination_country'].value_counts().head(5).index\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "for dest in top_dest_recept:\n",
    "    ts = ts_by_dest_recept[ts_by_dest_recept['destination_country'] == dest]\n",
    "    plt.plot(ts['date'], ts['RECPTCL_FID'], marker='o', label=dest)\n",
    "\n",
    "plt.title('Monthly Receptacle count by Destination Country')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Receptacles')\n",
    "plt.legend(title='Destination Country')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155",
   "metadata": {},
   "source": [
    "## Creating additional features \n",
    " 1. 'flow_type' column with values: 'inbound' (to DZ), 'outbound' (from DZ), 'local' (DZ to DZ), otherwise 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_flow_type(df):\n",
    "    # Define the conditions\n",
    "    conditions = [\n",
    "        (df['destination_country'] == 'DZ') & (df['origin_country'] == 'DZ'), # local\n",
    "        (df['destination_country'] == 'DZ'),                                # inbound\n",
    "        (df['origin_country'] == 'DZ')                                     # outbound\n",
    "    ]\n",
    "    \n",
    "    # Define the results for each condition\n",
    "    choices = ['local', 'inbound', 'outbound']\n",
    "    \n",
    "    # Apply logic with 'other' as the default\n",
    "    return np.select(conditions, choices, default='other')\n",
    "\n",
    "# Apply to both DataFrames instantly\n",
    "parsed_packages_df['flow_type'] = get_flow_type(parsed_packages_df)\n",
    "parsed_receptacles_df['flow_type'] = get_flow_type(parsed_receptacles_df)\n",
    "\n",
    "# Print counts\n",
    "print(\"Flow type counts in parsed_packages_df:\\n\", parsed_packages_df['flow_type'].value_counts())\n",
    "print(\"\\nFlow type counts in parsed_receptacles_df:\\n\", parsed_receptacles_df['flow_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157",
   "metadata": {},
   "source": [
    "we can see that there are some values of flow type with the type \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 rows of origin_country and destination_country for flow_type 'other' in parsed_receptacles_df\n",
    "print(parsed_receptacles_df.loc[parsed_receptacles_df['flow_type'] == 'other', ['origin_country', 'destination_country']].head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159",
   "metadata": {},
   "source": [
    "## Analysis of the relation between the flow_type and the event_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160",
   "metadata": {},
   "source": [
    "## 1. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Analyse relation between flow_type and EVENT_TYPE_CD in parsed_packages_df\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "ax = sns.countplot(data=parsed_packages_df, x=\"EVENT_TYPE_CD\", hue=\"flow_type\", palette='Set2')\n",
    "plt.title(\"Distribution of EVENT_TYPE_CD by flow_type in parsed_packages_df\")\n",
    "plt.xlabel(\"EVENT_TYPE_CD\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"Flow Type\")\n",
    "plt.tight_layout()\n",
    "for c in ax.containers:\n",
    "    ax.bar_label(c, label_type='edge', fontsize=8, padding=2)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162",
   "metadata": {},
   "source": [
    "## 2. parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each flow_type, list the unique EVENT_TYPE_CD values and the most frequent EVENT_TYPE_CD value\n",
    "for flow in parsed_receptacles_df['flow_type'].unique():\n",
    "    event_types = parsed_receptacles_df.loc[parsed_receptacles_df['flow_type'] == flow, 'EVENT_TYPE_CD'].unique()\n",
    "    most_common_event_type = parsed_receptacles_df.loc[parsed_receptacles_df['flow_type'] == flow, 'EVENT_TYPE_CD'].mode()\n",
    "    print(f\"Flow type: {flow}\")\n",
    "    print(f\"EVENT_TYPE_CD values: {sorted(event_types)}\")\n",
    "    if not most_common_event_type.empty:\n",
    "        print(f\"Most frequent EVENT_TYPE_CD: {most_common_event_type.iloc[0]}\")\n",
    "    else:\n",
    "        print(\"No EVENT_TYPE_CD available\")\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "ax = sns.countplot(data=parsed_receptacles_df, x=\"EVENT_TYPE_CD\", hue=\"flow_type\", palette='Set1')\n",
    "plt.title(\"Distribution of EVENT_TYPE_CD by flow_type in parsed_receptacles_df\")\n",
    "plt.xlabel(\"EVENT_TYPE_CD\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"Flow Type\")\n",
    "plt.tight_layout()\n",
    "\n",
    "for c in ax.containers:\n",
    "    ax.bar_label(c, label_type='edge', fontsize=8, padding=2)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165",
   "metadata": {},
   "source": [
    "we can see that there are major event types related to the inbound flow type( coming to DZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166",
   "metadata": {},
   "source": [
    "# Track multiple receptacles just to see the flow of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track multiple receptacles: visualize all events for 5 different RECPTCL_FID in parsed_receptacles_df\n",
    "# Pick 5 unique RECPTCL_FID values to demonstrate\n",
    "num_examples = 5\n",
    "example_receptacle_ids = parsed_receptacles_df['RECPTCL_FID'].drop_duplicates().iloc[:num_examples]\n",
    "for rid in example_receptacle_ids:\n",
    "    print(\"\\n--- Events for RECPTCL_FID:\", rid, \"---\")\n",
    "    display(parsed_receptacles_df[parsed_receptacles_df['RECPTCL_FID'] == rid][['RECPTCL_FID', 'date', 'EVENT_TYPE_CD', 'etablissement_postal', 'next_etablissement_postal']].sort_values('date').reset_index(drop=True))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "for i, receptacle_id in enumerate(example_receptacle_ids):\n",
    "    ex_df = parsed_receptacles_df[parsed_receptacles_df['RECPTCL_FID'] == receptacle_id].sort_values('date')\n",
    "    plt.plot(\n",
    "        ex_df['date'],\n",
    "        ex_df['EVENT_TYPE_CD'],\n",
    "        marker='o',\n",
    "        label=f\"RECPTCL_FID: {receptacle_id}\"\n",
    "    )\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(f\"Event Timeline (EVENT_TYPE_CD) for {num_examples} Receptacles\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"EVENT_TYPE_CD\")\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df=parsed_packages_df.copy()\n",
    "receptacles_df=parsed_receptacles_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df['EVENT_TYPE_CD'] = packages_df['EVENT_TYPE_CD'].astype(str)\n",
    "receptacles_df['EVENT_TYPE_CD'] = receptacles_df['EVENT_TYPE_CD'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df['month'] = packages_df['date'].dt.month\n",
    "receptacles_df['month'] = receptacles_df['date'].dt.month\n",
    "packages_df['hour'] = packages_df['date'].dt.hour\n",
    "receptacles_df['hour'] = receptacles_df['date'].dt.hour\n",
    "packages_df['day_of_week'] = packages_df['date'].dt.day_name()\n",
    "receptacles_df['day_of_week'] = receptacles_df['date'].dt.day_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171",
   "metadata": {},
   "source": [
    "# **START OF MODELING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172",
   "metadata": {},
   "source": [
    "- we need to delete the first record of each package to not take into consideration the time it was inside its receptacle since this time would be calculated separatly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_first_pkg_row(df):\n",
    "    # keep='first' marks the first occurrence of each ID as False\n",
    "    # Subsequent occurrences are marked as True\n",
    "    is_not_first_row = df.duplicated(subset=['MAILITM_FID'], keep='first')\n",
    "\n",
    "    # Return the filtered dataframe WITHOUT resetting the index\n",
    "    return df[is_not_first_row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df = drop_first_pkg_row(packages_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176",
   "metadata": {},
   "source": [
    "### Adding the target `route_duration`\n",
    "* In the following cell, we're adding the target `route_duration` for each row.\n",
    "* for rows (of the same package/receptacle) having `next_etablissement_postal` different than `etablissement_postal` of their next row, we keep the value of `route_duration` NaN.\n",
    "* otherwise, we get the difference of `date` of the row and its next (of the same package/receptacle) in `hours` and store it in `route_duration`.\n",
    "- **NOTE**: We'll drop the first row of each package before adding the target `route_duration`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_target(df, id):\n",
    "\n",
    "    # 1. Sort the entire dataset chronologically\n",
    "    df = df.sort_values(by=[id, 'date'], ascending=True)\n",
    "\n",
    "    # ... ADD THE route_duration TARGET ...\n",
    "\n",
    "    df['next_event_date'] = df.groupby(id)['date'].shift(-1)\n",
    "    df['route_duration'] = (df['next_event_date'] - df['date']).dt.total_seconds() / 3600\n",
    "    # Create logical consistency mask m\n",
    "    df['next_row_etab'] = df.groupby(id)['etablissement_postal'].shift(-1)\n",
    "    valid_route_duration_mask = (df['next_etablissement_postal'] == df['next_row_etab'])\n",
    "\n",
    "    df.loc[~valid_route_duration_mask, 'route_duration'] = np.nan\n",
    "    df = df.drop(columns=['next_event_date', 'next_row_etab']) # to keep only to the original features.\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df = add_target(packages_df, 'MAILITM_FID')\n",
    "receptacles_df = add_target(receptacles_df, 'RECPTCL_FID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_duration_pkg = packages_df[packages_df['route_duration'] > 1000]\n",
    "large_duration_pkg.sort_values('route_duration', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_duration_rcptcl = receptacles_df[receptacles_df['route_duration'] > 1000]\n",
    "large_duration_rcptcl.sort_values('route_duration', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181",
   "metadata": {},
   "source": [
    "### Handling null values in the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df[packages_df['route_duration'].isna()].shape[0] / packages_df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183",
   "metadata": {},
   "source": [
    "* 75% of the rows of `packages_df` have null target, these will be dropped since target values needs to be **correct** and not imputed or guessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df = packages_df[~packages_df['route_duration'].isna()]\n",
    "packages_df.shape, packages_df['next_etablissement_postal'].isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185",
   "metadata": {},
   "source": [
    "* For `receptacles_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df[receptacles_df['route_duration'].isna()].shape[0] / receptacles_df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187",
   "metadata": {},
   "source": [
    "* 77% of the rows of `receptacles_df` have null target, these will be dropped since target values needs to be **correct** and not imputed or guessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df = receptacles_df[~receptacles_df['route_duration'].isna()]\n",
    "receptacles_df.shape, receptacles_df['next_etablissement_postal'].isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189",
   "metadata": {},
   "source": [
    "### Before splitting We add :\n",
    "  - `packages_per_receptacle`: number of packages per receptacle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_packages_per_receptacle(receptacles_df, packages_df):\n",
    "    # 1. Calculate counts directly from packages\n",
    "    package_counts = packages_df.groupby('RECPTCL_FID')['MAILITM_FID'].count().reset_index()\n",
    "    package_counts.columns = ['RECPTCL_FID', 'packages_per_receptacle']\n",
    "\n",
    "    # 2. Merge once onto the receptacles dataframe\n",
    "    receptacles_df = pd.merge(receptacles_df, package_counts, on='RECPTCL_FID', how='left')\n",
    "    \n",
    "    # 3. Fill empty receptacles with 0\n",
    "    receptacles_df['packages_per_receptacle'] = receptacles_df['packages_per_receptacle'].fillna(0)\n",
    "    \n",
    "    return receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df = add_packages_per_receptacle(receptacles_df, packages_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192",
   "metadata": {},
   "source": [
    "# Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193",
   "metadata": {},
   "source": [
    "- Since the data is dependent with time we need to take that into consideration and split into past and future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train_test_split(df):\n",
    "    df = df.sort_values('date')\n",
    "\n",
    "    split_idx = int(len(df) * 0.8)\n",
    "\n",
    "    train_df = df.iloc[:split_idx]\n",
    "    test_df = df.iloc[split_idx:]\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195",
   "metadata": {},
   "source": [
    "* Splitting `packages` and dataset into training and testing sets using `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pkg_X_train, pkg_X_test= my_train_test_split(packages_df)\n",
    "print (f\"Training set size: {pkg_X_train.shape[0]} rows\")\n",
    "print (f\"Testing set size: {pkg_X_test.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197",
   "metadata": {},
   "source": [
    "* Splitting `receptacles` and dataset into training and testing sets using `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcp_X_train, rcp_X_test= my_train_test_split(receptacles_df)\n",
    "print (f\"Training set size: {rcp_X_train.shape[0]} rows\")\n",
    "print (f\"Testing set size: {rcp_X_test.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199",
   "metadata": {},
   "source": [
    "# Adding New features \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200",
   "metadata": {},
   "source": [
    "### Dynamic Route load and Etablissement load\n",
    "* We'll add two features `route_load_1h` and `ETAB_load_1h` which for each row, calculates the number of packages/receptacles passing by a specific route (ETAB_XXXX -> ETAB_YYYY) and the number of packages/receptacles at a specific ETAB_XXXX for the previous 1h, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_etab_load_1h(df,id_col):\n",
    "    # 1. Store the original order/index\n",
    "    df = df.copy()\n",
    "    df['original_index'] = df.index \n",
    "    \n",
    "    # 2. Sort by etab and date for the rolling calculation\n",
    "    # We keep the unique original_index to map values back correctly\n",
    "    df_sorted = df.sort_values(['etablissement_postal', 'date'])\n",
    "    \n",
    "    # 3. Calculate rolling count\n",
    "    # We use 'date' as the window, but we keep it in the index alongside the original_index\n",
    "    rolling_series = (\n",
    "        df_sorted.set_index('date')\n",
    "        .groupby('etablissement_postal')[id_col]\n",
    "        .rolling('1h', closed='left')\n",
    "        .count()\n",
    "    )\n",
    "    \n",
    "    # 4. Map it back safely\n",
    "    # We reset the index of rolling_series to get a flat dataframe\n",
    "    # Then we align it back to the original dataframe\n",
    "    rolling_df = rolling_series.reset_index()\n",
    "    \n",
    "    # Since rolling and groupby can reorder rows, we merge on \n",
    "    # the specific columns to ensure every package gets its correct count\n",
    "    # To handle duplicates, we add a temporary 'sequence' within each millisecond\n",
    "    df_sorted['temp_seq'] = df_sorted.groupby(['etablissement_postal', 'date']).cumcount()\n",
    "    rolling_df['temp_seq'] = rolling_df.groupby(['etablissement_postal', 'date']).cumcount()\n",
    "    \n",
    "    df_final = pd.merge(\n",
    "        df_sorted, \n",
    "        rolling_df.rename(columns={id_col: 'etab_load_1h'}),\n",
    "        on=['etablissement_postal', 'date', 'temp_seq'],\n",
    "        how='left'\n",
    "    )\n",
    "    df_final['etab_load_1h'] = df_final['etab_load_1h'].fillna(0)\n",
    "    \n",
    "    return df_final.drop(columns=['temp_seq', 'original_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_route_load_1h(df,id_col):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Sort by route and date\n",
    "    # We use next_etablissement_postal which you've already filled\n",
    "    df_sorted = df.sort_values(['etablissement_postal', 'next_etablissement_postal', 'date'])\n",
    "    \n",
    "    # 2. Calculate rolling count for the specific LANE\n",
    "    rolling_series = (\n",
    "        df_sorted.set_index('date')\n",
    "        .groupby(['etablissement_postal', 'next_etablissement_postal'])[id_col]\n",
    "        .rolling('1h', closed='left')\n",
    "        .count()\n",
    "    )\n",
    "    \n",
    "    # 3. Flatten and prepare for merge\n",
    "    rolling_df = rolling_series.reset_index()\n",
    "    \n",
    "    # Handle duplicates with sequence counts\n",
    "    df_sorted['temp_seq'] = df_sorted.groupby(['etablissement_postal', 'next_etablissement_postal', 'date']).cumcount()\n",
    "    rolling_df['temp_seq'] = rolling_df.groupby(['etablissement_postal', 'next_etablissement_postal', 'date']).cumcount()\n",
    "    \n",
    "    # 4. Merge back\n",
    "    df_final = pd.merge(\n",
    "        df_sorted, \n",
    "        rolling_df.rename(columns={id_col: 'route_load_1h'}),\n",
    "        on=['etablissement_postal', 'next_etablissement_postal', 'date', 'temp_seq'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Fill NaNs with 0 (No other packages on this lane in the last 1h)\n",
    "    df_final['route_load_1h'] = df_final['route_load_1h'].fillna(0)\n",
    "    \n",
    "    return df_final.drop(columns=['temp_seq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203",
   "metadata": {},
   "source": [
    "### Time since first scan\n",
    "- The difference between the time of the actual scan with the first scan this will be usefull when giving the total duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_since_first_scan(df, id_col):\n",
    "    # Ensure we are looking at the same entity in the right order\n",
    "    df = df.sort_values([id_col, 'date'])\n",
    "    \n",
    "    # 1. TIME SINCE FIRST SCAN\n",
    "    # Subtract the minimum date of the group from the current date\n",
    "    df['time_since_first_scan'] = df.groupby(id_col)['date'].transform(\n",
    "        lambda x: (x - x.min()).dt.total_seconds() / 3600\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_time_since_last_scan(df,id_col):\n",
    "#     # Ensure we are looking at the same package in the right order\n",
    "#     df = df.sort_values([id_col, 'date'])\n",
    "\n",
    "#     # 1. TIME SINCE LAST SCAN\n",
    "#     # This is NOT cumulative.\n",
    "#     df['time_since_last_scan'] = df.groupby(id_col)['date'].diff().dt.total_seconds() / 3600\n",
    "#     df['time_since_last_scan'] = df['time_since_last_scan'].fillna(0)\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df in [pkg_X_train, pkg_X_test]:\n",
    "#     # 4 = Friday, 5 = Saturday\n",
    "#     df['is_weekend'] = df['date'].dt.dayofweek.isin([4, 5]).astype(int)\n",
    "#     df['first_last_week_day']= df['date'].dt.dayofweek.isin([3,6]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkg_X_train['country_service'] = pkg_X_train['origin_country'].astype(str) + \"_\" + pkg_X_train['service_indicator'].astype(str)\n",
    "# pkg_X_test['country_service'] = pkg_X_test['origin_country'].astype(str) + \"_\" + pkg_X_test['service_indicator'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208",
   "metadata": {},
   "source": [
    "- Adding features that represents events that could affect the flow of packages in algeria ``Note that this is only for packages because they are the ones that are related to Algeria``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "\n",
    "# 1. Setup Algeria Holidays (Keep this outside the function for speed)\n",
    "al_holidays = holidays.Algeria(years=[2023, 2024, 2025, 2026])\n",
    "holiday_dates = sorted(al_holidays.keys())\n",
    "holiday_df = pd.DataFrame({'holiday_date': pd.to_datetime(holiday_dates)})\n",
    "\n",
    "def add_holidays_features(df):\n",
    "    # Ensure Date is datetime and SORTED (merge_asof requires sorting)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').copy() # .copy() avoids SettingWithCopy warnings\n",
    "\n",
    "    # 3. Calculate \"Days Since Last Holiday\"\n",
    "    # direction='backward' looks for the last holiday <= current date\n",
    "    df = pd.merge_asof(df, holiday_df, left_on='date', right_on='holiday_date', direction='backward')\n",
    "    df['days_since_last_holiday'] = (df['date'] - df['holiday_date']).dt.days\n",
    "    df = df.drop(columns=['holiday_date']) # Drop it so the next merge doesn't conflict\n",
    "\n",
    "    # 4. Calculate \"Days Until Next Holiday\"\n",
    "    # direction='forward' looks for the next holiday >= current date\n",
    "    df = pd.merge_asof(df, holiday_df, left_on='date', right_on='holiday_date', direction='forward')\n",
    "    df['days_until_next_holiday'] = (df['holiday_date'] - df['date']).dt.days\n",
    "    df = df.drop(columns=['holiday_date'])\n",
    "\n",
    "    # 5. Clean up and Cap\n",
    "    df[['days_since_last_holiday', 'days_until_next_holiday']] = df[['days_since_last_holiday', 'days_until_next_holiday']].fillna(30)\n",
    "    \n",
    "    df['days_since_last_holiday'] = df['days_since_last_holiday'].clip(upper=30)\n",
    "    df['days_until_next_holiday'] = df['days_until_next_holiday'].clip(upper=30)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg_X_train = add_holidays_features(pkg_X_train)\n",
    "pkg_X_test = add_holidays_features(pkg_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211",
   "metadata": {},
   "source": [
    "- This will be tested later to see whether we can add it to receptacles or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import holidays\n",
    "\n",
    "# # 1. Pre-generate holidays for all required countries\n",
    "# # Example: Algeria, France, USA, etc.\n",
    "# SUPPORTED_COUNTRIES = ['Algeria', 'France', 'UnitedKingdom', 'UnitedStates']\n",
    "# all_holidays = []\n",
    "\n",
    "# for country in SUPPORTED_COUNTRIES:\n",
    "#     # Get holidays for each country\n",
    "#     country_hols = holidays.CountryHoliday(country, years=[2023, 2024, 2025, 2026])\n",
    "#     for date, name in country_hols.items():\n",
    "#         all_holidays.append({'holiday_date': pd.to_datetime(date), 'country_code': country})\n",
    "\n",
    "# # Create the master holiday DataFrame\n",
    "# holiday_df = pd.DataFrame(all_holidays).sort_values('holiday_date')\n",
    "\n",
    "# def add_holidays_features(df, country_col='country_name'):\n",
    "#     \"\"\"\n",
    "#     df: input dataframe\n",
    "#     country_col: the column containing the country name (must match holidays package names)\n",
    "#     \"\"\"\n",
    "#     df['date'] = pd.to_datetime(df['date'])\n",
    "#     df = df.sort_values('date').copy()\n",
    "\n",
    "#     # We will process each country group separately and concat them back\n",
    "#     processed_groups = []\n",
    "    \n",
    "#     for country, group in df.groupby(country_col):\n",
    "#         # Filter holiday_df for just this country\n",
    "#         country_hols = holiday_df[holiday_df['country_code'] == country]\n",
    "        \n",
    "#         if country_hols.empty:\n",
    "#             # If country not in holidays, set defaults immediately\n",
    "#             group['days_since_last_holiday'] = 30\n",
    "#             group['days_until_next_holiday'] = 30\n",
    "#         else:\n",
    "#             # Days Since Last Holiday\n",
    "#             group = pd.merge_asof(group, country_hols, left_on='date', right_on='holiday_date', direction='backward')\n",
    "#             group['days_since_last_holiday'] = (group['date'] - group['holiday_date']).dt.days\n",
    "#             group = group.drop(columns=['holiday_date', 'country_code'])\n",
    "\n",
    "#             # Days Until Next Holiday\n",
    "#             group = pd.merge_asof(group, country_hols, left_on='date', right_on='holiday_date', direction='forward')\n",
    "#             group['days_until_next_holiday'] = (group['holiday_date'] - group['date']).dt.days\n",
    "#             group = group.drop(columns=['holiday_date', 'country_code'])\n",
    "\n",
    "#         processed_groups.append(group)\n",
    "\n",
    "#     # Combine back into one dataframe\n",
    "#     df_final = pd.concat(processed_groups)\n",
    "\n",
    "#     # 5. Clean up and Cap (same as your original logic)\n",
    "#     cols = ['days_since_last_holiday', 'days_until_next_holiday']\n",
    "#     df_final[cols] = df_final[cols].fillna(30).clip(upper=30)\n",
    "\n",
    "#     return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213",
   "metadata": {},
   "source": [
    "### Creating the final function to create all the shared features at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_shared_features(df, id_col):\n",
    "    df = calculate_etab_load_1h(df, id_col)\n",
    "    df = calculate_route_load_1h(df, id_col)\n",
    "    df = add_time_since_first_scan(df, id_col)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg_X_train = create_all_shared_features(pkg_X_train, 'MAILITM_FID')\n",
    "pkg_X_test = create_all_shared_features(pkg_X_test, 'MAILITM_FID')\n",
    "rcp_X_train = create_all_shared_features(rcp_X_train, 'RECPTCL_FID')\n",
    "rcp_X_test = create_all_shared_features(rcp_X_test, 'RECPTCL_FID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216",
   "metadata": {},
   "source": [
    "- check for features creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg_X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcp_X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219",
   "metadata": {},
   "source": [
    "# Use CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220",
   "metadata": {},
   "source": [
    " ## 1. `For packages`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221",
   "metadata": {},
   "source": [
    "### Use Catboost with Huber loss function\n",
    "- Huber with delta=20 hours act as MAE for the ones with less than 20 hours and act as RMSE for the ones greater than delta.\n",
    "- This will solve the obssession of RMSE toward predicting the outliers(large values)\n",
    "- parameters are set manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222",
   "metadata": {},
   "source": [
    "- This could be run for testing since the hyperparameter tuning would take time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from catboost import CatBoostRegressor, Pool, cv\n",
    "\n",
    "# 1. Ensure chronological order\n",
    "pkg_X_train = pkg_X_train.sort_values('date').reset_index(drop=True)\n",
    "pkg_X_test = pkg_X_test.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# --- Define Model Parameters and Features ---\n",
    "TARGET_COL = 'route_duration'\n",
    "\n",
    "cat_features = [\n",
    "    'etablissement_postal',\n",
    "    'next_etablissement_postal',\n",
    "    'day_of_week',\n",
    "    'service_indicator',\n",
    "    'origin_destination',\n",
    "    'EVENT_TYPE_CD',\n",
    "    'hour',\n",
    "    'month',\n",
    "    # 'is_weekend',\n",
    "    # 'first_last_week_day',\n",
    "    # 'country_service'\n",
    "]\n",
    "\n",
    "# Columns to drop\n",
    "drop_cols = [TARGET_COL, 'MAILITM_FID', 'RECPTCL_FID', 'date', 'serial_number', \n",
    "             'flow_type', 'country_code', 'destination_country', 'origin_country']\n",
    "\n",
    "# Prepare the full 80% training set\n",
    "X_train_full = pkg_X_train.drop(columns=[c for c in drop_cols if c in pkg_X_train.columns])\n",
    "y_train_full = pkg_X_train[TARGET_COL]\n",
    "\n",
    "# Identify categorical features by name\n",
    "categorical_feature_names = [c for c in cat_features if c in X_train_full.columns]\n",
    "\n",
    "# --- TimeSeries Cross-Validation to Find Optimal Iterations ---\n",
    "\n",
    "# Create the training Pool\n",
    "train_pool = Pool(\n",
    "    data=X_train_full,\n",
    "    label=y_train_full,\n",
    "    cat_features=categorical_feature_names,\n",
    ")\n",
    "\n",
    "cv_params = {\n",
    "    'loss_function': 'Huber:delta=20.0',          #Hurber(acts as MAE for normal ones but acts as RMSE for outliers)\n",
    "    'eval_metric': 'MAE',            \n",
    "    'iterations': 1500, \n",
    "    'learning_rate': 0.03,\n",
    "    'depth': 8,                 \n",
    "    'random_seed': 42,\n",
    "    'verbose': 0,\n",
    "    # 'early_stopping_rounds': 100,\n",
    "    # 'l2_leaf_reg': 3,          # Standard regularization\n",
    "}\n",
    "\n",
    "print(\"Starting TimeSeries Cross-Validation (5 Folds) optimizing for MAE...\")\n",
    "\n",
    "cv_results = cv(\n",
    "    params=cv_params,\n",
    "    pool=train_pool,\n",
    "    fold_count=5,\n",
    "    shuffle=False, \n",
    "    type='TimeSeries', \n",
    ")\n",
    "\n",
    "# Find the best iteration based on the minimum average MAE\n",
    "# Note: cv_results keys change based on the loss_function used\n",
    "best_iter = cv_results['test-MAE-mean'].values.argmin()\n",
    "best_mae = cv_results['test-MAE-mean'].min()\n",
    "print(f\"Optimal number of CatBoost iterations: {best_iter + 1} (Best CV MAE: {best_mae:.4f})\")\n",
    "\n",
    "# --- Train Final Model on ENTIRE Training Set ---\n",
    "\n",
    "final_model = CatBoostRegressor(\n",
    "    iterations=best_iter + 1,\n",
    "    learning_rate=0.03,\n",
    "    depth=8,\n",
    "    loss_function='Huber:delta=20.0',            \n",
    "    eval_metric='MAE',              \n",
    "    random_seed=42,\n",
    "    cat_features=categorical_feature_names,\n",
    "    verbose=100 \n",
    ")\n",
    "\n",
    "print(\"\\nTraining final model on the entire 80% training set...\")\n",
    "final_model.fit(train_pool)\n",
    "#saving the model\n",
    "final_model.save_model(\"../models/final_pkg_route_duration_model.cbm\")\n",
    "print(\"Model trained and saved as 'final_pkg_route_duration_model.cbm'\")\n",
    "# --- Predict and Evaluate on the Future Test Set ---\n",
    "\n",
    "X_test_features = pkg_X_test.drop(columns=[c for c in drop_cols if c in pkg_X_test.columns])\n",
    "y_test_target = pkg_X_test[TARGET_COL]\n",
    "\n",
    "predictions = final_model.predict(X_test_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224",
   "metadata": {},
   "source": [
    "### Using Optuna for hyperparameters tuning before passing them to CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225",
   "metadata": {},
   "source": [
    "- Preparing the data pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool\n",
    "\n",
    "# ---  Data Preparation ---\n",
    "pkg_X_train = pkg_X_train.sort_values('date').reset_index(drop=True)\n",
    "pkg_X_test = pkg_X_test.sort_values('date').reset_index(drop=True)\n",
    "TARGET_COL = 'route_duration'\n",
    "cat_features = [\n",
    "    'etablissement_postal', 'next_etablissement_postal', 'day_of_week',\n",
    "    'service_indicator', 'origin_destination', 'EVENT_TYPE_CD', 'hour', 'month',\n",
    "    'is_weekend', 'first_last_week_day', 'service_country'\n",
    "]\n",
    "drop_cols = [TARGET_COL, 'MAILITM_FID', 'RECPTCL_FID', 'date', 'serial_number', \n",
    "             'flow_type', 'country_code', 'destination_country', 'origin_country']\n",
    "\n",
    "X_train_full = pkg_X_train.drop(columns=[c for c in drop_cols if c in pkg_X_train.columns])\n",
    "y_train_full = pkg_X_train[TARGET_COL]\n",
    "categorical_feature_names = [c for c in cat_features if c in X_train_full.columns]\n",
    "\n",
    "train_pool = Pool(data=X_train_full, label=y_train_full, cat_features=categorical_feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227",
   "metadata": {},
   "source": [
    "- This will find the best parameters and saves them into a json file so they can be used later without the need to do the tuning again unless changes are made in the data or the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import json\n",
    "from catboost import cv\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'loss_function': 'Huber:delta=20.0',\n",
    "        'eval_metric': 'MAE',\n",
    "        'custom_metric': ['RMSE'], # Ensure RMSE is calculated\n",
    "        'random_seed': 42,\n",
    "        'verbose': 0,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'depth': trial.suggest_int('depth', 4, 8),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "        'iterations': 1000, \n",
    "        'early_stopping_rounds': 30\n",
    "    }\n",
    "    \n",
    "    cv_results = cv(\n",
    "        params=params, \n",
    "        pool=train_pool, \n",
    "        fold_count=3, \n",
    "        shuffle=False, \n",
    "        type='TimeSeries',\n",
    "        plot=False\n",
    "    )\n",
    "    \n",
    "    # Track both metrics\n",
    "    best_mae = cv_results['test-MAE-mean'].min()\n",
    "    best_rmse = cv_results['test-RMSE-mean'].min()\n",
    "    \n",
    "    return best_mae, best_rmse\n",
    "\n",
    "# --- 2. Run Multi-Objective Optimization ---\n",
    "print(\"Starting Hyperparameter Tuning (Tracking MAE & RMSE)...\")\n",
    "study = optuna.create_study(directions=['minimize', 'minimize'])\n",
    "study.optimize(objective, n_trials=20) \n",
    "\n",
    "# --- 3. Pick the best trial and find optimal iterations ---\n",
    "# We pick the trial with the lowest MAE \n",
    "best_trial = min(study.best_trials, key=lambda t: t.values[0])\n",
    "\n",
    "print(f\"\\nBest Trial selected (MAE: {best_trial.values[0]:.4f}, RMSE: {best_trial.values[1]:.4f})\")\n",
    "\n",
    "# Find best iterations for the winning set on 5 folds for stability\n",
    "final_params_temp = {\n",
    "    'loss_function': 'Huber:delta=20.0', \n",
    "    'eval_metric': 'MAE', \n",
    "    **best_trial.params\n",
    "}\n",
    "\n",
    "print(\"Calculating final optimal iterations...\")\n",
    "final_cv = cv(params=final_params_temp, pool=train_pool, fold_count=5, shuffle=False, type='TimeSeries', verbose=0)\n",
    "best_iteration = int(final_cv['test-MAE-mean'].values.argmin() + 1)\n",
    "\n",
    "# --- 4. Save to JSON ---\n",
    "best_config = {\n",
    "    **best_trial.params,\n",
    "    \"iterations\": best_iteration,\n",
    "    \"loss_function\": 'Huber:delta=20.0',\n",
    "    \"eval_metric\": 'MAE',\n",
    "    \"final_mae\": best_trial.values[0],\n",
    "    \"final_rmse\": best_trial.values[1]\n",
    "}\n",
    "\n",
    "with open(\"catboost_pkg_best_params.json\", \"w\") as f:\n",
    "    json.dump(best_config, f, indent=4)\n",
    "\n",
    "print(f\"Tuning complete. Best parameters saved to 'catboost_pkg_best_params.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229",
   "metadata": {},
   "source": [
    "- Load the parameters from the json file and use them for the final training and then test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# --- 1. Load Parameters ---\n",
    "with open(\"catboost_pkg_best_params.json\", \"r\") as f:\n",
    "    best_config = json.load(f)\n",
    "\n",
    "# Extract iterations and remove metrics/metadata so CatBoost doesn't crash\n",
    "iters = best_config.pop(\"iterations\")\n",
    "best_config.pop(\"final_mae\", None)   # Remove if present\n",
    "best_config.pop(\"final_rmse\", None)  # Remove if present\n",
    "\n",
    "# --- 2. Initialize and Train Model ---\n",
    "# Ensure categorical_feature_names and train_pool are defined in your environment\n",
    "final_model = CatBoostRegressor(\n",
    "    iterations=iters,\n",
    "    **best_config,\n",
    "    cat_features=categorical_feature_names,\n",
    "    random_seed=42,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "print(\"Training final model with saved parameters...\")\n",
    "final_model.fit(train_pool)\n",
    "\n",
    "# --- 3. Save the actual Model weights ---\n",
    "final_model.save_model(\"final_pkg_route_duration_model.cbm\")\n",
    "print(\"Model trained and saved as 'final_pkg_route_duration_model.cbm'\")\n",
    "\n",
    "# --- 4. Predict on Test Set ---\n",
    "# Ensure pkg_X_test and drop_cols are defined in your environment\n",
    "X_test_features = pkg_X_test.drop(columns=[c for c in drop_cols if c in pkg_X_test.columns])\n",
    "predictions = final_model.predict(X_test_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231",
   "metadata": {},
   "source": [
    " ## 2. `For receptacles`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232",
   "metadata": {},
   "source": [
    "### Use Catboost with Huber loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from catboost import CatBoostRegressor, Pool, cv\n",
    "\n",
    "# 1. Ensure chronological order\n",
    "rcp_X_train = rcp_X_train.sort_values('date').reset_index(drop=True)\n",
    "rcp_X_test = rcp_X_test.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# --- Define Model Parameters and Features ---\n",
    "TARGET_COL = 'route_duration'\n",
    "\n",
    "cat_features = [\n",
    "    'etablissement_postal',\n",
    "    'next_etablissement_postal',\n",
    "    'day_of_week',\n",
    "    'origin_destination',\n",
    "    'EVENT_TYPE_CD',\n",
    "    'hour',\n",
    "    'month',\n",
    "    'flow_type',\n",
    "]\n",
    "\n",
    "# Columns to drop\n",
    "drop_cols = [TARGET_COL, 'RECPTCL_FID', 'date', 'destination_country', 'origin_country']\n",
    "\n",
    "# Prepare the full 80% training set\n",
    "X_train_full = rcp_X_train.drop(columns=[c for c in drop_cols if c in rcp_X_train.columns])\n",
    "y_train_full = rcp_X_train[TARGET_COL]\n",
    "\n",
    "# Identify categorical features by name\n",
    "categorical_feature_names = [c for c in cat_features if c in X_train_full.columns]\n",
    "\n",
    "# --- TimeSeries Cross-Validation to Find Optimal Iterations ---\n",
    "\n",
    "# Create the training Pool\n",
    "train_pool = Pool(\n",
    "    data=X_train_full,\n",
    "    label=y_train_full,\n",
    "    cat_features=categorical_feature_names,\n",
    ")\n",
    "\n",
    "cv_params = {\n",
    "    'loss_function': 'Huber:delta=20.0',          #  Hurber(acts as MAE for normal ones but acts as RMSE for outliers)\n",
    "    'eval_metric': 'MAE',            \n",
    "    'iterations': 1500, \n",
    "    'learning_rate': 0.03,\n",
    "    'depth': 8,                 \n",
    "    'random_seed': 42,\n",
    "    'verbose': 0,\n",
    "    # 'early_stopping_rounds': 100,\n",
    "    # 'l2_leaf_reg': 3,          # Standard regularization\n",
    "}\n",
    "\n",
    "print(\"Starting TimeSeries Cross-Validation (5 Folds) optimizing for MAE...\")\n",
    "\n",
    "cv_results = cv(\n",
    "    params=cv_params,\n",
    "    pool=train_pool,\n",
    "    fold_count=5,\n",
    "    shuffle=False, \n",
    "    type='TimeSeries', \n",
    ")\n",
    "\n",
    "# Find the best iteration based on the minimum average MAE\n",
    "# Note: cv_results keys change based on the loss_function used\n",
    "best_iter = cv_results['test-MAE-mean'].values.argmin()\n",
    "best_mae = cv_results['test-MAE-mean'].min()\n",
    "print(f\"Optimal number of CatBoost iterations: {best_iter + 1} (Best CV MAE: {best_mae:.4f})\")\n",
    "\n",
    "# --- Train Final Model on ENTIRE Training Set ---\n",
    "\n",
    "final_model = CatBoostRegressor(\n",
    "    iterations=best_iter + 1,\n",
    "    learning_rate=0.03,\n",
    "    depth=8,\n",
    "    loss_function='Huber:delta=20.0',            \n",
    "    eval_metric='MAE',              \n",
    "    random_seed=42,\n",
    "    cat_features=categorical_feature_names,\n",
    "    verbose=100 \n",
    ")\n",
    "\n",
    "print(\"\\nTraining final model on the entire 80% training set...\")\n",
    "final_model.fit(train_pool)\n",
    "\n",
    "# --- Predict and Evaluate on the Future Test Set ---\n",
    "\n",
    "X_test_features = rcp_X_test.drop(columns=[c for c in drop_cols if c in rcp_X_test.columns])\n",
    "y_test_target = rcp_X_test[TARGET_COL]\n",
    "\n",
    "predictions = final_model.predict(X_test_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234",
   "metadata": {},
   "source": [
    "### Using Optuna for hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool\n",
    "\n",
    "# ---  Data Preparation ---\n",
    "rcp_X_train = rcp_X_train.sort_values('date').reset_index(drop=True)\n",
    "rcp_X_test = rcp_X_test.sort_values('date').reset_index(drop=True)\n",
    "TARGET_COL = 'route_duration'\n",
    "cat_features = [\n",
    "    'etablissement_postal',\n",
    "    'next_etablissement_postal',\n",
    "    'day_of_week',\n",
    "    'origin_destination',\n",
    "    'EVENT_TYPE_CD',\n",
    "    'hour',\n",
    "    'month',\n",
    "    'flow_type',\n",
    "]\n",
    "drop_cols = [TARGET_COL, 'RECPTCL_FID', 'date', 'destination_country', 'origin_country']\n",
    "X_train_full = rcp_X_train.drop(columns=[c for c in drop_cols if c in rcp_X_train.columns])\n",
    "y_train_full = rcp_X_train[TARGET_COL]\n",
    "categorical_feature_names = [c for c in cat_features if c in X_train_full.columns]\n",
    "\n",
    "train_pool = Pool(data=X_train_full, label=y_train_full, cat_features=categorical_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import json\n",
    "from catboost import cv\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'loss_function': 'Huber:delta=20.0',\n",
    "        'eval_metric': 'MAE',\n",
    "        'custom_metric': ['RMSE'], # Ensure RMSE is calculated\n",
    "        'random_seed': 42,\n",
    "        'verbose': 0,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'depth': trial.suggest_int('depth', 4, 8),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "        'iterations': 1000, \n",
    "        'early_stopping_rounds': 30\n",
    "    }\n",
    "    \n",
    "    cv_results = cv(\n",
    "        params=params, \n",
    "        pool=train_pool, \n",
    "        fold_count=3, \n",
    "        shuffle=False, \n",
    "        type='TimeSeries',\n",
    "        plot=False\n",
    "    )\n",
    "    \n",
    "    # Track both metrics\n",
    "    best_mae = cv_results['test-MAE-mean'].min()\n",
    "    best_rmse = cv_results['test-RMSE-mean'].min()\n",
    "    \n",
    "    return best_mae, best_rmse\n",
    "\n",
    "# --- 2. Run Multi-Objective Optimization ---\n",
    "print(\"Starting Hyperparameter Tuning (Tracking MAE & RMSE)...\")\n",
    "study = optuna.create_study(directions=['minimize', 'minimize'])\n",
    "study.optimize(objective, n_trials=20) \n",
    "\n",
    "# --- 3. Pick the best trial and find optimal iterations ---\n",
    "# We pick the trial with the lowest MAE \n",
    "best_trial = min(study.best_trials, key=lambda t: t.values[0])\n",
    "\n",
    "print(f\"\\nBest Trial selected (MAE: {best_trial.values[0]:.4f}, RMSE: {best_trial.values[1]:.4f})\")\n",
    "\n",
    "# Find best iterations for the winning set on 5 folds for stability\n",
    "final_params_temp = {\n",
    "    'loss_function': 'Huber:delta=20.0', \n",
    "    'eval_metric': 'MAE', \n",
    "    **best_trial.params\n",
    "}\n",
    "\n",
    "print(\"Calculating final optimal iterations...\")\n",
    "final_cv = cv(params=final_params_temp, pool=train_pool, fold_count=5, shuffle=False, type='TimeSeries', verbose=0)\n",
    "best_iteration = int(final_cv['test-MAE-mean'].values.argmin() + 1)\n",
    "\n",
    "# --- 4. Save to JSON ---\n",
    "best_config = {\n",
    "    **best_trial.params,\n",
    "    \"iterations\": best_iteration,\n",
    "    \"loss_function\": 'Huber:delta=20.0',\n",
    "    \"eval_metric\": 'MAE',\n",
    "    \"final_mae\": best_trial.values[0],\n",
    "    \"final_rmse\": best_trial.values[1]\n",
    "}\n",
    "\n",
    "with open(\"catboost_rcp_best_params.json\", \"w\") as f:\n",
    "    json.dump(best_config, f, indent=4)\n",
    "\n",
    "print(f\"Tuning complete. Best parameters saved to 'catboost_rcp_best_params.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# --- 1. Load Parameters ---\n",
    "with open(\"catboost_rcp_best_params.json\", \"r\") as f:\n",
    "    best_config = json.load(f)\n",
    "\n",
    "# Extract iterations and remove metrics/metadata so CatBoost doesn't crash\n",
    "iters = best_config.pop(\"iterations\")\n",
    "best_config.pop(\"final_mae\", None)   # Remove if present\n",
    "best_config.pop(\"final_rmse\", None)  # Remove if present\n",
    "\n",
    "# --- 2. Initialize and Train Model ---\n",
    "# Ensure categorical_feature_names and train_pool are defined in your environment\n",
    "final_model = CatBoostRegressor(\n",
    "    iterations=iters,\n",
    "    **best_config,\n",
    "    cat_features=categorical_feature_names,\n",
    "    random_seed=42,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "print(\"Training final model with saved parameters...\")\n",
    "final_model.fit(train_pool)\n",
    "\n",
    "# --- 3. Save the actual Model weights ---\n",
    "final_model.save_model(\"final_rcp_route_duration_model.cbm\")\n",
    "print(\"Model trained and saved as 'final_rcp_route_duration_model.cbm'\")\n",
    "\n",
    "# --- 4. Predict on Test Set ---\n",
    "# Ensure rcp_X_test and drop_cols are defined in your environment\n",
    "X_test_features = rcp_X_test.drop(columns=[c for c in drop_cols if c in rcp_X_test.columns])\n",
    "predictions = final_model.predict(X_test_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239",
   "metadata": {},
   "source": [
    "- choose which one to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test=pkg_X_test.copy()\n",
    "X_test=rcp_X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error,root_mean_squared_error\n",
    "y_test_true = X_test[TARGET_COL]\n",
    "y_test_pred_subset = predictions\n",
    "    \n",
    "final_rmse = root_mean_squared_error(y_test_true, y_test_pred_subset)\n",
    "final_mae = mean_absolute_error(y_test_true, y_test_pred_subset)\n",
    "\n",
    "print(f\"\\nFinal Model Evaluation on Test Set:\")\n",
    "print(f\"RMSE: {final_rmse:.4f} hours\")\n",
    "print(f\"MAE: {final_mae:.4f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Importance Analysis ---\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the feature importances from the trained model\n",
    "feature_importances = final_model.get_feature_importance()\n",
    "feature_names = X_train_full.columns\n",
    "\n",
    "# Create a Series for easy sorting and handling\n",
    "importance_series = pd.Series(feature_importances, index=feature_names)\n",
    "\n",
    "# Sort the features by importance (descending)\n",
    "sorted_importance = importance_series.sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n--- Feature Importances (Top 10) ---\")\n",
    "print(sorted_importance.head(10))\n",
    "\n",
    "# Optional: Plot the top 10 features\n",
    "plt.figure(figsize=(10, 6))\n",
    "sorted_importance.head(10).plot(kind='barh', color='skyblue')\n",
    "plt.title('Top 10 CatBoost Feature Importances')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = final_model.get_feature_importance(type='Interaction', prettified=True)\n",
    "print(\"\\nTop Feature Interactions:\")\n",
    "print(interactions.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244",
   "metadata": {},
   "source": [
    "### Analysis of the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate residuals\n",
    "y_test_target = X_test[TARGET_COL]\n",
    "residuals = y_test_target - predictions\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=predictions, y=residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Residual Plot: Prediction vs. Error')\n",
    "plt.xlabel('Predicted Route Duration (Hours)')\n",
    "plt.ylabel('Residuals (Actual - Predicted)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate the raw error (not absolute)\n",
    "results = pd.DataFrame({\n",
    "    'Actual': y_test_target,\n",
    "    'Predicted': predictions\n",
    "})\n",
    "results['Residual'] = results['Predicted'] - results['Actual']\n",
    "\n",
    "# 2. Filter for large errors (greater than 50 hours)\n",
    "big_errors = results[np.abs(results['Residual']) > 50]\n",
    "\n",
    "# 3. Count which is more common\n",
    "missed_route_duration = big_errors[big_errors['Residual'] < 0].shape[0]\n",
    "model_hallucinations = big_errors[big_errors['Residual'] > 0].shape[0]\n",
    "\n",
    "print(f\"Total Large Errors (>50h): {len(big_errors)}\")\n",
    "print(f\"---\")\n",
    "print(f\"Missed route_duration (Under-predicted): {missed_route_duration}\")\n",
    "print(f\"Model Hallucinations (Over-predicted): {model_hallucinations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247",
   "metadata": {},
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline\n",
    "This section consolidates all the preprocessing steps defined above into a reusable pipeline that can be applied to `X_train_full` and `y_train_full`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Ensure chronological order\n",
    "pkg_X_train = pkg_X_train.sort_values('date').reset_index(drop=True)\n",
    "pkg_X_test = pkg_X_test.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# --- Define Model Parameters and Features ---\n",
    "TARGET_COL = 'route_duration'\n",
    "\n",
    "# List of categorical features (Kept country_service as it was helping!)\n",
    "cat_features = [\n",
    "    'etablissement_postal',\n",
    "    'next_etablissement_postal',\n",
    "    'day_of_week',\n",
    "    'service_indicator',\n",
    "    'origin_destination',\n",
    "    'EVENT_TYPE_CD',\n",
    "    'hour',\n",
    "    'month',\n",
    "]\n",
    "\n",
    "# Columns to drop\n",
    "drop_cols = [TARGET_COL, 'MAILITM_FID', 'RECPTCL_FID', 'date', 'serial_number',\n",
    "             'flow_type', 'country_code', 'destination_country', 'origin_country']\n",
    "\n",
    "# Prepare the full 80% training set\n",
    "X_train_full = pkg_X_train.drop(columns=[c for c in drop_cols if c in pkg_X_train.columns])\n",
    "y_train_full = pkg_X_train[TARGET_COL]\n",
    "\n",
    "# Identify categorical features by name\n",
    "categorical_feature_names = [c for c in cat_features if c in X_train_full.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import necessary libraries ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from category_encoders import CountEncoder\n",
    "\n",
    "# --- Define feature groups based on encoding strategy ---\n",
    "# Low cardinality features: One-Hot Encoding\n",
    "ONE_HOT_COLS = ['EVENT_TYPE_CD', 'day_of_week']\n",
    "\n",
    "# High cardinality features: Target Encoding\n",
    "HIGH_CARDINALITY_COLS = ['etablissement_postal', 'next_etablissement_postal']\n",
    "\n",
    "# Medium cardinality features: Count Encoding\n",
    "MEDIUM_CARDINALITY_COLS = ['origin_country', 'service_indicator']\n",
    "\n",
    "# Cyclical features: Sin/Cos Transformation\n",
    "CYCLICAL_COLS = ['hour', 'month']\n",
    "\n",
    "# Numerical features that should be scaled\n",
    "NUMERICAL_COLS = [\n",
    "    'etab_load_1h', \n",
    "    'route_load_1h', \n",
    "    'time_since_first_scan',\n",
    "    'days_since_last_holiday',\n",
    "    'days_until_next_holiday'\n",
    "]\n",
    "\n",
    "print(\"Feature groups defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define encoding functions ---\n",
    "def cyclical_transformer(period):\n",
    "    \"\"\"Create a cyclical encoding function for a given period.\"\"\"\n",
    "    def transform(X):\n",
    "        X = np.asarray(X).astype(float)\n",
    "        sin = np.sin(2 * np.pi * X / period)\n",
    "        cos = np.cos(2 * np.pi * X / period)\n",
    "        return np.c_[sin, cos]\n",
    "    return transform\n",
    "\n",
    "# Hour transformer (24-hour cycle)\n",
    "hour_transformer = FunctionTransformer(\n",
    "    cyclical_transformer(24),\n",
    "    feature_names_out=lambda transformer, names: [\"hour_sin\", \"hour_cos\"]\n",
    ")\n",
    "\n",
    "# Month transformer (12-month cycle)\n",
    "month_transformer = FunctionTransformer(\n",
    "    cyclical_transformer(12),\n",
    "    feature_names_out=lambda transformer, names: [\"month_sin\", \"month_cos\"]\n",
    ")\n",
    "\n",
    "print(\"Cyclical transformers created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build the preprocessing pipeline function ---\n",
    "def create_preprocessing_pipeline(\n",
    "    one_hot_cols=ONE_HOT_COLS,\n",
    "    high_cardinality_cols=HIGH_CARDINALITY_COLS,\n",
    "    medium_cardinality_cols=MEDIUM_CARDINALITY_COLS,\n",
    "    numerical_cols=NUMERICAL_COLS,\n",
    "    scale_numerical=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a comprehensive preprocessing pipeline for the packages dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    one_hot_cols : list\n",
    "        Columns for one-hot encoding (low cardinality categorical)\n",
    "    high_cardinality_cols : list\n",
    "        Columns for target encoding (high cardinality categorical)\n",
    "    medium_cardinality_cols : list\n",
    "        Columns for count encoding (medium cardinality categorical)\n",
    "    numerical_cols : list\n",
    "        Numerical columns to scale\n",
    "    scale_numerical : bool\n",
    "        Whether to apply StandardScaler to numerical features\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    sklearn.compose.ColumnTransformer\n",
    "        A preprocessing pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    transformers = [\n",
    "        # Cyclical encoding for hour\n",
    "        ('hour_cyclical', hour_transformer, ['hour']),\n",
    "        \n",
    "        # Cyclical encoding for month\n",
    "        ('month_cyclical', month_transformer, ['month']),\n",
    "        \n",
    "        # One-hot encoding for low cardinality categorical features\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False), one_hot_cols),\n",
    "        \n",
    "        # Target encoding for high cardinality features\n",
    "        ('target_enc', TargetEncoder(), high_cardinality_cols),\n",
    "        \n",
    "        # Count encoding for medium cardinality features\n",
    "        ('count_enc', CountEncoder(cols=medium_cardinality_cols), medium_cardinality_cols),\n",
    "    ]\n",
    "    \n",
    "    # Add numerical scaler if requested\n",
    "    if scale_numerical and numerical_cols:\n",
    "        transformers.append(\n",
    "            ('num_scaler', StandardScaler(), numerical_cols)\n",
    "        )\n",
    "    elif numerical_cols:\n",
    "        transformers.append(\n",
    "            ('num_passthrough', 'passthrough', numerical_cols)\n",
    "        )\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=transformers,\n",
    "        remainder='drop',  # Drop any columns not explicitly handled\n",
    "        verbose_feature_names_out=True\n",
    "    )\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "print(\"Pipeline function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper function to preprocess data ---\n",
    "def preprocess_data(X, y=None, preprocessor=None, fit=True):\n",
    "    \"\"\"\n",
    "    Apply preprocessing to the data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pd.DataFrame\n",
    "        Feature dataframe\n",
    "    y : pd.Series, optional\n",
    "        Target variable (required for target encoding during fit)\n",
    "    preprocessor : ColumnTransformer, optional\n",
    "        Pre-fitted preprocessor (for transform only)\n",
    "    fit : bool\n",
    "        Whether to fit the preprocessor or just transform\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (X_processed, preprocessor)\n",
    "        Processed features and the fitted preprocessor\n",
    "    \"\"\"\n",
    "    if preprocessor is None:\n",
    "        preprocessor = create_preprocessing_pipeline()\n",
    "    \n",
    "    # Set output to pandas DataFrame\n",
    "    preprocessor.set_output(transform='pandas')\n",
    "    \n",
    "    if fit:\n",
    "        if y is None:\n",
    "            raise ValueError(\"y is required for fitting (needed for TargetEncoder)\")\n",
    "        X_processed = preprocessor.fit_transform(X, y)\n",
    "    else:\n",
    "        X_processed = preprocessor.transform(X)\n",
    "    \n",
    "    return X_processed, preprocessor\n",
    "\n",
    "print(\"Preprocessing helper function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254",
   "metadata": {},
   "source": [
    "## Apply the pipeline to X_train_full and y_train_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Apply the pipeline to X_train_full and y_train_full ---\n",
    "print(\"Creating preprocessing pipeline...\")\n",
    "\n",
    "# Filter columns that exist in X_train_full\n",
    "available_one_hot = [c for c in ONE_HOT_COLS if c in X_train_full.columns]\n",
    "available_high_card = [c for c in HIGH_CARDINALITY_COLS if c in X_train_full.columns]\n",
    "available_medium_card = [c for c in MEDIUM_CARDINALITY_COLS if c in X_train_full.columns]\n",
    "available_numerical = [c for c in NUMERICAL_COLS if c in X_train_full.columns]\n",
    "\n",
    "print(f\"One-hot encoding columns: {available_one_hot}\")\n",
    "print(f\"Target encoding columns: {available_high_card}\")\n",
    "print(f\"Count encoding columns: {available_medium_card}\")\n",
    "print(f\"Numerical columns: {available_numerical}\")\n",
    "\n",
    "# Create the preprocessor with available columns\n",
    "preprocessor = create_preprocessing_pipeline(\n",
    "    one_hot_cols=available_one_hot,\n",
    "    high_cardinality_cols=available_high_card,\n",
    "    medium_cardinality_cols=available_medium_card,\n",
    "    numerical_cols=available_numerical,\n",
    "    scale_numerical=True\n",
    ")\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_processed, fitted_preprocessor = preprocess_data(\n",
    "    X_train_full, \n",
    "    y_train_full, \n",
    "    preprocessor=preprocessor, \n",
    "    fit=True\n",
    ")\n",
    "\n",
    "print(f\"\\nOriginal X_train_full shape: {X_train_full.shape}\")\n",
    "print(f\"Processed X_train shape: {X_train_processed.shape}\")\n",
    "print(f\"\\nProcessed feature names:\")\n",
    "print(X_train_processed.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Transform test data using the fitted preprocessor ---\n",
    "# Prepare X_test_full similar to X_train_full\n",
    "X_test_full = pkg_X_test.drop(columns=[c for c in drop_cols if c in pkg_X_test.columns])\n",
    "y_test_full = pkg_X_test[TARGET_COL]\n",
    "\n",
    "# Transform test data (fit=False to avoid data leakage)\n",
    "X_test_processed, _ = preprocess_data(\n",
    "    X_test_full,\n",
    "    preprocessor=fitted_preprocessor,\n",
    "    fit=False\n",
    ")\n",
    "\n",
    "print(f\"X_test_full shape: {X_test_full.shape}\")\n",
    "print(f\"X_test_processed shape: {X_test_processed.shape}\")\n",
    "\n",
    "# Display sample of processed data\n",
    "print(\"\\n--- Sample of processed training data ---\")\n",
    "X_train_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257",
   "metadata": {},
   "source": [
    "## KNN Regressor with RandomizedSearchCV\n",
    "Using the processed data from the preprocessing pipeline to train a KNN Regressor with hyperparameter tuning via RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, make_scorer\n",
    "from scipy.stats import randint, uniform\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'n_neighbors': randint(20, 25),           # Number of neighbors (3 to 50)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TimeSeriesSplit to avoid data leakage in time-series data\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Initialize KNN Regressor\n",
    "knn_base = KNeighborsRegressor(n_jobs=-1)\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "# Using negative MAE as scoring (sklearn convention: higher is better)\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=knn_base,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=5,                    # Number of random combinations to try\n",
    "    scoring='neg_mean_absolute_error',  # Optimize for MAE\n",
    "    cv=tscv,                      # TimeSeriesSplit cross-validation\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,                    # Use all available cores\n",
    "    return_train_score=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the random search\n",
    "random_search.fit(X_train_processed, y_train_full)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RandomizedSearchCV Complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest Cross-Validation MAE: {-random_search.best_score_:.4f} hours\")\n",
    "\n",
    "# Get the best model\n",
    "best_knn_model = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save best model using joblib \n",
    "import joblib\n",
    "filename = 'knn-model.joblib'\n",
    "joblib.dump(best_knn_model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264",
   "metadata": {},
   "source": [
    "## Evaluate Best KNN Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_predictions = best_knn_model.predict(X_test_processed)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "knn_mae = mean_absolute_error(y_test_full, knn_predictions)\n",
    "knn_rmse = root_mean_squared_error(y_test_full, knn_predictions)\n",
    "\n",
    "# MAE as percentage of mean\n",
    "mean_actual = np.mean(y_test_full)\n",
    "knn_mae_pct = (knn_mae / mean_actual) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KNN Regressor - Test Set Evaluation\")\n",
    "print(\"=\"*60)\n",
    "print(f\"MAE:  {knn_mae:.4f} hours\")\n",
    "print(f\"RMSE: {knn_rmse:.4f} hours\")\n",
    "print(f\"\\nMean Actual route duration: {mean_actual:.4f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "knn_residuals = y_test_full - knn_predictions\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Actual vs Predicted\n",
    "axes[0].scatter(y_test_full, knn_predictions, alpha=0.3, s=10)\n",
    "axes[0].plot([y_test_full.min(), y_test_full.max()], \n",
    "             [y_test_full.min(), y_test_full.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual route_duration (hours)')\n",
    "axes[0].set_ylabel('Predicted route_duration (hours)')\n",
    "axes[0].set_title('KNN: Actual vs Predicted')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Residual Distribution\n",
    "axes[1].hist(knn_residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Residual (Actual - Predicted)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('KNN: Residual Distribution')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Residuals vs Predicted\n",
    "axes[2].scatter(knn_predictions, knn_residuals, alpha=0.3, s=10)\n",
    "axes[2].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[2].set_xlabel('Predicted route_duration (hours)')\n",
    "axes[2].set_ylabel('Residual (hours)')\n",
    "axes[2].set_title('KNN: Residuals vs Predicted')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print residual statistics\n",
    "print(\"\\nResidual Statistics:\")\n",
    "print(f\"  Mean Residual: {np.mean(knn_residuals):.4f} hours\")\n",
    "print(f\"  Std Residual: {np.std(knn_residuals):.4f} hours\")\n",
    "print(f\"  Min Residual: {np.min(knn_residuals):.4f} hours\")\n",
    "print(f\"  Max Residual: {np.max(knn_residuals):.4f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267",
   "metadata": {},
   "source": [
    "## KNN with Log-Transformed Target\n",
    "Since the delay has a long right tail, we'll try log-transforming the target to see if it improves KNN performance.\n",
    "\n",
    "**Why this should help:**\n",
    "- KNN averages neighbors' values, so extreme outliers dominate predictions\n",
    "- Log transformation compresses outliers (7000h → ~8.85 in log-space)\n",
    "- Creates a more symmetric distribution better suited for distance-based averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Original distribution\n",
    "axes[0].hist(y_train_full, bins=100, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('route_duration (hours)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Original route_duration Distribution')\n",
    "axes[0].axvline(y_train_full.mean(), color='r', linestyle='--', label=f'Mean: {y_train_full.mean():.2f}')\n",
    "axes[0].axvline(y_train_full.median(), color='g', linestyle='--', label=f'Median: {y_train_full.median():.2f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Log-transformed distribution (using log1p to handle zeros)\n",
    "y_train_log = np.log1p(y_train_full)\n",
    "axes[1].hist(y_train_log, bins=100, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1].set_xlabel('Log(1 + route_duration)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Log-Transformed route_duration Distribution')\n",
    "axes[1].axvline(y_train_log.mean(), color='r', linestyle='--', label=f'Mean: {y_train_log.mean():.2f}')\n",
    "axes[1].axvline(y_train_log.median(), color='g', linestyle='--', label=f'Median: {y_train_log.median():.2f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Original - Skewness: {y_train_full.skew():.2f}\")\n",
    "print(f\"Log-transformed - Skewness: {y_train_log.skew():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_log = np.log1p(y_train_full)  # log(1 + y) to handle zeros\n",
    "y_test_log = np.log1p(y_test_full)\n",
    "\n",
    "print(f\"Training target - Original range: [{y_train_full.min():.2f}, {y_train_full.max():.2f}]\")\n",
    "print(f\"Training target - Log range: [{y_train_log.min():.2f}, {y_train_log.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_distributions_log = {\n",
    "    'n_neighbors': randint(9, 30),\n",
    "}\n",
    "\n",
    "# Set up TimeSeriesSplit\n",
    "tscv_log = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Initialize KNN and RandomizedSearchCV\n",
    "knn_log = KNeighborsRegressor(n_jobs=-1)\n",
    "\n",
    "random_search_log = RandomizedSearchCV(\n",
    "    estimator=knn_log,\n",
    "    param_distributions=param_distributions_log,\n",
    "    n_iter=5,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=tscv_log,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"RandomizedSearchCV configured for log-transformed target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting RandomizedSearchCV for KNN with log-transformed target...\")\n",
    "print(f\"Training data shape: {X_train_processed.shape}\")\n",
    "print(f\"Log target shape: {y_train_log.shape}\")\n",
    "print(\"This may take a while...\\n\")\n",
    "\n",
    "random_search_log.fit(X_train_processed, y_train_log)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RandomizedSearchCV Complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Display Best Hyperparameters (Log Target) ---\n",
    "print(\"Best Hyperparameters for KNN with Log-Transformed Target:\")\n",
    "print(\"-\" * 50)\n",
    "for param, value in random_search_log.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"\\nBest CV Score (Neg MSE on log scale): {random_search_log.best_score_:.4f}\")\n",
    "print(f\"Best CV RMSE (on log scale): {np.sqrt(-random_search_log.best_score_):.4f}\")\n",
    "\n",
    "# Get the best model\n",
    "best_knn_log = random_search_log.best_estimator_\n",
    "print(f\"\\nBest KNN Model (Log Target): {best_knn_log}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate KNN (Log Target) on Test Set ---\n",
    "# Predict on log scale\n",
    "y_pred_log = best_knn_log.predict(X_test_processed)\n",
    "\n",
    "# Inverse transform to get predictions on original scale\n",
    "y_pred_original = np.expm1(y_pred_log)\n",
    "\n",
    "# Ensure no negative predictions (can happen due to log transform edge cases)\n",
    "y_pred_original = np.maximum(y_pred_original, 0)\n",
    "\n",
    "print(\"Predictions converted back to original scale using exp(x) - 1\")\n",
    "print(f\"Prediction range: [{y_pred_original.min():.2f}, {y_pred_original.max():.2f}] hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Calculate metrics on original scale for fair comparison\n",
    "knn_log_mae = mean_absolute_error(y_test_full, y_pred_original)\n",
    "knn_log_rmse = np.sqrt(mean_squared_error(y_test_full, y_pred_original))\n",
    "knn_log_mape = np.mean(np.abs((y_test_full - y_pred_original) / (y_test_full + 1e-8))) * 100\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"KNN with Log-Transformed Target - Test Set Performance\")\n",
    "print(\"(Evaluated on ORIGINAL scale after inverse transform)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  MAE:  {knn_log_mae:.2f} hours\")\n",
    "print(f\"  RMSE: {knn_log_rmse:.2f} hours\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize Predictions: Original vs Log Target KNN ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Get predictions from original KNN for comparison\n",
    "y_pred_original_knn = best_knn_model.predict(X_test_processed)\n",
    "\n",
    "# 1. Actual vs Predicted scatter plot\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(y_test_full, y_pred_original_knn, alpha=0.3, label='Original Target', s=10)\n",
    "ax1.scatter(y_test_full, y_pred_original, alpha=0.3, label='Log Target', s=10)\n",
    "max_val = max(y_test_full.max(), y_pred_original_knn.max(), y_pred_original.max())\n",
    "ax1.plot([0, max_val], [0, max_val], 'r--', label='Perfect Prediction')\n",
    "ax1.set_xlabel('Actual route_duration (hours)')\n",
    "ax1.set_ylabel('Predicted route_duration (hours)')\n",
    "ax1.set_title('Actual vs Predicted')\n",
    "ax1.legend()\n",
    "ax1.set_xlim(0, np.percentile(y_test_full, 99))\n",
    "ax1.set_ylim(0, np.percentile(y_test_full, 99))\n",
    "\n",
    "# 2. Residual distribution\n",
    "ax2 = axes[1]\n",
    "residuals_orig = y_test_full - y_pred_original_knn\n",
    "residuals_log = y_test_full - y_pred_original\n",
    "ax2.hist(residuals_orig, bins=50, alpha=0.5, label=f'Original (std={residuals_orig.std():.1f})', density=True)\n",
    "ax2.hist(residuals_log, bins=50, alpha=0.5, label=f'Log (std={residuals_log.std():.1f})', density=True)\n",
    "ax2.axvline(x=0, color='r', linestyle='--')\n",
    "ax2.set_xlabel('Residual (hours)')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.set_title('Residual Distribution')\n",
    "ax2.legend()\n",
    "ax2.set_xlim(-100, 100)\n",
    "\n",
    "# 3. Prediction distribution comparison\n",
    "ax3 = axes[2]\n",
    "ax3.hist(y_pred_original_knn, bins=50, alpha=0.5, label='Original Target KNN', density=True)\n",
    "ax3.hist(y_pred_original, bins=50, alpha=0.5, label='Log Target KNN', density=True)\n",
    "ax3.hist(y_test_full, bins=50, alpha=0.3, label='Actual', density=True)\n",
    "ax3.set_xlabel('route_duration (hours)')\n",
    "ax3.set_ylabel('Density')\n",
    "ax3.set_title('Prediction Distribution vs Actual')\n",
    "ax3.legend()\n",
    "ax3.set_xlim(0, np.percentile(y_test_full, 99))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277",
   "metadata": {},
   "source": [
    "## 1. For Packages\n",
    "\n",
    "**Goal:** Identify abnormal delivery durations in package test data.\n",
    "\n",
    "**Approach:** We'll use multiple anomaly detection techniques to flag suspicious routes:\n",
    "- **Statistical Methods** (Z-Score & IQR) - Detect extreme residuals from baseline predictions\n",
    "- **Isolation Forest** - Identify unusual feature patterns\n",
    "- **Ensemble Scoring** - Combine methods for robust anomaly confidence scores\n",
    "\n",
    "**Output:** Labeled dataset with anomaly scores and flags for high-confidence anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278",
   "metadata": {},
   "source": [
    "## 1.1 Statistical Methods (Z-Score & IQR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279",
   "metadata": {},
   "source": [
    "## 1.2 Isolation Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280",
   "metadata": {},
   "source": [
    "## 1.3 Ensemble Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281",
   "metadata": {},
   "source": [
    "## 1.4 Anomaly Characterization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282",
   "metadata": {},
   "source": [
    "## 1.5 Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283",
   "metadata": {},
   "source": [
    "## 1.6 Save Results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
