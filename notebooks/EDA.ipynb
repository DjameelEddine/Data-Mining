{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "make sure to rename the columns by removing é\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) for Packages and Receptacle Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "packages_df = pd.read_csv('../data/raw/packages_data_2023_2025.csv',delimiter=';', encoding='latin-1')\n",
    "receptacles_df = pd.read_csv('../data/raw/receptacle_data_2023_2025.csv',delimiter=';', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "* Columns' names adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df = packages_df.rename(columns={'établissement_postal': 'etablissement_postal', 'next_établissement_postal': 'next_etablissement_postal'})\n",
    "receptacles_df = receptacles_df.rename(columns={'ï»¿RECPTCL_FID': 'RECPTCL_FID', 'EVENT_TYPECD': 'EVENT_TYPE_CD', 'nextetablissement_postal': 'next_etablissement_postal'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df.shape, receptacles_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "* Columns' types adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df['date'] = pd.to_datetime(packages_df['date'])\n",
    "receptacles_df['date'] = pd.to_datetime(receptacles_df['date'])\n",
    "packages_df['RECPTCL_FID'] = packages_df['RECPTCL_FID'].str.strip()\n",
    "packages_df['MAILITM_FID'] = packages_df['MAILITM_FID'].str.strip()\n",
    "packages_df['etablissement_postal'] = packages_df['etablissement_postal'].str.strip()\n",
    "packages_df['next_etablissement_postal'] = packages_df['next_etablissement_postal'].str.strip()\n",
    "receptacles_df['etablissement_postal'] = receptacles_df['etablissement_postal'].str.strip()\n",
    "receptacles_df['next_etablissement_postal'] = receptacles_df['next_etablissement_postal'].str.strip()\n",
    "receptacles_df['RECPTCL_FID'] = receptacles_df['RECPTCL_FID'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    " Initial Observations\n",
    "- both datasets cover the period from 2023 to 2025\n",
    "- we have no target variable in either dataset\n",
    "- for packages dataset:\n",
    "    - 6 features in total with 5 categorical and 1 numerical\n",
    "    - MAILITM_FID is unique identifier for each package\n",
    "    - RECPTCL_FID is foreign key linking to receptacle dataset\n",
    "    - etablissement_postal and next_etablissement_postal have some null values\n",
    "- for receptacle dataset:\n",
    "    - 5 features in total with 4 categorical and 1 numerical\n",
    "    - RECPTCL_FID is unique identifier for each receptacle\n",
    "    - EVENT_TYPE_CD has some null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in packages_df.columns:\n",
    "    print(f'{column} has {packages_df[column].nunique()} unique values.')\n",
    "    print(f'{column} has {packages_df[column].isnull().sum()} null values.')\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in receptacles_df.columns:\n",
    "    print(f'{column} has {receptacles_df[column].nunique()} unique values.')\n",
    "    print(f'{column} has {receptacles_df[column].isnull().sum()} null values.')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "we notice the following:<br>\n",
    "- receptacle dataset has more unique values for RECPTCL_FID than packages dataset, indicating one-to-many relationship<br>\n",
    "- MAILITM_FID is unique in packages dataset.<br>\n",
    "- packages dataset have more unique date values than receptacle dataset.<br>\n",
    "- both datasets have null values in etablissement_postal and next_etablissement_postal columns. This requires processing later on<br>\n",
    "- packages dataset has more unique values in the next_etablissement_postal column compared to receptacle dataset but also more null values. **further investigation is needed to understand why**<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "for EVENT_TYPE_CD we notice different range of values for packages and receptacle datasets indicating different types of events.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### Visualizations\n",
    "for now we will visualize the distribution of EVENT_TYPE_CD in both datasets.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of EVENT_TYPE_CD in packages dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=packages_df,x='EVENT_TYPE_CD')\n",
    "plt.title('distribution of EVENT_TYPE_CD in packages dataset')\n",
    "plt.xlabel('EVENT_TYPE_CD')\n",
    "plt.ylabel('count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of EVENT_TYPE_CD in receptacle dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=receptacles_df,x='EVENT_TYPE_CD')\n",
    "plt.title('distribution of EVENT_TYPE_CD in Receptacle dataset')\n",
    "plt.xlabel('EVENT_TYPE_CD')\n",
    "plt.ylabel('count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "for etablissement_postal and next_etablissement_postal I will start with visualizing the receptacle dataset since the packages dataset has a lot of unique values<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of etablissement_postal in receptacle dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=receptacles_df,x='etablissement_postal')\n",
    "plt.title('distribution of etablissement_postal in Receptacle dataset')\n",
    "plt.xlabel('etablissement_postal')\n",
    "plt.ylabel('count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of next_etablissement_postal in receptacle dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=receptacles_df,x='next_etablissement_postal')\n",
    "plt.title('distribution of next_etablissement_postal in Receptacle dataset')\n",
    "plt.xlabel('next_etablissement_postal')\n",
    "plt.ylabel('count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.  Top 10 locations to keep the map small\n",
    "top_locs = receptacles_df['etablissement_postal'].value_counts().head(10).index\n",
    "top_next = receptacles_df['next_etablissement_postal'].value_counts().head(10).index\n",
    "\n",
    "# 2. include only these top locations\n",
    "filtered_df = receptacles_df[\n",
    "    (receptacles_df['etablissement_postal'].isin(top_locs)) &\n",
    "    (receptacles_df['next_etablissement_postal'].isin(top_next))\n",
    "]\n",
    "\n",
    "# 3. Create a Matrix (Cross-tabulation)\n",
    "matrix = pd.crosstab(filtered_df['etablissement_postal'], filtered_df['next_etablissement_postal'])\n",
    "\n",
    "# 4. Plot Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(matrix, cmap='Reds', annot=True, fmt='d', linewidths=.5)\n",
    "plt.title(\"Most Frequent Routes (Origin vs Destination) - Receptacle Dataset\")\n",
    "plt.xlabel(\"Next Destination\")\n",
    "plt.ylabel(\"Current Location\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "we notice that some etablissements have significantly higher traffic compared to others, indicating  major distribution centers.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "for etablissement_postal and next_etablissement_postal we will create a heatmap to visualize the flow between current location and next destination.<br>\n",
    "Count of parcels moving from A to B to see the density of connections between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.  Top 10 locations to keep the map small\n",
    "top_locs = packages_df['etablissement_postal'].value_counts().head(10).index\n",
    "top_next = packages_df['next_etablissement_postal'].value_counts().head(10).index\n",
    "\n",
    "# 2. include only these top locations\n",
    "filtered_df = packages_df[\n",
    "    (packages_df['etablissement_postal'].isin(top_locs)) &\n",
    "    (packages_df['next_etablissement_postal'].isin(top_next))\n",
    "]\n",
    "\n",
    "# 3. Create a Matrix (Cross-tabulation)\n",
    "matrix = pd.crosstab(filtered_df['etablissement_postal'], filtered_df['next_etablissement_postal'])\n",
    "\n",
    "# 4. Plot Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(matrix, cmap='Reds', annot=True, fmt='d', linewidths=.5)\n",
    "plt.title(\"Most Frequent Routes (Origin vs Destination) - Packages Dataset\")\n",
    "plt.xlabel(\"Next Destination\")\n",
    "plt.ylabel(\"Current Location\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count packages per location\n",
    "location_counts = packages_df['etablissement_postal'].value_counts().reset_index()\n",
    "location_counts.columns = ['Location', 'Volume']\n",
    "\n",
    "# keep only top 20 busiest centers\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Volume', y='Location', data=location_counts.head(20), palette='viridis')\n",
    "plt.title(\"Top 20 Busiest Postal Centers\")\n",
    "plt.xlabel(\"Number of Packages\")\n",
    "plt.ylabel(\"Center ID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "we notice the same pattern as before with some etablissements having significantly higher trafic compared to others.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Extract Time Features\n",
    "packages_df['hour'] = packages_df['date'].dt.hour\n",
    "packages_df['day_of_week'] = packages_df['date'].dt.day_name()\n",
    "\n",
    "# 2. Create a Pivot Table (Cross-tabulation)\n",
    "# Rows = Day, Cols = Hour, Values = Count of Scans\n",
    "heatmap_data = pd.crosstab(\n",
    "    packages_df['day_of_week'],\n",
    "    packages_df['hour']\n",
    ")\n",
    "\n",
    "days_order = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "heatmap_data = heatmap_data.reindex(days_order)\n",
    "\n",
    "# 3. Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(heatmap_data, cmap='YlOrRd', linewidths=.5, annot=False)\n",
    "plt.title(\"Package Scan Activity by Day and Hour\")\n",
    "plt.xlabel(\"Hour of Day (0-23)\")\n",
    "plt.ylabel(\"Day of Week\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "we notice that the busiest times for package scans are during weekdays, particularly from mid-morning to late afternoon.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of packages per receptacle\n",
    "packages_per_receptacle = packages_df.groupby('RECPTCL_FID')['MAILITM_FID'].nunique()\n",
    "packages_per_receptacle.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "- Drop the packages starting from 2020 and keep only the ones starting from 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in packages df Drop the values that are from 2020 and start only from 2023\n",
    "packages_df = packages_df[packages_df['date'].dt.year >= 2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df['etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "* `etablissement_postal` have 26772 null values (2.7% of the whole dataset)\n",
    "* As its null values are less than 5% of the dataset (2.7%), we drop these null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df = packages_df[~packages_df['etablissement_postal'].isna()]\n",
    "packages_df['etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "* We propose to consider the packages having null `next_etablissement_postal`\n",
    "as having issue during transfer, we'll try to validate that using\n",
    "`EVENT_TYPE_CD` also\n",
    "* Let's check if `EVENT_TYPE_CD` can indicate whether the `next_etablissement_postal` is null or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_unknown_next_etablissement = packages_df[packages_df['next_etablissement_postal'].isna()]\n",
    "# keep only top EVENT_TYPES_ID\n",
    "packages_unknown_next_etablissement = packages_unknown_next_etablissement['EVENT_TYPE_CD'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "packages_unknown_next_etablissement.head(10).plot(kind='bar')\n",
    "plt.xlabel('EVENT TYPE CD')\n",
    "plt.ylabel('Null Next Etablissement Postal')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "* `EVENT_TYPE_CD` doesn't actually indicate null values of `next_etablissement_postal`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "* the function `fill_NaN_next_etab` cell fills the `next_etablissement_postal` using the next `etablissement_postal` for the same package.\n",
    "* if the last route for a specific package is null, then it keeps it null because there's no next `etablissement_postal` for that package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_NaN_next_etab(df, id_col):\n",
    "    # 1. Ensure the dataframe is sorted (same as before)\n",
    "    df = df.sort_values([id_col, 'date'])\n",
    "\n",
    "    # 2. Look ahead to the next row's postal code and ID\n",
    "    shifted_postal = df['etablissement_postal'].shift(-1)\n",
    "    shifted_id = df[id_col].shift(-1)\n",
    "# 3. Identify the \"boundaries\" where the postal code changes within the same package\n",
    "# This marks the last row of a block with the value of the start of the next block\n",
    "    is_boundary = (df['etablissement_postal'] != shifted_postal) & \\\n",
    "              (df[id_col] == shifted_id)\n",
    "# 4. Use grouped backfill to broadcast those values to all preceding rows in the block\n",
    "# This replaces your 'blocks.map' logic with a single vectorized pass\n",
    "    fill_values = shifted_postal.where(is_boundary).groupby(df[id_col]).bfill()\n",
    "\n",
    "# 5. Fill only the NaNs in the existing column to match your original logic\n",
    "    df['next_etablissement_postal'] = df['next_etablissement_postal'].fillna(fill_values)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to fill NaN values in next_etablissement_postal\n",
    "packages_df = fill_NaN_next_etab(packages_df, 'MAILITM_FID')\n",
    "# Check remaining NaNs\n",
    "packages_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "* Like this, we've handled a good part of null values and inconsitencies for `packages` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "* **We'll be doing the same steps for `receptacle` dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df['etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "* Dropping rows having null `etablissement_postal`, as they're just 0.1% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df = receptacles_df[~receptacles_df['etablissement_postal'].isna()]\n",
    "receptacles_df['etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "* apply the function that fills null values of `next_etablissement_postal` using `etablissement_postal` to `receptacles_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "receptacles_df = fill_NaN_next_etab(receptacles_df, 'RECPTCL_FID')\n",
    "# Check remaining NaNs\n",
    "receptacles_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "* Null values are mostly gone, but there are still some illogical packages' and receptacles' routes between `etablissements`\n",
    "* We'll treat these logical routes now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each package (group of rows), check whether there's any illogical route\n",
    "# between 'etablissement_postal' and 'next_etablissement_postal'\n",
    "def isPackageIllogical(group):\n",
    "    return (\n",
    "        group['next_etablissement_postal']\n",
    "        .iloc[:-1]\n",
    "        .ne(group['etablissement_postal'].shift(-1).iloc[:-1])\n",
    "        .any()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "illogical_packages = packages_df.groupby('MAILITM_FID').apply(isPackageIllogical)\n",
    "illogical_packages.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "103376 / packages_df['MAILITM_FID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "* 103376 Packages have illogical routes (98%) of all packages, so it's impossible to drop them, but instead, we plan to ignore the `MAILITM_FID` and `RECPTCL_FID` in the training and testing sets that will come next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each receptacle (group of rows), check whether there's any illogical route\n",
    "# between 'etablissement_postal' and 'next_etablissement_postal'\n",
    "def isReceptacleIllogical(group):\n",
    "    return (\n",
    "        group['next_etablissement_postal']\n",
    "        .iloc[:-1]\n",
    "        .ne(group['etablissement_postal'].shift(-1).iloc[:-1])\n",
    "        .any()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "illogical_receptacles = receptacles_df.groupby('RECPTCL_FID').apply(isReceptacleIllogical)\n",
    "illogical_receptacles.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "205519 / receptacles_df['RECPTCL_FID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "205519 receptacles have illogical routes (95%) of all receptacles, so it's also impossible to drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep copies for backup (en cas ou)\n",
    "packages_df_copy = packages_df.copy()\n",
    "receptacles_df_copy = receptacles_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "# Feature Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "### Check RECPTCL_FID and MAILITM_FID having same length formats\n",
    "if yes then we can split them into meaningfull parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "same=1\n",
    "print(\"\\n=== RECPTCL_FID  ===\")\n",
    "print(f\"testing if the lengths of RECPTCL_FID values are all the same:\")\n",
    "for val in packages_df['RECPTCL_FID'].values:\n",
    "    if len(str(val)) != 29 :\n",
    "        print(f\"  {val} (length: {len(str(val))})\")\n",
    "        same=0\n",
    "        break\n",
    "if same==1:\n",
    "    print('all same length' )\n",
    "\n",
    "print(\"\\n=== MAILITM_FID  ===\")\n",
    "print(f\"testing if the lengths of MAILITM_FID values are all the same:\")\n",
    "for val in packages_df['MAILITM_FID'].values:\n",
    "    if len(str(val)) != 13 :\n",
    "        print(f\"  {val} (length: {len(str(val))})\")\n",
    "        same=0\n",
    "        break\n",
    "if same==1:\n",
    "    print ('all same length' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "### RECPTCL_FID Analysis\n",
    "- **Format:** 29-character string (e.g., `USORDADZALGDAUN30050001900005`)\n",
    "- **Data Quality:** No null values (1,000,000) | 215,867 unique values in receptacle dataset and 45306 unique values in packages dataset\n",
    "- **Extractable Features:**\n",
    "  - Origin Country (2 chars): US, FR, AE, etc.\n",
    "  - Destination Country (2 chars): DZ, AI, AA, etc.\n",
    "\n",
    "### MAILITM_FID Analysis\n",
    "- **Format:** 13-character string according to the S10-12 patern (e.g., `CA000132868US`, `CA000340856PK`)\n",
    "- **Data Quality:** No null values (1,000,000 packages)\n",
    "- **Extractable Features:**\n",
    "  - Service Indicator (2 chars): CA, etc.\n",
    "  - Serial Number (8 chars): 00013286, 00034085, etc.\n",
    "  - Check Digit (1 char): 8, 6, etc.\n",
    "  - Country Code (3 chars, right-stripped): US, PK.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "## Definition of the parser funtions\n",
    "These functions are responsible for spliting the IDs into parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_recptcl_fid(id_str):\n",
    "    origin_country = id_str[0:2]\n",
    "    destination_country = id_str[6:8]\n",
    "    return origin_country, destination_country\n",
    "\n",
    "def parse_mailitm_fid(id_str):\n",
    "    service_indicator = id_str[0:2]\n",
    "    serial_number = id_str[2:11]\n",
    "    country_code = id_str[11:14].strip()\n",
    "    return service_indicator, serial_number, country_code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "### Apply parsing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_receptacles_df = receptacles_df.copy()\n",
    "# parsed_receptacles_df[['origin_country', 'destination_country']] = parsed_receptacles_df['RECPTCL_FID'].apply(lambda x: pd.Series(parse_recptcl_fid(x)))\n",
    "\n",
    "# Faster method:\n",
    "parsed_data = list(receptacles_df['RECPTCL_FID'].apply(parse_recptcl_fid))\n",
    "\n",
    "# Assign to new columns by creating a temporary DataFrame\n",
    "parsed_receptacles_df = receptacles_df.copy()\n",
    "parsed_receptacles_df[['origin_country', 'destination_country']] = pd.DataFrame(\n",
    "    parsed_data, index=receptacles_df.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_packages_df = packages_df.copy()\n",
    "# parsed_packages_df[['service_indicator', 'serial_number', 'country_code']] = parsed_packages_df['MAILITM_FID'].apply(lambda x: pd.Series(parse_mailitm_fid(x)))\n",
    "# parsed_packages_df[['origin_country','destination_country']] = parsed_packages_df['RECPTCL_FID'].apply(lambda x: pd.Series(parse_recptcl_fid(x)))\n",
    "\n",
    "# Faster method:\n",
    "parsed_packages_df = packages_df.copy()\n",
    "\n",
    "# 1. Optimize MAILITM_FID parsing\n",
    "mailitm_data = list(parsed_packages_df['MAILITM_FID'].apply(parse_mailitm_fid))\n",
    "parsed_packages_df[['service_indicator', 'serial_number', 'country_code']] = pd.DataFrame(\n",
    "    mailitm_data, index=parsed_packages_df.index\n",
    ")\n",
    "\n",
    "# 2. Optimize RECPTCL_FID parsing\n",
    "recptcl_data = list(parsed_packages_df['RECPTCL_FID'].apply(parse_recptcl_fid))\n",
    "parsed_packages_df[['origin_country', 'destination_country']] = pd.DataFrame(\n",
    "    recptcl_data, index=parsed_packages_df.index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "### show samples of new parsed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=== packages_df sample with new parsed columns ===\")\n",
    "parsed_packages_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== receptacles_df sample with new parsed columns ===\")\n",
    "parsed_receptacles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "# Analysis of the extrcted features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "## 1. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n--- Unique Value Counts for parsed_packages_df ---\")\n",
    "print(\"\\nFor receptacle FID parsing:\")\n",
    "print(f\"Unique origin_country values: {parsed_packages_df['origin_country'].nunique()}\")\n",
    "print(f\"Unique destination_country values: {parsed_packages_df['destination_country'].nunique()}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"\\nFor mail item FID parsing:\")\n",
    "print(f\"Unique service_indicator values: {parsed_packages_df['service_indicator'].nunique()}\")\n",
    "print(f\"Unique country_code values: {parsed_packages_df['country_code'].nunique()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "## 2. parsed_receptacles_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Unique Value Counts for parsed_receptacles_df ---\")\n",
    "print(f\"Unique origin_country values: {parsed_receptacles_df['origin_country'].nunique()}\")\n",
    "print(f\"Unique destination_country values: {parsed_receptacles_df['destination_country'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "## List values of the new columns obtained from receptacle FID parsing for both parsed dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "### 1. for parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#listing the values \n",
    "print(\"\\n--- Values of origin_country ---\")\n",
    "print(parsed_packages_df['origin_country'].unique())\n",
    "\n",
    "print(\"\\n--- Values of destination_country ---\")\n",
    "print(parsed_packages_df['destination_country'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "### 2. for parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#listing the values \n",
    "print(\"\\n--- Values of origin_country ---\")\n",
    "print(parsed_receptacles_df['origin_country'].unique())\n",
    "\n",
    "print(\"\\n--- Values of destination_country ---\")\n",
    "print(parsed_receptacles_df['destination_country'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "### Do the intersection of origin_country of both dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the intersection of origin_country values in both parsed datasets\n",
    "packages_origin_countries = set(parsed_packages_df['origin_country'].unique())\n",
    "receptacle_origin_countries = set(parsed_receptacles_df['origin_country'].unique())\n",
    "common_origin_countries = packages_origin_countries.intersection(receptacle_origin_countries)\n",
    "print(\"number of common origin_country values in both parsed datasets:\", len(common_origin_countries))\n",
    "print(f\"\\nCommon origin_country values in both paesed datasets: \")\n",
    "print(common_origin_countries)\n",
    "# remaining ones \n",
    "remaining_in_packages = packages_origin_countries - common_origin_countries\n",
    "remaining_in_receptacle = receptacle_origin_countries - common_origin_countries\n",
    "print(f\"Remaining origin_country values only in parsed_packages_df:\")\n",
    "print(remaining_in_packages)\n",
    "print(f\"Remaining origin_country values only in parsed_receptacles_df:\")\n",
    "print(remaining_in_receptacle )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "### list the values of both service indicators and country code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "### 1. service indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Values of service_indicator ---\")\n",
    "print(parsed_packages_df['service_indicator'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    " we can see that there are values that don't follow the standards in the S10-12 format so we need to handle that correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform country_code to uppercase for consistency\n",
    "parsed_packages_df['service_indicator'] = parsed_packages_df['service_indicator'].str.upper()\n",
    "print(\"values of service_indicator after transformation to uppercase:\")\n",
    "print(parsed_packages_df['service_indicator'].unique())\n",
    "print(\"number of unique service indicators after transformation:\", parsed_packages_df['service_indicator'].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "### 2. country code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Values of country codes ---\")\n",
    "\n",
    "print(parsed_packages_df['country_code'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "we can see that many values for the country codes are numbers instead of ISO 3166-1 format these values should be replaced by the values of origin country gotten from the receptacle when doing the preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "### replace them with the correct origin country code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Vectorized string capitalization\n",
    "parsed_packages_df['country_code'] = parsed_packages_df['country_code'].str.upper()\n",
    "\n",
    "# 2. Vectorized comparison to find mismatches\n",
    "mismatch_mask = parsed_packages_df['origin_country'] != parsed_packages_df['country_code']\n",
    "\n",
    "# 3. Count the Trues\n",
    "count = mismatch_mask.sum()\n",
    "\n",
    "print(f\"Number of rows where origin_country does not match country_code: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "### replace them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .loc to find rows where they don't match, and update only the 'country_code' column\n",
    "parsed_packages_df.loc[parsed_packages_df['origin_country'] != parsed_packages_df['country_code'], 'country_code'] = parsed_packages_df['origin_country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the unique values again\n",
    "print(\"\\n--- Values of country codes after correction ---\")\n",
    "print(parsed_packages_df['country_code'].unique())\n",
    "print(\"number of unique country codes after correction:\", parsed_packages_df['country_code'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "## visualization of Origin Country distribution according to number of packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104",
   "metadata": {},
   "source": [
    "### 1. for the parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_country_counts = parsed_packages_df['origin_country'].value_counts().head(20)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.barh(origin_country_counts.index, origin_country_counts.values, color='steelblue')\n",
    "plt.xlabel('Count', fontsize=11)\n",
    "plt.ylabel('Origin Country', fontsize=11)\n",
    "plt.title('Top 20 Origin Countries by packages count', fontsize=12, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "### 2. for the parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_country_counts = parsed_receptacles_df['origin_country'].value_counts().head(20)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.barh(origin_country_counts.index, origin_country_counts.values, color='steelblue')\n",
    "plt.xlabel('Count', fontsize=11)\n",
    "plt.ylabel('Origin Country', fontsize=11)\n",
    "plt.title('Top 20 Origin Countries by receptacle count', fontsize=12, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {},
   "source": [
    "## Visualiation of the service indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_indicator_count = parsed_packages_df['service_indicator'].value_counts().head(20)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.barh(service_indicator_count.index, service_indicator_count.values, color='mediumseagreen')\n",
    "plt.xlabel('Count', fontsize=11)\n",
    "plt.ylabel('Service Indicator', fontsize=11)\n",
    "plt.title('Top 20 Service Indicators by packages count', fontsize=12, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110",
   "metadata": {},
   "source": [
    "# Origin–Destination Flow Analysis\n",
    "\n",
    "This section investigates the flow of receptacles and packages from origin countries to destination. We examine:\n",
    "- packages count by origin country\n",
    "- Top origin countries delivering to each destination\n",
    "- Visual representation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {},
   "source": [
    "### 1. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages count by origin country\n",
    "origin_country_volume = parsed_packages_df['origin_country'].value_counts()\n",
    "print(\"\\n--- Packages count by Origin Country ---\")\n",
    "print(origin_country_volume.head(15))\n",
    "\n",
    "# Create origin_country × destination matrix\n",
    "flow_matrix = pd.crosstab(parsed_packages_df['origin_country'], \n",
    "                           parsed_packages_df['destination_country'])\n",
    "\n",
    "# Keep only top 10 origin countries and top 10 destination countries for readability\n",
    "top_origins = parsed_packages_df['origin_country'].value_counts().head(10).index\n",
    "top_arrivals = parsed_packages_df['destination_country'].value_counts().head(10).index\n",
    "\n",
    "flow_matrix_top = flow_matrix.loc[top_origins, top_arrivals]\n",
    "\n",
    "print(\"\\n--- Origin Country × destination Country (Top 10 × Top 10) ---\")\n",
    "print(flow_matrix_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize origin × destination country flow\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(flow_matrix_top, cmap='YlOrRd', annot=True, fmt='d', \n",
    "            cbar_kws={'label': 'packages Count'}, linewidths=0.5)\n",
    "plt.title('packages Flow: Origin Country × destination country (Top 10 × Top 10)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Arrival Hub', fontsize=12)\n",
    "plt.ylabel('Origin Country', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114",
   "metadata": {},
   "source": [
    "### 2. parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# receptacle count by origin country\n",
    "origin_country_receptacle_volume = parsed_receptacles_df['origin_country'].value_counts()\n",
    "print(\"\\n--- Receptacle count by Origin Country ---\")\n",
    "print(origin_country_receptacle_volume.head(15))\n",
    "\n",
    "destination__receptacle_volume=parsed_receptacles_df['destination_country'].value_counts()\n",
    "print(\"\\n--- Receptacle count by destination ---\")\n",
    "print(destination__receptacle_volume.head(15))\n",
    "\n",
    "# Create origin_country × destination matrix\n",
    "receptacle_flow_matrix = pd.crosstab(parsed_receptacles_df['origin_country'], \n",
    "                           parsed_receptacles_df['destination_country'])\n",
    "\n",
    "# Keep only top 10 origin countries and top 10 destination countries for readability\n",
    "receptacle_top_origins = parsed_receptacles_df['origin_country'].value_counts().head(10).index\n",
    "receptacle_top_arrivals = parsed_receptacles_df['destination_country'].value_counts().head(10).index\n",
    "\n",
    "receptacle_flow_matrix_top = receptacle_flow_matrix.loc[receptacle_top_origins, receptacle_top_arrivals]\n",
    "\n",
    "print(\"\\n--- Origin Country × destination Country (Top 10 × Top 10) ---\")\n",
    "print(receptacle_flow_matrix_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize origin × destination country flow\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(receptacle_flow_matrix_top, cmap='YlOrRd', annot=True, fmt='d', \n",
    "            cbar_kws={'label': 'receptacles Count'}, linewidths=0.5)\n",
    "plt.title('receptacles Flow: Origin Country × destination country (Top 10 × Top 10)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Arrival Hub', fontsize=12)\n",
    "plt.ylabel('Origin Country', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117",
   "metadata": {},
   "source": [
    "### create pairs (origin, destination) for more detailed analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_packages_df['origin_destination'] = parsed_packages_df['origin_country'] + '_' + parsed_packages_df['destination_country']\n",
    "parsed_receptacles_df['origin_destination'] = parsed_receptacles_df['origin_country'] + '_' + parsed_receptacles_df['destination_country']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {},
   "source": [
    "listing the obtained values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('(origin_destination) pairs obtained for ')\n",
    "print(\"\\nfor parsed_packages_df :\")\n",
    "print(\"\\nNumber of unique values\", parsed_packages_df['origin_destination'].nunique())\n",
    "print(\"\\nValues : \")\n",
    "print(parsed_packages_df['origin_destination'].unique())\n",
    "print(\"\\nfor parsed_receptacles_df :\")\n",
    "print(\"\\nNumber of unique values\", parsed_receptacles_df['origin_destination'].nunique())\n",
    "print(\"\\nValues : \")\n",
    "print(parsed_receptacles_df['origin_destination'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121",
   "metadata": {},
   "source": [
    "### visualization of obtained results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122",
   "metadata": {},
   "source": [
    "### 1. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_dest_counts = parsed_packages_df['origin_destination'].value_counts()\n",
    "top_origin_dest_counts= origin_dest_counts.head(15)\n",
    "plt.figure(figsize=(14, 7))\n",
    "top_origin_dest_counts.plot(kind='bar')\n",
    "plt.title('Histogram of (origin, destination) Pairs Based on Package Counts')\n",
    "plt.xlabel('Origin_Destination')\n",
    "plt.ylabel('Number of Packages')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124",
   "metadata": {},
   "source": [
    "\n",
    "### 2. parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_dest_counts = parsed_receptacles_df['origin_destination'].value_counts()\n",
    "top_origin_dest_counts = origin_dest_counts.head(15)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "top_origin_dest_counts.plot(kind='bar')\n",
    "plt.title('Histogram of (origin, destination) Pairs Based on receptacle Counts')\n",
    "plt.xlabel('Origin_Destination')\n",
    "plt.ylabel('Number of receptacles')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126",
   "metadata": {},
   "source": [
    "## origin_destination X etablissments analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {},
   "source": [
    "## 1. current etablissment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128",
   "metadata": {},
   "source": [
    "### a. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_packages_df: visualize the histogram of counts by (origin_destination, etablissement_postal) pairs\n",
    "\n",
    "# First, select top N most common (origin_destination, etablissement_postal) pairs for clarity\n",
    "pair_counts_current = parsed_packages_df.groupby(['origin_destination', 'etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_current = pair_counts_current.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_current,\n",
    "    x='count',\n",
    "    y=top_pairs_current.apply(lambda x: f\"{x['origin_destination']} | {x['etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin-Destination, Current Etablissement) Pairs by Package Counts')\n",
    "plt.xlabel('Number of Packages')\n",
    "plt.ylabel('(Origin_Destination | Current Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130",
   "metadata": {},
   "source": [
    "### b. parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_receptacles_dffffff: visualize the histogram of counts by (origin_destination, etablissement_postal) pairs\n",
    "\n",
    "# First, select top N most common (origin_destination, etablissement_postal) pairs for clarity\n",
    "pair_counts_current_receptacle = parsed_receptacles_df.groupby(['origin_destination', 'etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_current_receptacle = pair_counts_current_receptacle.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_current_receptacle,\n",
    "    x='count',\n",
    "    y=top_pairs_current_receptacle.apply(lambda x: f\"{x['origin_destination']} | {x['etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin-Destination, Current Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Origin_Destination | Current Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132",
   "metadata": {},
   "source": [
    "we can see that the ETAB0002 is dominating and we remark that when the destination is DZ\n",
    "we'll try to confirm that by taking into consideration the destination only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_receptacles_dffff: visualize the histogram of counts by (destination_country, etablissement_postal) pairs\n",
    "\n",
    "# First, select top N most common (destination_country, etablissement_postal) pairs for clarity\n",
    "pair_counts_dest_receptacle = parsed_receptacles_df.groupby(['destination_country', 'etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_dest_receptacle = pair_counts_dest_receptacle.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_dest_receptacle,\n",
    "    x='count',\n",
    "    y=top_pairs_dest_receptacle.apply(lambda x: f\"{x['destination_country']} | {x['etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Destination Country, Current Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Destination Country | Current Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134",
   "metadata": {},
   "source": [
    "### This is to test the origin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_receptacles_dffffff: visualize the histogram of counts by (origin_country, etablissement_postal) pairs\n",
    "\n",
    "# First, select top N most common (origin_country, etablissement_postal) pairs for clarity\n",
    "pair_counts_origin_receptacle = parsed_receptacles_df.groupby(['origin_country', 'etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_origin_receptacle = pair_counts_origin_receptacle.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_origin_receptacle,\n",
    "    x='count',\n",
    "    y=top_pairs_origin_receptacle.apply(lambda x: f\"{x['origin_country']} | {x['etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin Country, Current Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Origin Country | Current Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136",
   "metadata": {},
   "source": [
    "most of the values with ETAB0002 values are european countries in addition to AE and China(CN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137",
   "metadata": {},
   "source": [
    "## 2. Next etablissement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138",
   "metadata": {},
   "source": [
    "### a. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_packages_df: visualize the histogram of counts by (origin_destination, next_etablissement_postal)\n",
    "\n",
    "# First, select top N most common (origin_destination, next_etablissement_postal) pairs for clarity\n",
    "pair_counts = parsed_packages_df.groupby(['origin_destination', 'next_etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs = pair_counts.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs,\n",
    "    x='count',\n",
    "    y=top_pairs.apply(lambda x: f\"{x['origin_destination']} | {x['next_etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin-Destination, Next Etablissement) Pairs by Package Counts')\n",
    "plt.xlabel('Number of Packages')\n",
    "plt.ylabel('(Origin_Destination | Next Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140",
   "metadata": {},
   "source": [
    "### b. parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_receptacles_dffff: visualize the histogram of counts by (origin_destination, next_etablissement_postal)\n",
    "\n",
    "# First, select top N most common (origin_destination, next_etablissement_postal) pairs for clarity\n",
    "pair_counts_recept = parsed_receptacles_df.groupby(['origin_destination', 'next_etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_recept = pair_counts_recept.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_recept,\n",
    "    x='count',\n",
    "    y=top_pairs_recept.apply(lambda x: f\"{x['origin_destination']} | {x['next_etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin-Destination, Next Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Origin_Destination | Next Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142",
   "metadata": {},
   "source": [
    "Do for origin and for destination separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For parsed_receptacles_df: visualize the histogram of counts by destination_country and next_etablissement_postal\n",
    "\n",
    "# First, select top N most common (destination_country, next_etablissement_postal) pairs for clarity\n",
    "pair_counts_dest = parsed_receptacles_df.groupby(['destination_country', 'next_etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_dest = pair_counts_dest.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_dest,\n",
    "    x='count',\n",
    "    y=top_pairs_dest.apply(lambda x: f\"{x['destination_country']} | {x['next_etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Destination Country, Next Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Destination_Country | Next Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parsed_receptacles_df: visualize the histogram of counts by origin_country and next_etablissement_postal\n",
    "\n",
    "# First, select top N most common (origin_country, next_etablissement_postal) pairs for clarity\n",
    "pair_counts_origin = parsed_receptacles_df.groupby(['origin_country', 'next_etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_origin = pair_counts_origin.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_origin,\n",
    "    x='count',\n",
    "    y=top_pairs_origin.apply(lambda x: f\"{x['origin_country']} | {x['next_etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin Country, Next Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Origin_Country | Next Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145",
   "metadata": {},
   "source": [
    "from the analysis we can see that there are some etablissments that get congested forming a sort of loop (ETAB0030, ETAB0002, ETAB0006)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146",
   "metadata": {},
   "source": [
    "### Time analysis regarding origin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147",
   "metadata": {},
   "source": [
    "### 1. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the parsed_packages_df by date and origin_country to count packages per month per origin country\n",
    "parsed_packages_df['date'] = pd.to_datetime(parsed_packages_df['date'])\n",
    "\n",
    "# Group by origin_country and resample by month\n",
    "ts_by_origin = parsed_packages_df.groupby(['origin_country', pd.Grouper(key='date', freq='ME')])['MAILITM_FID'].count().reset_index()\n",
    "\n",
    "# Choose top 5 origin countries by total count for plotting\n",
    "top_origin = parsed_packages_df['origin_country'].value_counts().head(5).index\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "for origin in top_origin:\n",
    "    ts = ts_by_origin[ts_by_origin['origin_country'] == origin]\n",
    "    plt.plot(ts['date'], ts['MAILITM_FID'], marker='o', label=origin)\n",
    "\n",
    "plt.title('Monthly Package count by Origin Country')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Packages')\n",
    "plt.legend(title='Origin Country')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149",
   "metadata": {},
   "source": [
    "### 2. parsed_receptacles_dfffff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the parsed_receptacles_df by date and origin_country to count receptacles per month per origin country\n",
    "parsed_receptacles_df['date'] = pd.to_datetime(parsed_receptacles_df['date'])\n",
    "\n",
    "# Group by origin_country and resample by month\n",
    "ts_by_origin_recept = parsed_receptacles_df.groupby(['origin_country', pd.Grouper(key='date', freq='ME')])['RECPTCL_FID'].count().reset_index()\n",
    "\n",
    "# Choose top 5 origin countries by total count for plotting\n",
    "top_origin_recept = parsed_receptacles_df['origin_country'].value_counts().head(5).index\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "for origin in top_origin_recept:\n",
    "    ts = ts_by_origin_recept[ts_by_origin_recept['origin_country'] == origin]\n",
    "    plt.plot(ts['date'], ts['RECPTCL_FID'], marker='o', label=origin)\n",
    "\n",
    "plt.title('Monthly Receptacle count by Origin Country')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Receptacles')\n",
    "plt.legend(title='Origin Country')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151",
   "metadata": {},
   "source": [
    "### Time analysis by destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the parsed_packages_df by date and destination_country to count packages per month per destination country\n",
    "parsed_packages_df['date'] = pd.to_datetime(parsed_packages_df['date'])\n",
    "\n",
    "# Group by destination_country and resample by month\n",
    "ts_by_dest = parsed_packages_df.groupby(['destination_country', pd.Grouper(key='date', freq='ME')])['MAILITM_FID'].count().reset_index()\n",
    "\n",
    "# Choose top 5 destination countries by total count for plotting\n",
    "top_dest = parsed_packages_df['destination_country'].value_counts().head(5).index\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "for dest in top_dest:\n",
    "    ts = ts_by_dest[ts_by_dest['destination_country'] == dest]\n",
    "    plt.plot(ts['date'], ts['MAILITM_FID'], marker='o', label=dest)\n",
    "\n",
    "plt.title('Monthly Package count by Destination Country')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Packages')\n",
    "plt.legend(title='Destination Country')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153",
   "metadata": {},
   "source": [
    "### 2. parsed_receptacles_dffff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the parsed_receptacles_df by date and destination_country to count receptacles per month per destination country\n",
    "parsed_receptacles_df['date'] = pd.to_datetime(parsed_receptacles_df['date'])\n",
    "\n",
    "# Group by destination_country and resample by month \n",
    "ts_by_dest_recept = parsed_receptacles_df.groupby(['destination_country', pd.Grouper(key='date', freq='ME')])['RECPTCL_FID'].count().reset_index()\n",
    "\n",
    "# Choose top 5 destination countries by total count for plotting\n",
    "top_dest_recept = parsed_receptacles_df['destination_country'].value_counts().head(5).index\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "for dest in top_dest_recept:\n",
    "    ts = ts_by_dest_recept[ts_by_dest_recept['destination_country'] == dest]\n",
    "    plt.plot(ts['date'], ts['RECPTCL_FID'], marker='o', label=dest)\n",
    "\n",
    "plt.title('Monthly Receptacle count by Destination Country')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Receptacles')\n",
    "plt.legend(title='Destination Country')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155",
   "metadata": {},
   "source": [
    "## Creating additional features \n",
    " 1. 'flow_type' column with values: 'inbound' (to DZ), 'outbound' (from DZ), 'local' (DZ to DZ), otherwise 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_flow_type(df):\n",
    "    # Define the conditions\n",
    "    conditions = [\n",
    "        (df['destination_country'] == 'DZ') & (df['origin_country'] == 'DZ'), # local\n",
    "        (df['destination_country'] == 'DZ'),                                # inbound\n",
    "        (df['origin_country'] == 'DZ')                                     # outbound\n",
    "    ]\n",
    "    \n",
    "    # Define the results for each condition\n",
    "    choices = ['local', 'inbound', 'outbound']\n",
    "    \n",
    "    # Apply logic with 'other' as the default\n",
    "    return np.select(conditions, choices, default='other')\n",
    "\n",
    "# Apply to both DataFrames instantly\n",
    "parsed_packages_df['flow_type'] = get_flow_type(parsed_packages_df)\n",
    "parsed_receptacles_df['flow_type'] = get_flow_type(parsed_receptacles_df)\n",
    "\n",
    "# Print counts\n",
    "print(\"Flow type counts in parsed_packages_df:\\n\", parsed_packages_df['flow_type'].value_counts())\n",
    "print(\"\\nFlow type counts in parsed_receptacles_df:\\n\", parsed_receptacles_df['flow_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157",
   "metadata": {},
   "source": [
    "we can see that there are some values of flow type with the type \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 rows of origin_country and destination_country for flow_type 'other' in parsed_receptacles_df\n",
    "print(parsed_receptacles_df.loc[parsed_receptacles_df['flow_type'] == 'other', ['origin_country', 'destination_country']].head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159",
   "metadata": {},
   "source": [
    "## Analysis of the relation between the flow_type and the event_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160",
   "metadata": {},
   "source": [
    "## 1. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Analyse relation between flow_type and EVENT_TYPE_CD in parsed_packages_df\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "ax = sns.countplot(data=parsed_packages_df, x=\"EVENT_TYPE_CD\", hue=\"flow_type\", palette='Set2')\n",
    "plt.title(\"Distribution of EVENT_TYPE_CD by flow_type in parsed_packages_df\")\n",
    "plt.xlabel(\"EVENT_TYPE_CD\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"Flow Type\")\n",
    "plt.tight_layout()\n",
    "for c in ax.containers:\n",
    "    ax.bar_label(c, label_type='edge', fontsize=8, padding=2)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162",
   "metadata": {},
   "source": [
    "## 2. parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each flow_type, list the unique EVENT_TYPE_CD values and the most frequent EVENT_TYPE_CD value\n",
    "for flow in parsed_receptacles_df['flow_type'].unique():\n",
    "    event_types = parsed_receptacles_df.loc[parsed_receptacles_df['flow_type'] == flow, 'EVENT_TYPE_CD'].unique()\n",
    "    most_common_event_type = parsed_receptacles_df.loc[parsed_receptacles_df['flow_type'] == flow, 'EVENT_TYPE_CD'].mode()\n",
    "    print(f\"Flow type: {flow}\")\n",
    "    print(f\"EVENT_TYPE_CD values: {sorted(event_types)}\")\n",
    "    if not most_common_event_type.empty:\n",
    "        print(f\"Most frequent EVENT_TYPE_CD: {most_common_event_type.iloc[0]}\")\n",
    "    else:\n",
    "        print(\"No EVENT_TYPE_CD available\")\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "ax = sns.countplot(data=parsed_receptacles_df, x=\"EVENT_TYPE_CD\", hue=\"flow_type\", palette='Set1')\n",
    "plt.title(\"Distribution of EVENT_TYPE_CD by flow_type in parsed_receptacles_df\")\n",
    "plt.xlabel(\"EVENT_TYPE_CD\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"Flow Type\")\n",
    "plt.tight_layout()\n",
    "\n",
    "for c in ax.containers:\n",
    "    ax.bar_label(c, label_type='edge', fontsize=8, padding=2)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165",
   "metadata": {},
   "source": [
    "we can see that there are major event types related to the inbound flow type( coming to DZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166",
   "metadata": {},
   "source": [
    "# Track multiple receptacles just to see the flow of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track multiple receptacles: visualize all events for 5 different RECPTCL_FID in parsed_receptacles_df\n",
    "# Pick 5 unique RECPTCL_FID values to demonstrate\n",
    "num_examples = 5\n",
    "example_receptacle_ids = parsed_receptacles_df['RECPTCL_FID'].drop_duplicates().iloc[:num_examples]\n",
    "for rid in example_receptacle_ids:\n",
    "    print(\"\\n--- Events for RECPTCL_FID:\", rid, \"---\")\n",
    "    display(parsed_receptacles_df[parsed_receptacles_df['RECPTCL_FID'] == rid][['RECPTCL_FID', 'date', 'EVENT_TYPE_CD', 'etablissement_postal', 'next_etablissement_postal']].sort_values('date').reset_index(drop=True))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "for i, receptacle_id in enumerate(example_receptacle_ids):\n",
    "    ex_df = parsed_receptacles_df[parsed_receptacles_df['RECPTCL_FID'] == receptacle_id].sort_values('date')\n",
    "    plt.plot(\n",
    "        ex_df['date'],\n",
    "        ex_df['EVENT_TYPE_CD'],\n",
    "        marker='o',\n",
    "        label=f\"RECPTCL_FID: {receptacle_id}\"\n",
    "    )\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(f\"Event Timeline (EVENT_TYPE_CD) for {num_examples} Receptacles\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"EVENT_TYPE_CD\")\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168",
   "metadata": {},
   "source": [
    "# **ATTENTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169",
   "metadata": {},
   "source": [
    "* ... **Add feature engineering starting from here, as the next part is the splitting part, and it needs to be done to the whole datasets** ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE\n",
    "# ... Number of `etablissements` will be added for both datasets ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e521b4e0",
   "metadata": {},
   "source": [
    "### Adding the target `delay`\n",
    "* In the following cell, we're adding the target `delay` for each row.\n",
    "* for rows (of the same package/receptacle) having `next_etablissement_postal` different than `etablissement_postal` of their next row, we keep the value of `delay` NaN.\n",
    "* otherwise, we get the difference of `date` of the row and its next (of the same package/receptacle) in `hours` and store it in `delay`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_target(df):\n",
    "\n",
    "    # 1. Sort the entire dataset chronologically\n",
    "    df = df.sort_values(by=['MAILITM_FID', 'date'], ascending=True)\n",
    "\n",
    "    # ... ADD THE DELAY TARGET ...\n",
    "\n",
    "    df['next_event_date'] = df.groupby('MAILITM_FID')['date'].shift(-1)\n",
    "    df['delay'] = (df['next_event_date'] - df['date']).dt.total_seconds() / 3600\n",
    "    # Create logical consistency mask\n",
    "    df['next_row_etab'] = df.groupby('MAILITM_FID')['etablissement_postal'].shift(-1)\n",
    "    valid_delay_mask = (df['next_etablissement_postal'] == df['next_row_etab'])\n",
    "\n",
    "    df.loc[~valid_delay_mask, 'delay'] = np.nan\n",
    "    df = df.drop(columns=['next_event_date', 'next_row_etab']) # to keep only to the original features.\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7896748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_df = add_target(packages_df)\n",
    "receptacles_df = add_target(receptacles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea5aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_delay_pkg = packages_df[packages_df['delay'] > 1000]\n",
    "large_delay_pkg.sort_values('delay', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae40fc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_delay_rcp = receptacles_df[receptacles_df['delay'] > 1000]\n",
    "large_delay_rcp.sort_values('delay', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172",
   "metadata": {},
   "source": [
    "# Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173",
   "metadata": {},
   "source": [
    "* Splitting `packages` and dataset into training and testing sets using `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84f5ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#sort the values by time taking into consideration the id\n",
    "#packages_df= packages_df.sort_values(by=['MAILITM_FID', 'date'])\n",
    "# Step 1: Train/Test split (80/20)\n",
    "pkg_X_train, pkg_X_test, = train_test_split(\n",
    "    packages_df,\n",
    "    test_size=0.2, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print (f\"Training set size: {pkg_X_train.shape[0]} rows\")\n",
    "print (f\"Testing set size: {pkg_X_test.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7bfcf6",
   "metadata": {},
   "source": [
    "### Filling NaN values of `delay`\n",
    "- We plan to use the average of the previous calculated delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a4caed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with the global mean\n",
    "global_mean_hrs = pkg_X_train[~pkg_X_train['delay'].isna()]['delay'].mean()\n",
    "pkg_X_train['delay'] = pkg_X_train['delay'].fillna(global_mean_hrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f6a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg_X_train['delay'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# #sort the values by time taking into consideration the id\n",
    "# #packages_df= packages_df.sort_values(by=['MAILITM_FID', 'date'])\n",
    "# # Step 1: Train/Test split (80/20)\n",
    "# pkg_X_train, pkg_X_test, = train_test_split(\n",
    "#     packages_df,\n",
    "#     test_size=0.2, \n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# print (f\"Training set size: {pkg_X_train.shape[0]} rows\")\n",
    "# print (f\"Testing set size: {pkg_X_test.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176",
   "metadata": {},
   "source": [
    "* Splitting `receptacles` and dataset into training and testing sets using `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Step 1: Train/Test split (80/20)\n",
    "# rcp_X_train, rcp_X_test, = train_test_split(\n",
    "#     receptacles_df,\n",
    "#     test_size=0.2, \n",
    "#     random_state=42,\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# print (f\"Training set size: {rcp_X_train.shape[0]} rows\")\n",
    "# print (f\"Testing set size: {rcp_X_test.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178",
   "metadata": {},
   "source": [
    "* Splitting the training data for cross-validation (using 5 folds)\n",
    "* splits list holds 5 pairs of (X_train_fold, X_val_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# def cross_validation_split(pkg_X_train):\n",
    "#     splits = []\n",
    "\n",
    "#     for train_index, val_index in kf.split(pkg_X_train):\n",
    "#         pkg_X_train_fold, pkg_X_val_fold = pkg_X_train.iloc[train_index], pkg_X_train.iloc[val_index]\n",
    "#         splits.append((pkg_X_train_fold, pkg_X_val_fold))\n",
    "\n",
    "#     return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180",
   "metadata": {},
   "source": [
    "* Splitting the training data for cross-validation (using 5 folds)\n",
    "* splits list holds 5 pairs of (X_train_fold, X_val_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# def cross_validation_split(rcp_X_train):\n",
    "#     splits = []\n",
    "\n",
    "#     for train_index, val_index in kf.split(rcp_X_train):\n",
    "#         rcp_X_train_fold, rcp_X_val_fold = rcp_X_train.iloc[train_index], rcp_X_train.iloc[val_index]\n",
    "#         splits.append((rcp_X_train_fold, rcp_X_val_fold))\n",
    "\n",
    "#     return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_etab_mapping(df):\n",
    "    # 1. Calculate the global mode once (to use as a safe fallback)\n",
    "    global_mode = df['next_etablissement_postal'].mode().iat[0]\n",
    "\n",
    "    # 2. Get the mode for every group at once\n",
    "    # This creates a Series where index = etablissement, value = most frequent next\n",
    "    modes_per_group = df.groupby('etablissement_postal')['next_etablissement_postal'].agg(\n",
    "        lambda x: x.mode().iat[0] if not x.mode().empty else global_mode\n",
    "    )\n",
    "\n",
    "    # 3. Convert to dictionary\n",
    "    etablissement_dict = modes_per_group.to_dict()\n",
    "    return etablissement_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183",
   "metadata": {},
   "source": [
    "* After splitting, for each pair `(X_train, X_test)` of `packages_splits`, we'll fill the null values of `next_etablissement_postal` based on the most frequent value of `next_etablissement_postal` appearing with the value of `etablissement_postal` of each specific row having a null value in `next_etablissement_postal`\n",
    "* Example:\n",
    "say that a row has a null `next_etablissement_postal`, we look at it's `etablissement_postal` value (say `v`), we iterate through the training set, we count how many times each `ETAB_XXXX` in ` next_etablissement_postal` appears with `v` being in `etablissement_postal`, we take the mode, and we use it fill all rows having null value at `next_etablissement_postal` where their `etablissement_postal` is `v`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg_etablissement_dict = get_etab_mapping(pkg_X_train)\n",
    "# rcp_etablissement_dict = get_etab_mapping(rcp_X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185",
   "metadata": {},
   "source": [
    "* fill remaining null values of `next_etablissement_postal` with most frequent values of the training set only, avoiding *data leakage*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_mask = pkg_X_train['next_etablissement_postal'].isna()\n",
    "\n",
    "# Apply to original dataframe directly\n",
    "pkg_X_train.loc[null_mask, 'next_etablissement_postal'] = (\n",
    "    pkg_X_train.loc[null_mask, 'etablissement_postal'].map(pkg_etablissement_dict)\n",
    ")\n",
    "\n",
    "pkg_X_train['next_etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_mask = pkg_X_test['next_etablissement_postal'].isna()\n",
    "\n",
    "# Apply to original dataframe directly\n",
    "pkg_X_test.loc[null_mask, 'next_etablissement_postal'] = (\n",
    "    pkg_X_test.loc[null_mask, 'etablissement_postal'].map(pkg_etablissement_dict)\n",
    ")\n",
    "\n",
    "pkg_X_test['next_etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188",
   "metadata": {},
   "source": [
    "* 11 rows still have NaN `next_etablissement_postal` because some values in the test set didn't exist in the training set, therefore they didn't find the right etablissement to map to\n",
    "* we'll fill them with `Unknown`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg_X_test['next_etablissement_postal'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190",
   "metadata": {},
   "source": [
    "* apply to `receptacles` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# null_mask = rcp_X_train['next_etablissement_postal'].isna()\n",
    "\n",
    "# # Apply to original dataframe directly\n",
    "# rcp_X_train.loc[null_mask, 'next_etablissement_postal'] = (\n",
    "#     rcp_X_train.loc[null_mask, 'etablissement_postal'].map(rcp_etablissement_dict)\n",
    "# )\n",
    "\n",
    "# rcp_X_train['next_etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# null_mask = rcp_X_test['next_etablissement_postal'].isna()\n",
    "\n",
    "# # Apply to original dataframe directly\n",
    "# rcp_X_test.loc[null_mask, 'next_etablissement_postal'] = (\n",
    "#     rcp_X_test.loc[null_mask, 'etablissement_postal'].map(rcp_etablissement_dict)\n",
    "# )\n",
    "\n",
    "# rcp_X_test['next_etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193",
   "metadata": {},
   "source": [
    "* ... start adding new **useful and justified** features to each dataset ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194",
   "metadata": {},
   "source": [
    "- adding the delay target feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_average_delays(train_df, test_df):\n",
    "#     # 1. Ensure sorted order for calculation\n",
    "#     train_df = train_df.sort_values(['MAILITM_FID', 'date'])\n",
    "    \n",
    "#     # 2. Calculate time to next event in hours\n",
    "#     train_df['next_event_date'] = train_df.groupby('MAILITM_FID')['date'].shift(-1)\n",
    "#     train_df['actual_delay_val'] = (train_df['next_event_date'] - train_df['date']).dt.total_seconds() / 3600\n",
    "\n",
    "#     # 3. Create logical consistency mask\n",
    "#     train_df['next_row_etab'] = train_df.groupby('MAILITM_FID')['etablissement_postal'].shift(-1)\n",
    "#     valid_delay_mask = (train_df['next_etablissement_postal'] == train_df['next_row_etab'])\n",
    "    \n",
    "#     # 4. Calculate Mean Delay per Pair (using only valid rows)\n",
    "#     delay_mapping = train_df[valid_delay_mask].groupby(\n",
    "#         ['etablissement_postal', 'next_etablissement_postal']\n",
    "#     )['actual_delay_val'].mean().to_dict()\n",
    "\n",
    "#     # 5. Global mean for fallback\n",
    "#     global_mean_hrs = train_df[valid_delay_mask]['actual_delay_val'].mean()\n",
    "\n",
    "#     # 6. Apply to both DataFrames\n",
    "#     for d in [train_df, test_df]:\n",
    "#         pairs = list(zip(d['etablissement_postal'], d['next_etablissement_postal']))\n",
    "        \n",
    "#         # Applying logic: Use route mean if valid, else global mean\n",
    "#         # (Using valid_delay_mask for train_df to catch that 3rd row)\n",
    "#         if 'next_row_etab' in d.columns:\n",
    "#             d['delay'] = [\n",
    "#                 delay_mapping.get(pairs[i], global_mean_hrs) if valid_delay_mask.iloc[i] \n",
    "#                 else global_mean_hrs for i in range(len(d))\n",
    "#             ]\n",
    "#         else:\n",
    "#             d['delay'] = [delay_mapping.get(p, global_mean_hrs) for p in pairs]\n",
    "\n",
    "#     # --- Print Statements for Debugging ---\n",
    "#     print(f\"Global Mean applied to invalid/unseen routes: {global_mean_hrs:.2f} hours\")\n",
    "#         # ---------------------------------------\n",
    "\n",
    "#     return train_df.drop(columns=['next_event_date', 'actual_delay_val', 'next_row_etab']), \\\n",
    "#            test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196",
   "metadata": {},
   "source": [
    "- Apply the function on packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #apply to packages data\n",
    "# pkg_X_train, pkg_X_test = apply_average_delays(pkg_X_train, pkg_X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198",
   "metadata": {},
   "source": [
    "# Use CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from catboost import CatBoostRegressor, Pool, cv\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# --- Define Model Parameters and Features ---\n",
    "TARGET_COL = 'delay'\n",
    "\n",
    "# List of categorical features\n",
    "cat_features = [\n",
    "    'etablissement_postal',\n",
    "    'next_etablissement_postal',\n",
    "    'origin_country',\n",
    "    'destination_country',\n",
    "    'service_indicator',\n",
    "    'day_of_week',\n",
    "    'origin_destination'\n",
    "\n",
    "    # Add any other categorical columns here\n",
    "]\n",
    "\n",
    "# Columns to drop from features (IDs, date used for sorting, and the target)\n",
    "drop_cols = [TARGET_COL, 'MAILITM_FID', 'RECPTCL_FID', 'date', 'serial_number','flow_type','origin','destination']\n",
    "\n",
    "# Prepare the full 80% training set features and target\n",
    "X_train_full = pkg_X_train.drop(columns=[c for c in drop_cols if c in pkg_X_train.columns])\n",
    "y_train_full = pkg_X_train[TARGET_COL]\n",
    "\n",
    "# Identify categorical features by name\n",
    "categorical_feature_names = [c for c in cat_features if c in X_train_full.columns]\n",
    "\n",
    "# --- TimeSeries Cross-Validation to Find Optimal Iterations ---\n",
    "\n",
    "# Create the training Pool\n",
    "train_pool = Pool(\n",
    "    data=X_train_full,\n",
    "    label=y_train_full,\n",
    "    cat_features=categorical_feature_names\n",
    ")\n",
    "\n",
    "cv_params = {\n",
    "    'loss_function': 'RMSE',\n",
    "    'iterations': 1500,\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'random_seed': 42,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "print(\"Starting TimeSeries Cross-Validation (5 Folds) to find optimal tree count...\")\n",
    "\n",
    "cv_results = cv(\n",
    "    params=cv_params,\n",
    "    pool=train_pool,\n",
    "    fold_count=5,\n",
    "    shuffle=False,               # CRITICAL: Ensures chronological order\n",
    "    type='TimeSeries',           # Uses the rolling window strategy\n",
    ")\n",
    "\n",
    "\n",
    "# Find the best iteration based on the minimum average RMSE\n",
    "best_iter = cv_results['test-RMSE-mean'].values.argmin()\n",
    "best_rmse = cv_results['test-RMSE-mean'].min()\n",
    "print(f\"Optimal number of CatBoost iterations: {best_iter + 1} (Best CV RMSE: {best_rmse:.4f})\")\n",
    "\n",
    "# --- Train Final Model on ENTIRE Training Set ---\n",
    "\n",
    "final_model = CatBoostRegressor(\n",
    "    iterations=best_iter + 1,  # Use the optimal number of trees found in CV\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    loss_function='RMSE',\n",
    "    random_seed=42,\n",
    "    cat_features=categorical_feature_names,\n",
    "    verbose=100  # Show training progress for the final model\n",
    ")\n",
    "\n",
    "print(\"\\nTraining final model on the entire 80% training set...\")\n",
    "final_model.fit(train_pool)\n",
    "\n",
    "# --- Predict and Evaluate on the Future Test Set ---\n",
    "\n",
    "# Prepare the test features\n",
    "X_test_features = pkg_X_test.drop(columns=[c for c in drop_cols if c in pkg_X_test.columns])\n",
    "y_test_target = pkg_X_test[TARGET_COL] # The 'delay' column in pkg_X_test is the ground truth target\n",
    "\n",
    "predictions = final_model.predict(X_test_features)\n",
    "test_rmse = root_mean_squared_error(y_test_target, predictions)\n",
    "\n",
    "print(f\"\\nFinal Model Performance on 20% Future Test Data (RMSE): {test_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7278c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_target = y_test_target.fillna(global_mean_hrs)\n",
    "y_test_target.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d25c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "test_rmse = root_mean_squared_error(y_test_target, predictions)\n",
    "\n",
    "maee = np.mean(np.abs(y_test_target - predictions))\n",
    "\n",
    "print(f\"\\nFinal Model Performance on 20% Future Test Data (RMSE): {test_rmse:.4f}\")\n",
    "print(f\"\\nFinal Model Performance on 20% Future Test Data (mae): {maee:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d124f2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg_X_train.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Importance Analysis ---\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the feature importances from the trained model\n",
    "feature_importances = final_model.get_feature_importance()\n",
    "feature_names = X_train_full.columns\n",
    "\n",
    "# Create a Series for easy sorting and handling\n",
    "importance_series = pd.Series(feature_importances, index=feature_names)\n",
    "\n",
    "# Sort the features by importance (descending)\n",
    "sorted_importance = importance_series.sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n--- Feature Importances (Top 10) ---\")\n",
    "print(sorted_importance.head(10))\n",
    "\n",
    "# Optional: Plot the top 10 features\n",
    "plt.figure(figsize=(10, 6))\n",
    "sorted_importance.head(10).plot(kind='barh', color='skyblue')\n",
    "plt.title('Top 10 CatBoost Feature Importances')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Target Variable Comparison ---\")\n",
    "print(\"Training Delay Statistics:\")\n",
    "print(pkg_X_train['delay'].describe())\n",
    "\n",
    "print(\"\\nTesting Delay Statistics:\")\n",
    "print(pkg_X_test['delay'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202",
   "metadata": {},
   "source": [
    "The 6,024-hour max in the training set and the 3,883-hour max in the test set are still extremely high (e.g., 6,024 hours≈8.4 months). These current outliers, combined with the high mean delay (52−57 hours), mean that the target variable (delay) is still heavily right-skewed.\n",
    "This extreme skew is the root cause of the massive difference between the internal CV RMSE (≈24 hours) and the final Test RMSE (≈6,030 hours). The CatBoost model is predicting the average of the distribution, and the tail (the very long delays) is pulling the final RMSE score up dramatically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
