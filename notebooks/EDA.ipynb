{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "id": "0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "1"
   },
   "source": [
    "make sure to rename the columns by removing é\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GOwfAmPIq0Rt",
    "outputId": "580c693c-41fd-42dd-a393-ff7e70fc5e64"
   },
   "outputs": [],
   "source": [
    "###### UNCOMMENT THIS IF YOU ARE WORKING ON COLAB #######\n",
    "\n",
    "\n",
    "# from google.colab import drive\n",
    "# import os\n",
    "\n",
    "# # 1. This will ask for permission to access your Drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# # 2. Navigate to your project folder\n",
    "# # Check your Drive path - it usually starts with /content/drive/MyDrive/\n",
    "# # Adjust the path below to match where you uploaded the folder\n",
    "# path = \"/content/drive/MyDrive/Data-Mining/data/raw\"\n",
    "# os.chdir(path)\n",
    "\n",
    "# # 3. List files to make sure you are in the right spot\n",
    "# print(\"Successfully connected! Files in folder:\")\n",
    "# print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "2"
   },
   "source": [
    "# Exploratory Data Analysis (EDA) for Packages and Receptacle Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "3"
   },
   "outputs": [],
   "source": [
    "###### UNCOMMENT THIS IF YOU ARE WORKING ON COLAB #######\n",
    "\n",
    "# packages_df = pd.read_csv('packages_data_2023_2025.csv',delimiter=';', encoding='latin-1')\n",
    "# receptacles_df = pd.read_csv('receptacle_data_2023_2025.csv',delimiter=';', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "packages_df = pd.read_csv('../data/raw/packages_data_2023_2025.csv',delimiter=';', encoding='latin-1')\n",
    "receptacles_df = pd.read_csv('../data/raw/receptacle_data_2023_2025.csv',delimiter=';', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "4"
   },
   "source": [
    "* Columns' names adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "id": "5"
   },
   "outputs": [],
   "source": [
    "packages_df = packages_df.rename(columns={'établissement_postal': 'etablissement_postal', 'next_établissement_postal': 'next_etablissement_postal'})\n",
    "receptacles_df = receptacles_df.rename(columns={'ï»¿RECPTCL_FID': 'RECPTCL_FID', 'EVENT_TYPECD': 'EVENT_TYPE_CD', 'nextetablissement_postal': 'next_etablissement_postal'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6",
    "outputId": "01741b39-75e4-4c4d-a9fe-2d79a523fd53"
   },
   "outputs": [],
   "source": [
    "packages_df.shape, receptacles_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "id": "7"
   },
   "source": [
    "* Columns' types adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "id": "8"
   },
   "outputs": [],
   "source": [
    "packages_df['date'] = pd.to_datetime(packages_df['date'])\n",
    "receptacles_df['date'] = pd.to_datetime(receptacles_df['date'])\n",
    "packages_df['RECPTCL_FID'] = packages_df['RECPTCL_FID'].str.strip()\n",
    "packages_df['MAILITM_FID'] = packages_df['MAILITM_FID'].str.strip()\n",
    "packages_df['etablissement_postal'] = packages_df['etablissement_postal'].str.strip()\n",
    "packages_df['next_etablissement_postal'] = packages_df['next_etablissement_postal'].str.strip()\n",
    "receptacles_df['etablissement_postal'] = receptacles_df['etablissement_postal'].str.strip()\n",
    "receptacles_df['next_etablissement_postal'] = receptacles_df['next_etablissement_postal'].str.strip()\n",
    "receptacles_df['RECPTCL_FID'] = receptacles_df['RECPTCL_FID'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "id": "DrH_PNOEjLIc"
   },
   "outputs": [],
   "source": [
    "raw_clustering_pkg = packages_df.copy()\n",
    "raw_clustering_rcp = receptacles_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9",
    "outputId": "fc172018-78c4-424a-c4a6-d85a42907521"
   },
   "outputs": [],
   "source": [
    "packages_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10",
    "outputId": "1f7bc01f-c471-4355-a4e6-39078f6d449f"
   },
   "outputs": [],
   "source": [
    "receptacles_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "11",
    "outputId": "2803ddf9-4453-4ab5-bea5-bc84fc410014"
   },
   "outputs": [],
   "source": [
    "packages_df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "12",
    "outputId": "498d8746-c41e-4596-f5b5-c3a136e735f7"
   },
   "outputs": [],
   "source": [
    "receptacles_df.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "13"
   },
   "source": [
    " Initial Observations\n",
    "- both datasets cover the period from 2023 to 2025\n",
    "- we have no target variable in either dataset\n",
    "- for packages dataset:\n",
    "    - 6 features in total with 5 categorical and 1 numerical\n",
    "    - MAILITM_FID is unique identifier for each package\n",
    "    - RECPTCL_FID is foreign key linking to receptacle dataset\n",
    "    - etablissement_postal and next_etablissement_postal have some null values\n",
    "- for receptacle dataset:\n",
    "    - 5 features in total with 4 categorical and 1 numerical\n",
    "    - RECPTCL_FID is unique identifier for each receptacle\n",
    "    - EVENT_TYPE_CD has some null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14",
    "outputId": "1567a63e-1507-47f8-a6ba-22ac91d3b120"
   },
   "outputs": [],
   "source": [
    "for column in packages_df.columns:\n",
    "    print(f'{column} has {packages_df[column].nunique()} unique values.')\n",
    "    print(f'{column} has {packages_df[column].isnull().sum()} null values.')\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15",
    "outputId": "b9f3f173-1867-4ab3-d641-96fed8d62ede"
   },
   "outputs": [],
   "source": [
    "for column in receptacles_df.columns:\n",
    "    print(f'{column} has {receptacles_df[column].nunique()} unique values.')\n",
    "    print(f'{column} has {receptacles_df[column].isnull().sum()} null values.')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "id": "16"
   },
   "source": [
    "we notice the following:<br>\n",
    "- receptacle dataset has more unique values for RECPTCL_FID than packages dataset, indicating one-to-many relationship<br>\n",
    "- MAILITM_FID is unique in packages dataset.<br>\n",
    "- packages dataset have more unique date values than receptacle dataset.<br>\n",
    "- both datasets have null values in etablissement_postal and next_etablissement_postal columns. This requires processing later on<br>\n",
    "- packages dataset has more unique values in the next_etablissement_postal column compared to receptacle dataset but also more null values. **further investigation is needed to understand why**<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "17",
    "outputId": "241b6c3b-14f4-4d06-8c02-255afcadd0e4"
   },
   "outputs": [],
   "source": [
    "packages_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "18",
    "outputId": "6e62537e-e463-4215-a85f-a8079c00b1e1"
   },
   "outputs": [],
   "source": [
    "receptacles_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "19"
   },
   "source": [
    "for EVENT_TYPE_CD we notice different range of values for packages and receptacle datasets indicating different types of events.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "id": "20"
   },
   "source": [
    "#### Visualizations\n",
    "for now we will visualize the distribution of EVENT_TYPE_CD in both datasets.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "21",
    "outputId": "ec667815-3732-4486-88ba-8d96a4431afe"
   },
   "outputs": [],
   "source": [
    "# Distribution of EVENT_TYPE_CD in packages dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=packages_df,x='EVENT_TYPE_CD')\n",
    "plt.title('distribution of EVENT_TYPE_CD in packages dataset')\n",
    "plt.xlabel('EVENT_TYPE_CD')\n",
    "plt.ylabel('count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "id": "22",
    "outputId": "29d0afd8-77ed-4cbe-f2cd-5e657d694454"
   },
   "outputs": [],
   "source": [
    "# Distribution of EVENT_TYPE_CD in receptacle dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=receptacles_df,x='EVENT_TYPE_CD')\n",
    "plt.title('distribution of EVENT_TYPE_CD in Receptacle dataset')\n",
    "plt.xlabel('EVENT_TYPE_CD')\n",
    "plt.ylabel('count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "id": "23"
   },
   "source": [
    "for etablissement_postal and next_etablissement_postal I will start with visualizing the receptacle dataset since the packages dataset has a lot of unique values<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 628
    },
    "id": "24",
    "outputId": "7059b90a-7772-4059-ef7b-f8e2098bc724"
   },
   "outputs": [],
   "source": [
    "# Distribution of etablissement_postal in receptacle dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=receptacles_df,x='etablissement_postal')\n",
    "plt.title('distribution of etablissement_postal in Receptacle dataset')\n",
    "plt.xlabel('etablissement_postal')\n",
    "plt.ylabel('count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 628
    },
    "id": "25",
    "outputId": "82040228-591a-416c-dbc8-fbcb6df19cc7"
   },
   "outputs": [],
   "source": [
    "# Distribution of next_etablissement_postal in receptacle dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=receptacles_df,x='next_etablissement_postal')\n",
    "plt.title('distribution of next_etablissement_postal in Receptacle dataset')\n",
    "plt.xlabel('next_etablissement_postal')\n",
    "plt.ylabel('count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 781
    },
    "id": "26",
    "outputId": "b64f4856-1ed6-4584-cef4-18518fb8c55d"
   },
   "outputs": [],
   "source": [
    "# 1.  Top 10 locations to keep the map small\n",
    "top_locs = receptacles_df['etablissement_postal'].value_counts().head(10).index\n",
    "top_next = receptacles_df['next_etablissement_postal'].value_counts().head(10).index\n",
    "\n",
    "# 2. include only these top locations\n",
    "filtered_df = receptacles_df[\n",
    "    (receptacles_df['etablissement_postal'].isin(top_locs)) &\n",
    "    (receptacles_df['next_etablissement_postal'].isin(top_next))\n",
    "]\n",
    "\n",
    "# 3. Create a Matrix (Cross-tabulation)\n",
    "matrix = pd.crosstab(filtered_df['etablissement_postal'], filtered_df['next_etablissement_postal'])\n",
    "\n",
    "# 4. Plot Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(matrix, cmap='Reds', annot=True, fmt='d', linewidths=.5)\n",
    "plt.title(\"Most Frequent Routes (Origin vs Destination) - Receptacle Dataset\")\n",
    "plt.xlabel(\"Next Destination\")\n",
    "plt.ylabel(\"Current Location\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "id": "27"
   },
   "source": [
    "we notice that some etablissements have significantly higher traffic compared to others, indicating  major distribution centers.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "id": "28"
   },
   "source": [
    "for etablissement_postal and next_etablissement_postal we will create a heatmap to visualize the flow between current location and next destination.<br>\n",
    "Count of parcels moving from A to B to see the density of connections between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "id": "29",
    "outputId": "6a8d66ee-acfd-4d2d-96d8-8fa638803a45"
   },
   "outputs": [],
   "source": [
    "# 1.  Top 10 locations to keep the map small\n",
    "top_locs = packages_df['etablissement_postal'].value_counts().head(10).index\n",
    "top_next = packages_df['next_etablissement_postal'].value_counts().head(10).index\n",
    "\n",
    "# 2. include only these top locations\n",
    "filtered_df = packages_df[\n",
    "    (packages_df['etablissement_postal'].isin(top_locs)) &\n",
    "    (packages_df['next_etablissement_postal'].isin(top_next))\n",
    "]\n",
    "\n",
    "# 3. Create a Matrix (Cross-tabulation)\n",
    "matrix = pd.crosstab(filtered_df['etablissement_postal'], filtered_df['next_etablissement_postal'])\n",
    "\n",
    "# 4. Plot Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(matrix, cmap='Reds', annot=True, fmt='d', linewidths=.5)\n",
    "plt.title(\"Most Frequent Routes (Origin vs Destination) - Packages Dataset\")\n",
    "plt.xlabel(\"Next Destination\")\n",
    "plt.ylabel(\"Current Location\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 671
    },
    "id": "30",
    "outputId": "cca921f7-5aac-4205-e1fd-716d75d5d2f6"
   },
   "outputs": [],
   "source": [
    "# Count packages per location\n",
    "location_counts = packages_df['etablissement_postal'].value_counts().reset_index()\n",
    "location_counts.columns = ['Location', 'Volume']\n",
    "\n",
    "# keep only top 20 busiest centers\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Volume', y='Location', data=location_counts.head(20), palette='viridis')\n",
    "plt.title(\"Top 20 Busiest Postal Centers\")\n",
    "plt.xlabel(\"Number of Packages\")\n",
    "plt.ylabel(\"Center ID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "id": "31"
   },
   "source": [
    "we notice the same pattern as before with some etablissements having significantly higher trafic compared to others.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "32",
    "outputId": "9c6959c7-a4b5-4786-be8a-a6453003216c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. Extract Time Features\n",
    "packages_df['hour'] = packages_df['date'].dt.hour\n",
    "packages_df['day_of_week'] = packages_df['date'].dt.day_name()\n",
    "\n",
    "# 2. Create a Pivot Table (Cross-tabulation)\n",
    "# Rows = Day, Cols = Hour, Values = Count of Scans\n",
    "heatmap_data = pd.crosstab(\n",
    "    packages_df['day_of_week'],\n",
    "    packages_df['hour']\n",
    ")\n",
    "\n",
    "days_order = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "heatmap_data = heatmap_data.reindex(days_order)\n",
    "\n",
    "# 3. Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(heatmap_data, cmap='YlOrRd', linewidths=.5, annot=False)\n",
    "plt.title(\"Package Scan Activity by Day and Hour\")\n",
    "plt.xlabel(\"Hour of Day (0-23)\")\n",
    "plt.ylabel(\"Day of Week\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {
    "id": "33"
   },
   "source": [
    "we notice that the busiest times for package scans are during weekdays, particularly from mid-morning to late afternoon.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "34",
    "outputId": "78501e4a-6553-464f-efb7-3133c8acac54"
   },
   "outputs": [],
   "source": [
    "#number of packages per receptacle\n",
    "packages_per_receptacle = packages_df.groupby('RECPTCL_FID')['MAILITM_FID'].nunique()\n",
    "packages_per_receptacle.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "id": "35"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36",
    "outputId": "07356860-40ec-4fd0-f0ad-0f383e5426df"
   },
   "outputs": [],
   "source": [
    "packages_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37",
    "outputId": "12b1ac91-045b-4afd-8254-98f86882d9ff"
   },
   "outputs": [],
   "source": [
    "receptacles_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {
    "id": "38"
   },
   "source": [
    "- Drop the packages starting from 2020 and keep only the ones starting from 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "id": "39"
   },
   "outputs": [],
   "source": [
    "#in packages df Drop the values that are from 2020 and start only from 2023\n",
    "packages_df = packages_df[packages_df['date'].dt.year >= 2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40",
    "outputId": "156db96f-534b-4c38-9dcb-f028f98835df"
   },
   "outputs": [],
   "source": [
    "packages_df['etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {
    "id": "41"
   },
   "source": [
    "* `etablissement_postal` have 26772 null values (2.7% of the whole dataset)\n",
    "* As its null values are less than 5% of the dataset (2.7%), we drop these null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42",
    "outputId": "550ddb42-e40a-43ab-8916-85083bd69817"
   },
   "outputs": [],
   "source": [
    "packages_df = packages_df[~packages_df['etablissement_postal'].isna()]\n",
    "packages_df['etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {
    "id": "43"
   },
   "source": [
    "* We propose to consider the packages having null `next_etablissement_postal`\n",
    "as having issue during transfer, we'll try to validate that using\n",
    "`EVENT_TYPE_CD` also\n",
    "* Let's check if `EVENT_TYPE_CD` can indicate whether the `next_etablissement_postal` is null or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "44",
    "outputId": "b3780d54-d55e-409f-fef5-5a4c385787a0"
   },
   "outputs": [],
   "source": [
    "packages_unknown_next_etablissement = packages_df[packages_df['next_etablissement_postal'].isna()]\n",
    "# keep only top EVENT_TYPES_ID\n",
    "packages_unknown_next_etablissement = packages_unknown_next_etablissement['EVENT_TYPE_CD'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "packages_unknown_next_etablissement.head(10).plot(kind='bar')\n",
    "plt.xlabel('EVENT TYPE CD')\n",
    "plt.ylabel('Null Next Etablissement Postal')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {
    "id": "45"
   },
   "source": [
    "* `EVENT_TYPE_CD` doesn't actually indicate null values of `next_etablissement_postal`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {
    "id": "46"
   },
   "source": [
    "* the function `fill_NaN_next_etab` cell fills the `next_etablissement_postal` using the next `etablissement_postal` for the same package.\n",
    "* if the last route for a specific package is null, then it keeps it null because there's no next `etablissement_postal` for that package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "id": "47"
   },
   "outputs": [],
   "source": [
    "def fill_NaN_next_etab(df, id_col):\n",
    "    # 1. Ensure the dataframe is sorted (same as before)\n",
    "    df = df.sort_values([id_col, 'date'])\n",
    "\n",
    "    # 2. Look ahead to the next row's postal code and ID\n",
    "    shifted_postal = df['etablissement_postal'].shift(-1)\n",
    "    shifted_id = df[id_col].shift(-1)\n",
    "# 3. Identify the \"boundaries\" where the postal code changes within the same package\n",
    "# This marks the last row of a block with the value of the start of the next block\n",
    "    is_boundary = (df['etablissement_postal'] != shifted_postal) & \\\n",
    "              (df[id_col] == shifted_id)\n",
    "# 4. Use grouped backfill to broadcast those values to all preceding rows in the block\n",
    "# This replaces your 'blocks.map' logic with a single vectorized pass\n",
    "    fill_values = shifted_postal.where(is_boundary).groupby(df[id_col]).bfill()\n",
    "\n",
    "# 5. Fill only the NaNs in the existing column to match your original logic\n",
    "    df['next_etablissement_postal'] = df['next_etablissement_postal'].fillna(fill_values)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48",
    "outputId": "21a8d95e-70b5-49c3-b026-b92c137b36d5"
   },
   "outputs": [],
   "source": [
    "# Apply the function to fill NaN values in next_etablissement_postal\n",
    "packages_df = fill_NaN_next_etab(packages_df, 'MAILITM_FID')\n",
    "# Check remaining NaNs\n",
    "packages_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {
    "id": "49"
   },
   "source": [
    "* Like this, we've handled a good part of null values and inconsitencies for `packages` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {
    "id": "50"
   },
   "source": [
    "* **We'll be doing the same steps for `receptacle` dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51",
    "outputId": "cc4496be-4375-4bbc-928b-15f1e9b8a213"
   },
   "outputs": [],
   "source": [
    "receptacles_df['etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {
    "id": "52"
   },
   "source": [
    "* Dropping rows having null `etablissement_postal`, as they're just 0.1% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "53",
    "outputId": "83378e83-543a-45cc-ebe4-0e4ecded98e4"
   },
   "outputs": [],
   "source": [
    "receptacles_df = receptacles_df[~receptacles_df['etablissement_postal'].isna()]\n",
    "receptacles_df['etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {
    "id": "54"
   },
   "source": [
    "* apply the function that fills null values of `next_etablissement_postal` using `etablissement_postal` to `receptacles_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55",
    "outputId": "65eae88e-f21f-4355-f592-c2fbdb11bc74"
   },
   "outputs": [],
   "source": [
    "receptacles_df = fill_NaN_next_etab(receptacles_df, 'RECPTCL_FID')\n",
    "# Check remaining NaNs\n",
    "receptacles_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {
    "id": "56"
   },
   "source": [
    "* Null values are mostly gone, but there are still some illogical packages' and receptacles' routes between `etablissements`\n",
    "* We'll treat these logical routes now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {
    "id": "57"
   },
   "outputs": [],
   "source": [
    "# for each package (group of rows), check whether there's any illogical route\n",
    "# between 'etablissement_postal' and 'next_etablissement_postal'\n",
    "def isPackageIllogical(group):\n",
    "    return (\n",
    "        group['next_etablissement_postal']\n",
    "        .iloc[:-1]\n",
    "        .ne(group['etablissement_postal'].shift(-1).iloc[:-1])\n",
    "        .any()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "58",
    "outputId": "5d2f30e8-6f06-468b-b473-b3e25250fc0e"
   },
   "outputs": [],
   "source": [
    "illogical_packages = packages_df.groupby('MAILITM_FID').apply(isPackageIllogical)\n",
    "illogical_packages.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59",
    "outputId": "aec3193b-fdcb-4a00-bec8-462d4c4eceb3"
   },
   "outputs": [],
   "source": [
    "103376 / packages_df['MAILITM_FID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {
    "id": "60"
   },
   "source": [
    "* 103376 Packages have illogical routes (98%) of all packages, so it's impossible to drop them, but instead, we plan to ignore the `MAILITM_FID` and `RECPTCL_FID` in the training and testing sets that will come next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {
    "id": "61"
   },
   "outputs": [],
   "source": [
    "# for each receptacle (group of rows), check whether there's any illogical route\n",
    "# between 'etablissement_postal' and 'next_etablissement_postal'\n",
    "def isReceptacleIllogical(group):\n",
    "    return (\n",
    "        group['next_etablissement_postal']\n",
    "        .iloc[:-1]\n",
    "        .ne(group['etablissement_postal'].shift(-1).iloc[:-1])\n",
    "        .any()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "62",
    "outputId": "e74a7390-9a36-4b44-d676-b4e771058e18"
   },
   "outputs": [],
   "source": [
    "illogical_receptacles = receptacles_df.groupby('RECPTCL_FID').apply(isReceptacleIllogical)\n",
    "illogical_receptacles.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63",
    "outputId": "984d2906-5f5c-4ff0-8e13-039cac266fdc"
   },
   "outputs": [],
   "source": [
    "205519 / receptacles_df['RECPTCL_FID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {
    "id": "64"
   },
   "source": [
    "205519 receptacles have illogical routes (95%) of all receptacles, so it's also impossible to drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {
    "id": "65"
   },
   "outputs": [],
   "source": [
    "# keep copies for backup (en cas ou)\n",
    "packages_df_copy = packages_df.copy()\n",
    "receptacles_df_copy = receptacles_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {
    "id": "66"
   },
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {
    "id": "67"
   },
   "source": [
    "### Check RECPTCL_FID and MAILITM_FID having same length formats\n",
    "if yes then we can split them into meaningfull parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68",
    "outputId": "bfab005b-4b47-47d4-adbc-1e4053ebacfb"
   },
   "outputs": [],
   "source": [
    "same=1\n",
    "print(\"\\n=== RECPTCL_FID  ===\")\n",
    "print(f\"testing if the lengths of RECPTCL_FID values are all the same:\")\n",
    "for val in packages_df['RECPTCL_FID'].values:\n",
    "    if len(str(val)) != 29 :\n",
    "        print(f\"  {val} (length: {len(str(val))})\")\n",
    "        same=0\n",
    "        break\n",
    "if same==1:\n",
    "    print('all same length' )\n",
    "\n",
    "same=1\n",
    "print(\"\\n=== MAILITM_FID  ===\")\n",
    "print(f\"testing if the lengths of MAILITM_FID values are all the same:\")\n",
    "for val in packages_df['MAILITM_FID'].values:\n",
    "    if len(str(val)) != 13 :\n",
    "        print(f\"  {val} (length: {len(str(val))})\")\n",
    "        same=0\n",
    "        break\n",
    "if same==1:\n",
    "    print ('all same length' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {
    "id": "69"
   },
   "source": [
    "### RECPTCL_FID Analysis\n",
    "- **Format:** 29-character string (e.g., `USORDADZALGDAUN30050001900005`)\n",
    "- **Data Quality:** No null values (1,000,000) | 215,867 unique values in receptacle dataset and 45306 unique values in packages dataset\n",
    "- **Extractable Features:**\n",
    "  - Origin Country (2 chars): US, FR, AE, etc.\n",
    "  - Destination Country (2 chars): DZ, AI, AA, etc.\n",
    "\n",
    "### MAILITM_FID Analysis\n",
    "- **Format:** 13-character string according to the S10-12 patern (e.g., `CA000132868US`, `CA000340856PK`)\n",
    "- **Data Quality:** No null values (1,000,000 packages)\n",
    "- **Extractable Features:**\n",
    "  - Service Indicator (2 chars): CA, etc.\n",
    "  - Serial Number (8 chars): 00013286, 00034085, etc.\n",
    "  - Check Digit (1 char): 8, 6, etc.\n",
    "  - Country Code (3 chars, right-stripped): US, PK.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {
    "id": "70"
   },
   "source": [
    "## Definition of the parser funtions\n",
    "These functions are responsible for spliting the IDs into parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {
    "id": "71"
   },
   "outputs": [],
   "source": [
    "def parse_recptcl_fid(id_str):\n",
    "    origin_country = id_str[0:2]\n",
    "    destination_country = id_str[6:8]\n",
    "    return origin_country, destination_country\n",
    "\n",
    "def parse_mailitm_fid(id_str):\n",
    "    service_indicator = id_str[0:2]\n",
    "    serial_number = id_str[2:11]\n",
    "    country_code = id_str[11:14].strip()\n",
    "    return service_indicator, serial_number, country_code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {
    "id": "72"
   },
   "source": [
    "### Apply parsing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {
    "id": "73"
   },
   "outputs": [],
   "source": [
    "# parsed_receptacles_df = receptacles_df.copy()\n",
    "# parsed_receptacles_df[['origin_country', 'destination_country']] = parsed_receptacles_df['RECPTCL_FID'].apply(lambda x: pd.Series(parse_recptcl_fid(x)))\n",
    "\n",
    "# Faster method:\n",
    "parsed_data = list(receptacles_df['RECPTCL_FID'].apply(parse_recptcl_fid))\n",
    "\n",
    "# Assign to new columns by creating a temporary DataFrame\n",
    "parsed_receptacles_df = receptacles_df.copy()\n",
    "parsed_receptacles_df[['origin_country', 'destination_country']] = pd.DataFrame(\n",
    "    parsed_data, index=receptacles_df.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {
    "id": "74"
   },
   "outputs": [],
   "source": [
    "# parsed_packages_df = packages_df.copy()\n",
    "# parsed_packages_df[['service_indicator', 'serial_number', 'country_code']] = parsed_packages_df['MAILITM_FID'].apply(lambda x: pd.Series(parse_mailitm_fid(x)))\n",
    "# parsed_packages_df[['origin_country','destination_country']] = parsed_packages_df['RECPTCL_FID'].apply(lambda x: pd.Series(parse_recptcl_fid(x)))\n",
    "\n",
    "# Faster method:\n",
    "parsed_packages_df = packages_df.copy()\n",
    "\n",
    "# 1. Optimize MAILITM_FID parsing\n",
    "mailitm_data = list(parsed_packages_df['MAILITM_FID'].apply(parse_mailitm_fid))\n",
    "parsed_packages_df[['service_indicator', 'serial_number', 'country_code']] = pd.DataFrame(\n",
    "    mailitm_data, index=parsed_packages_df.index\n",
    ")\n",
    "\n",
    "# 2. Optimize RECPTCL_FID parsing\n",
    "recptcl_data = list(parsed_packages_df['RECPTCL_FID'].apply(parse_recptcl_fid))\n",
    "parsed_packages_df[['origin_country', 'destination_country']] = pd.DataFrame(\n",
    "    recptcl_data, index=parsed_packages_df.index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {
    "id": "75"
   },
   "source": [
    "### show samples of new parsed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "76",
    "outputId": "c2c8a582-be82-41fd-c8ff-6783423a5eb6"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"=== packages_df sample with new parsed columns ===\")\n",
    "parsed_packages_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "77",
    "outputId": "814ed1f8-895b-4ecc-bb4a-30f71a391e2b"
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== receptacles_df sample with new parsed columns ===\")\n",
    "parsed_receptacles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {
    "id": "78"
   },
   "source": [
    "# Analysis of the extrcted features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {
    "id": "79"
   },
   "source": [
    "## 1. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80",
    "outputId": "7d5c7f93-1f69-4f52-e4b0-710328b359a7"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n--- Unique Value Counts for parsed_packages_df ---\")\n",
    "print(\"\\nFor receptacle FID parsing:\")\n",
    "print(f\"Unique origin_country values: {parsed_packages_df['origin_country'].nunique()}\")\n",
    "print(f\"Unique destination_country values: {parsed_packages_df['destination_country'].nunique()}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"\\nFor mail item FID parsing:\")\n",
    "print(f\"Unique service_indicator values: {parsed_packages_df['service_indicator'].nunique()}\")\n",
    "print(f\"Unique country_code values: {parsed_packages_df['country_code'].nunique()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {
    "id": "81"
   },
   "source": [
    "## 2. parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82",
    "outputId": "f83cffcb-b040-40b3-e039-7095515f6741"
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Unique Value Counts for parsed_receptacles_df ---\")\n",
    "print(f\"Unique origin_country values: {parsed_receptacles_df['origin_country'].nunique()}\")\n",
    "print(f\"Unique destination_country values: {parsed_receptacles_df['destination_country'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {
    "id": "83"
   },
   "source": [
    "## List values of the new columns obtained from receptacle FID parsing for both parsed dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {
    "id": "84"
   },
   "source": [
    "### 1. for parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85",
    "outputId": "ceba95f3-7b9b-4154-91ac-f452e9877c9a"
   },
   "outputs": [],
   "source": [
    "#listing the values\n",
    "print(\"\\n--- Values of origin_country ---\")\n",
    "print(parsed_packages_df['origin_country'].unique())\n",
    "\n",
    "print(\"\\n--- Values of destination_country ---\")\n",
    "print(parsed_packages_df['destination_country'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {
    "id": "86"
   },
   "source": [
    "### 2. for parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87",
    "outputId": "dbd99784-214f-48b2-f8ea-9318c3934f53"
   },
   "outputs": [],
   "source": [
    "#listing the values\n",
    "print(\"\\n--- Values of origin_country ---\")\n",
    "print(parsed_receptacles_df['origin_country'].unique())\n",
    "\n",
    "print(\"\\n--- Values of destination_country ---\")\n",
    "print(parsed_receptacles_df['destination_country'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {
    "id": "88"
   },
   "source": [
    "### Do the intersection of origin_country of both dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89",
    "outputId": "7d90d5e5-e175-460a-ff11-9ca0615fe7a4"
   },
   "outputs": [],
   "source": [
    "#do the intersection of origin_country values in both parsed datasets\n",
    "packages_origin_countries = set(parsed_packages_df['origin_country'].unique())\n",
    "receptacle_origin_countries = set(parsed_receptacles_df['origin_country'].unique())\n",
    "common_origin_countries = packages_origin_countries.intersection(receptacle_origin_countries)\n",
    "print(\"number of common origin_country values in both parsed datasets:\", len(common_origin_countries))\n",
    "print(f\"\\nCommon origin_country values in both paesed datasets: \")\n",
    "print(common_origin_countries)\n",
    "# remaining ones\n",
    "remaining_in_packages = packages_origin_countries - common_origin_countries\n",
    "remaining_in_receptacle = receptacle_origin_countries - common_origin_countries\n",
    "print(f\"Remaining origin_country values only in parsed_packages_df:\")\n",
    "print(remaining_in_packages)\n",
    "print(f\"Remaining origin_country values only in parsed_receptacles_df:\")\n",
    "print(remaining_in_receptacle )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {
    "id": "90"
   },
   "source": [
    "### list the values of both service indicators and country code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {
    "id": "91"
   },
   "source": [
    "### 1. service indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92",
    "outputId": "2f948767-2ae2-42a8-cd0d-5637eea84ed6"
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Values of service_indicator ---\")\n",
    "print(parsed_packages_df['service_indicator'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {
    "id": "93"
   },
   "source": [
    " we can see that there are values that don't follow the standards in the S10-12 format so we need to handle that correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94",
    "outputId": "939c3a83-6657-4c40-d795-d1e3c91c7be7"
   },
   "outputs": [],
   "source": [
    "#transform country_code to uppercase for consistency\n",
    "parsed_packages_df['service_indicator'] = parsed_packages_df['service_indicator'].str.upper()\n",
    "print(\"values of service_indicator after transformation to uppercase:\")\n",
    "print(parsed_packages_df['service_indicator'].unique())\n",
    "print(\"number of unique service indicators after transformation:\", parsed_packages_df['service_indicator'].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {
    "id": "95"
   },
   "source": [
    "### 2. country code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96",
    "outputId": "348e1362-d60d-4d84-a893-aba6db8a8c8a"
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Values of country codes ---\")\n",
    "\n",
    "print(parsed_packages_df['country_code'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {
    "id": "97"
   },
   "source": [
    "we can see that many values for the country codes are numbers instead of ISO 3166-1 format these values should be replaced by the values of origin country gotten from the receptacle when doing the preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {
    "id": "98"
   },
   "source": [
    "### replace them with the correct origin country code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99",
    "outputId": "2d3ca4d1-fce7-4975-b058-25f50471fb3e"
   },
   "outputs": [],
   "source": [
    "# 1. Vectorized string capitalization\n",
    "parsed_packages_df['country_code'] = parsed_packages_df['country_code'].str.upper()\n",
    "\n",
    "# 2. Vectorized comparison to find mismatches\n",
    "mismatch_mask = parsed_packages_df['origin_country'] != parsed_packages_df['country_code']\n",
    "\n",
    "# 3. Count the Trues\n",
    "count = mismatch_mask.sum()\n",
    "\n",
    "print(f\"Number of rows where origin_country does not match country_code: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {
    "id": "100"
   },
   "source": [
    "### replace them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {
    "id": "101"
   },
   "outputs": [],
   "source": [
    "# Use .loc to find rows where they don't match, and update only the 'country_code' column\n",
    "parsed_packages_df.loc[parsed_packages_df['origin_country'] != parsed_packages_df['country_code'], 'country_code'] = parsed_packages_df['origin_country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "102",
    "outputId": "a40afc20-6a42-47c4-c89e-89d555ec16de"
   },
   "outputs": [],
   "source": [
    "#print the unique values again\n",
    "print(\"\\n--- Values of country codes after correction ---\")\n",
    "print(parsed_packages_df['country_code'].unique())\n",
    "print(\"number of unique country codes after correction:\", parsed_packages_df['country_code'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {
    "id": "103"
   },
   "source": [
    "## visualization of Origin Country distribution according to number of packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {
    "id": "104"
   },
   "source": [
    "### 1. for the parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "id": "105",
    "outputId": "f321d841-e497-4aaa-f6e3-61783f87ee7a"
   },
   "outputs": [],
   "source": [
    "origin_country_counts = parsed_packages_df['origin_country'].value_counts().head(20)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.barh(origin_country_counts.index, origin_country_counts.values, color='steelblue')\n",
    "plt.xlabel('Count', fontsize=11)\n",
    "plt.ylabel('Origin Country', fontsize=11)\n",
    "plt.title('Top 20 Origin Countries by packages count', fontsize=12, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {
    "id": "106"
   },
   "source": [
    "### 2. for the parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "id": "107",
    "outputId": "bfd0028e-361c-496e-dd82-8babce4711cb"
   },
   "outputs": [],
   "source": [
    "origin_country_counts = parsed_receptacles_df['origin_country'].value_counts().head(20)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.barh(origin_country_counts.index, origin_country_counts.values, color='steelblue')\n",
    "plt.xlabel('Count', fontsize=11)\n",
    "plt.ylabel('Origin Country', fontsize=11)\n",
    "plt.title('Top 20 Origin Countries by receptacle count', fontsize=12, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {
    "id": "108"
   },
   "source": [
    "## Visualiation of the service indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "id": "109",
    "outputId": "a933f9cf-8ee0-4dea-a653-e99d458c79ad"
   },
   "outputs": [],
   "source": [
    "service_indicator_count = parsed_packages_df['service_indicator'].value_counts().head(20)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.barh(service_indicator_count.index, service_indicator_count.values, color='mediumseagreen')\n",
    "plt.xlabel('Count', fontsize=11)\n",
    "plt.ylabel('Service Indicator', fontsize=11)\n",
    "plt.title('Top 20 Service Indicators by packages count', fontsize=12, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113",
   "metadata": {
    "id": "110"
   },
   "source": [
    "# Origin–Destination Flow Analysis\n",
    "\n",
    "This section investigates the flow of receptacles and packages from origin countries to destination. We examine:\n",
    "- packages count by origin country\n",
    "- Top origin countries delivering to each destination\n",
    "- Visual representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114",
   "metadata": {
    "id": "111"
   },
   "source": [
    "### 1. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "112",
    "outputId": "a46175dc-c904-425f-fbca-14176b79752a"
   },
   "outputs": [],
   "source": [
    "# packages count by origin country\n",
    "origin_country_volume = parsed_packages_df['origin_country'].value_counts()\n",
    "print(\"\\n--- Packages count by Origin Country ---\")\n",
    "print(origin_country_volume.head(15))\n",
    "\n",
    "# Create origin_country × destination matrix\n",
    "flow_matrix = pd.crosstab(parsed_packages_df['origin_country'],\n",
    "                           parsed_packages_df['destination_country'])\n",
    "\n",
    "# Keep only top 10 origin countries and top 10 destination countries for readability\n",
    "top_origins = parsed_packages_df['origin_country'].value_counts().head(10).index\n",
    "top_arrivals = parsed_packages_df['destination_country'].value_counts().head(10).index\n",
    "\n",
    "flow_matrix_top = flow_matrix.loc[top_origins, top_arrivals]\n",
    "\n",
    "print(\"\\n--- Origin Country × destination Country (Top 10 × Top 10) ---\")\n",
    "print(flow_matrix_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "id": "113",
    "outputId": "9717c930-22fc-4f49-a32d-ff7a91f02a84"
   },
   "outputs": [],
   "source": [
    "# Visualize origin × destination country flow\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(flow_matrix_top, cmap='YlOrRd', annot=True, fmt='d',\n",
    "            cbar_kws={'label': 'packages Count'}, linewidths=0.5)\n",
    "plt.title('packages Flow: Origin Country × destination country (Top 10 × Top 10)',\n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Arrival Hub', fontsize=12)\n",
    "plt.ylabel('Origin Country', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117",
   "metadata": {
    "id": "114"
   },
   "source": [
    "### 2. parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "115",
    "outputId": "b065e2e1-21ca-4b2d-b6df-f1a0b8232bd0"
   },
   "outputs": [],
   "source": [
    "# receptacle count by origin country\n",
    "origin_country_receptacle_volume = parsed_receptacles_df['origin_country'].value_counts()\n",
    "print(\"\\n--- Receptacle count by Origin Country ---\")\n",
    "print(origin_country_receptacle_volume.head(15))\n",
    "\n",
    "destination__receptacle_volume=parsed_receptacles_df['destination_country'].value_counts()\n",
    "print(\"\\n--- Receptacle count by destination ---\")\n",
    "print(destination__receptacle_volume.head(15))\n",
    "\n",
    "# Create origin_country × destination matrix\n",
    "receptacle_flow_matrix = pd.crosstab(parsed_receptacles_df['origin_country'],\n",
    "                           parsed_receptacles_df['destination_country'])\n",
    "\n",
    "# Keep only top 10 origin countries and top 10 destination countries for readability\n",
    "receptacle_top_origins = parsed_receptacles_df['origin_country'].value_counts().head(10).index\n",
    "receptacle_top_arrivals = parsed_receptacles_df['destination_country'].value_counts().head(10).index\n",
    "\n",
    "receptacle_flow_matrix_top = receptacle_flow_matrix.loc[receptacle_top_origins, receptacle_top_arrivals]\n",
    "\n",
    "print(\"\\n--- Origin Country × destination Country (Top 10 × Top 10) ---\")\n",
    "print(receptacle_flow_matrix_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "id": "116",
    "outputId": "de0f7be2-b6cc-4e5a-9108-1ef038627e10"
   },
   "outputs": [],
   "source": [
    "# Visualize origin × destination country flow\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(receptacle_flow_matrix_top, cmap='YlOrRd', annot=True, fmt='d',\n",
    "            cbar_kws={'label': 'receptacles Count'}, linewidths=0.5)\n",
    "plt.title('receptacles Flow: Origin Country × destination country (Top 10 × Top 10)',\n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Arrival Hub', fontsize=12)\n",
    "plt.ylabel('Origin Country', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120",
   "metadata": {
    "id": "117"
   },
   "source": [
    "### create pairs (origin, destination) for more detailed analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {
    "id": "118"
   },
   "outputs": [],
   "source": [
    "parsed_packages_df['origin_destination'] = parsed_packages_df['origin_country'] + '_' + parsed_packages_df['destination_country']\n",
    "parsed_receptacles_df['origin_destination'] = parsed_receptacles_df['origin_country'] + '_' + parsed_receptacles_df['destination_country']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122",
   "metadata": {
    "id": "119"
   },
   "source": [
    "listing the obtained values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "120",
    "outputId": "d9e332dc-4f83-402d-9da7-e018d2dad488"
   },
   "outputs": [],
   "source": [
    "print('(origin_destination) pairs obtained for ')\n",
    "print(\"\\nfor parsed_packages_df :\")\n",
    "print(\"\\nNumber of unique values\", parsed_packages_df['origin_destination'].nunique())\n",
    "print(\"\\nValues : \")\n",
    "print(parsed_packages_df['origin_destination'].unique())\n",
    "print(\"\\nfor parsed_receptacles_df :\")\n",
    "print(\"\\nNumber of unique values\", parsed_receptacles_df['origin_destination'].nunique())\n",
    "print(\"\\nValues : \")\n",
    "print(parsed_receptacles_df['origin_destination'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124",
   "metadata": {
    "id": "121"
   },
   "source": [
    "### visualization of obtained results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125",
   "metadata": {
    "id": "122"
   },
   "source": [
    "### 1. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "123",
    "outputId": "47b56a37-7908-4ebf-a556-0ebf88112210"
   },
   "outputs": [],
   "source": [
    "origin_dest_counts = parsed_packages_df['origin_destination'].value_counts()\n",
    "top_origin_dest_counts= origin_dest_counts.head(15)\n",
    "plt.figure(figsize=(14, 7))\n",
    "top_origin_dest_counts.plot(kind='bar')\n",
    "plt.title('Histogram of (origin, destination) Pairs Based on Package Counts')\n",
    "plt.xlabel('Origin_Destination')\n",
    "plt.ylabel('Number of Packages')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {
    "id": "124"
   },
   "source": [
    "\n",
    "### 2. parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "125",
    "outputId": "b6423e2c-a4cf-4726-f683-7bc387ccecd9"
   },
   "outputs": [],
   "source": [
    "origin_dest_counts = parsed_receptacles_df['origin_destination'].value_counts()\n",
    "top_origin_dest_counts = origin_dest_counts.head(15)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "top_origin_dest_counts.plot(kind='bar')\n",
    "plt.title('Histogram of (origin, destination) Pairs Based on receptacle Counts')\n",
    "plt.xlabel('Origin_Destination')\n",
    "plt.ylabel('Number of receptacles')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129",
   "metadata": {
    "id": "126"
   },
   "source": [
    "## origin_destination X etablissments analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130",
   "metadata": {
    "id": "127"
   },
   "source": [
    "## 1. current etablissment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131",
   "metadata": {
    "id": "128"
   },
   "source": [
    "### a. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "id": "129",
    "outputId": "e7f854f0-dc4f-453c-94c6-d368914a9295"
   },
   "outputs": [],
   "source": [
    "# For parsed_packages_df: visualize the histogram of counts by (origin_destination, etablissement_postal) pairs\n",
    "\n",
    "# First, select top N most common (origin_destination, etablissement_postal) pairs for clarity\n",
    "pair_counts_current = parsed_packages_df.groupby(['origin_destination', 'etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_current = pair_counts_current.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_current,\n",
    "    x='count',\n",
    "    y=top_pairs_current.apply(lambda x: f\"{x['origin_destination']} | {x['etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin-Destination, Current Etablissement) Pairs by Package Counts')\n",
    "plt.xlabel('Number of Packages')\n",
    "plt.ylabel('(Origin_Destination | Current Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133",
   "metadata": {
    "id": "130"
   },
   "source": [
    "### b. parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "id": "131",
    "outputId": "6880080e-5cfe-4ab2-9495-8c903d7e6b88"
   },
   "outputs": [],
   "source": [
    "# For parsed_receptacles_dffffff: visualize the histogram of counts by (origin_destination, etablissement_postal) pairs\n",
    "\n",
    "# First, select top N most common (origin_destination, etablissement_postal) pairs for clarity\n",
    "pair_counts_current_receptacle = parsed_receptacles_df.groupby(['origin_destination', 'etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_current_receptacle = pair_counts_current_receptacle.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_current_receptacle,\n",
    "    x='count',\n",
    "    y=top_pairs_current_receptacle.apply(lambda x: f\"{x['origin_destination']} | {x['etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin-Destination, Current Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Origin_Destination | Current Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135",
   "metadata": {
    "id": "132"
   },
   "source": [
    "we can see that the ETAB0002 is dominating and we remark that when the destination is DZ\n",
    "we'll try to confirm that by taking into consideration the destination only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "id": "133",
    "outputId": "eb0e672a-83c2-464b-e2ee-99442c749f6c"
   },
   "outputs": [],
   "source": [
    "# For parsed_receptacles_dffff: visualize the histogram of counts by (destination_country, etablissement_postal) pairs\n",
    "\n",
    "# First, select top N most common (destination_country, etablissement_postal) pairs for clarity\n",
    "pair_counts_dest_receptacle = parsed_receptacles_df.groupby(['destination_country', 'etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_dest_receptacle = pair_counts_dest_receptacle.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_dest_receptacle,\n",
    "    x='count',\n",
    "    y=top_pairs_dest_receptacle.apply(lambda x: f\"{x['destination_country']} | {x['etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Destination Country, Current Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Destination Country | Current Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137",
   "metadata": {
    "id": "134"
   },
   "source": [
    "### This is to test the origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "id": "135",
    "outputId": "0786be9a-0112-4f55-a66a-2a0ceca2b696"
   },
   "outputs": [],
   "source": [
    "# For parsed_receptacles_dffffff: visualize the histogram of counts by (origin_country, etablissement_postal) pairs\n",
    "\n",
    "# First, select top N most common (origin_country, etablissement_postal) pairs for clarity\n",
    "pair_counts_origin_receptacle = parsed_receptacles_df.groupby(['origin_country', 'etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_origin_receptacle = pair_counts_origin_receptacle.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_origin_receptacle,\n",
    "    x='count',\n",
    "    y=top_pairs_origin_receptacle.apply(lambda x: f\"{x['origin_country']} | {x['etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin Country, Current Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Origin Country | Current Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139",
   "metadata": {
    "id": "136"
   },
   "source": [
    "most of the values with ETAB0002 values are european countries in addition to AE and China(CN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140",
   "metadata": {
    "id": "137"
   },
   "source": [
    "## 2. Next etablissement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141",
   "metadata": {
    "id": "138"
   },
   "source": [
    "### a. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "id": "139",
    "outputId": "902a7aaf-9427-4355-ab18-ab2ab9b1b66b"
   },
   "outputs": [],
   "source": [
    "# For parsed_packages_df: visualize the histogram of counts by (origin_destination, next_etablissement_postal)\n",
    "\n",
    "# First, select top N most common (origin_destination, next_etablissement_postal) pairs for clarity\n",
    "pair_counts = parsed_packages_df.groupby(['origin_destination', 'next_etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs = pair_counts.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs,\n",
    "    x='count',\n",
    "    y=top_pairs.apply(lambda x: f\"{x['origin_destination']} | {x['next_etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin-Destination, Next Etablissement) Pairs by Package Counts')\n",
    "plt.xlabel('Number of Packages')\n",
    "plt.ylabel('(Origin_Destination | Next Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143",
   "metadata": {
    "id": "140"
   },
   "source": [
    "### b. parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "id": "141",
    "outputId": "f90019f4-79aa-4fe1-f854-7cac67d97e06"
   },
   "outputs": [],
   "source": [
    "# For parsed_receptacles_dffff: visualize the histogram of counts by (origin_destination, next_etablissement_postal)\n",
    "\n",
    "# First, select top N most common (origin_destination, next_etablissement_postal) pairs for clarity\n",
    "pair_counts_recept = parsed_receptacles_df.groupby(['origin_destination', 'next_etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_recept = pair_counts_recept.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_recept,\n",
    "    x='count',\n",
    "    y=top_pairs_recept.apply(lambda x: f\"{x['origin_destination']} | {x['next_etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin-Destination, Next Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Origin_Destination | Next Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145",
   "metadata": {
    "id": "142"
   },
   "source": [
    "Do for origin and for destination separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "id": "143",
    "outputId": "584413ac-b4ee-4b05-9658-6c3bcb1dd34b"
   },
   "outputs": [],
   "source": [
    "\n",
    "# For parsed_receptacles_df: visualize the histogram of counts by destination_country and next_etablissement_postal\n",
    "\n",
    "# First, select top N most common (destination_country, next_etablissement_postal) pairs for clarity\n",
    "pair_counts_dest = parsed_receptacles_df.groupby(['destination_country', 'next_etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_dest = pair_counts_dest.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_dest,\n",
    "    x='count',\n",
    "    y=top_pairs_dest.apply(lambda x: f\"{x['destination_country']} | {x['next_etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Destination Country, Next Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Destination_Country | Next Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "id": "144",
    "outputId": "e045c836-f51e-4bf8-b4f5-37a4ebb71c81"
   },
   "outputs": [],
   "source": [
    "# For parsed_receptacles_df: visualize the histogram of counts by origin_country and next_etablissement_postal\n",
    "\n",
    "# First, select top N most common (origin_country, next_etablissement_postal) pairs for clarity\n",
    "pair_counts_origin = parsed_receptacles_df.groupby(['origin_country', 'next_etablissement_postal']).size().reset_index(name='count')\n",
    "\n",
    "# Sort and keep only top N pairs (by count)\n",
    "top_n = 20\n",
    "top_pairs_origin = pair_counts_origin.sort_values('count', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=top_pairs_origin,\n",
    "    x='count',\n",
    "    y=top_pairs_origin.apply(lambda x: f\"{x['origin_country']} | {x['next_etablissement_postal']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top (Origin Country, Next Etablissement) Pairs by Receptacle Counts')\n",
    "plt.xlabel('Number of Receptacles')\n",
    "plt.ylabel('(Origin_Country | Next Etablissement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148",
   "metadata": {
    "id": "145"
   },
   "source": [
    "from the analysis we can see that there are some etablissments that get congested forming a sort of loop (ETAB0030, ETAB0002, ETAB0006)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149",
   "metadata": {
    "id": "146"
   },
   "source": [
    "### Time analysis regarding origin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150",
   "metadata": {
    "id": "147"
   },
   "source": [
    "### 1. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "148",
    "outputId": "57c7c9d7-875b-4332-87b4-9cad50e26ff7"
   },
   "outputs": [],
   "source": [
    "# Group the parsed_packages_df by date and origin_country to count packages per month per origin country\n",
    "parsed_packages_df['date'] = pd.to_datetime(parsed_packages_df['date'])\n",
    "\n",
    "# Group by origin_country and resample by month\n",
    "ts_by_origin = parsed_packages_df.groupby(['origin_country', pd.Grouper(key='date', freq='ME')])['MAILITM_FID'].count().reset_index()\n",
    "\n",
    "# Choose top 5 origin countries by total count for plotting\n",
    "top_origin = parsed_packages_df['origin_country'].value_counts().head(5).index\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "for origin in top_origin:\n",
    "    ts = ts_by_origin[ts_by_origin['origin_country'] == origin]\n",
    "    plt.plot(ts['date'], ts['MAILITM_FID'], marker='o', label=origin)\n",
    "\n",
    "plt.title('Monthly Package count by Origin Country')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Packages')\n",
    "plt.legend(title='Origin Country')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152",
   "metadata": {
    "id": "149"
   },
   "source": [
    "### 2. parsed_receptacles_dfffff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "150",
    "outputId": "64a0c746-86d2-4f76-85f6-cd38e72c600d"
   },
   "outputs": [],
   "source": [
    "# Group the parsed_receptacles_df by date and origin_country to count receptacles per month per origin country\n",
    "parsed_receptacles_df['date'] = pd.to_datetime(parsed_receptacles_df['date'])\n",
    "\n",
    "# Group by origin_country and resample by month\n",
    "ts_by_origin_recept = parsed_receptacles_df.groupby(['origin_country', pd.Grouper(key='date', freq='ME')])['RECPTCL_FID'].count().reset_index()\n",
    "\n",
    "# Choose top 5 origin countries by total count for plotting\n",
    "top_origin_recept = parsed_receptacles_df['origin_country'].value_counts().head(5).index\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "for origin in top_origin_recept:\n",
    "    ts = ts_by_origin_recept[ts_by_origin_recept['origin_country'] == origin]\n",
    "    plt.plot(ts['date'], ts['RECPTCL_FID'], marker='o', label=origin)\n",
    "\n",
    "plt.title('Monthly Receptacle count by Origin Country')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Receptacles')\n",
    "plt.legend(title='Origin Country')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154",
   "metadata": {
    "id": "151"
   },
   "source": [
    "### Time analysis by destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "152",
    "outputId": "35607882-33a6-4934-d4c3-3f271b903d66"
   },
   "outputs": [],
   "source": [
    "# Group the parsed_packages_df by date and destination_country to count packages per month per destination country\n",
    "parsed_packages_df['date'] = pd.to_datetime(parsed_packages_df['date'])\n",
    "\n",
    "# Group by destination_country and resample by month\n",
    "ts_by_dest = parsed_packages_df.groupby(['destination_country', pd.Grouper(key='date', freq='ME')])['MAILITM_FID'].count().reset_index()\n",
    "\n",
    "# Choose top 5 destination countries by total count for plotting\n",
    "top_dest = parsed_packages_df['destination_country'].value_counts().head(5).index\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "for dest in top_dest:\n",
    "    ts = ts_by_dest[ts_by_dest['destination_country'] == dest]\n",
    "    plt.plot(ts['date'], ts['MAILITM_FID'], marker='o', label=dest)\n",
    "\n",
    "plt.title('Monthly Package count by Destination Country')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Packages')\n",
    "plt.legend(title='Destination Country')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156",
   "metadata": {
    "id": "153"
   },
   "source": [
    "### 2. parsed_receptacles_dffff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "154",
    "outputId": "9be15bbe-f261-4d89-dd08-df65fd2f8bda"
   },
   "outputs": [],
   "source": [
    "# Group the parsed_receptacles_df by date and destination_country to count receptacles per month per destination country\n",
    "parsed_receptacles_df['date'] = pd.to_datetime(parsed_receptacles_df['date'])\n",
    "\n",
    "# Group by destination_country and resample by month\n",
    "ts_by_dest_recept = parsed_receptacles_df.groupby(['destination_country', pd.Grouper(key='date', freq='ME')])['RECPTCL_FID'].count().reset_index()\n",
    "\n",
    "# Choose top 5 destination countries by total count for plotting\n",
    "top_dest_recept = parsed_receptacles_df['destination_country'].value_counts().head(5).index\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "for dest in top_dest_recept:\n",
    "    ts = ts_by_dest_recept[ts_by_dest_recept['destination_country'] == dest]\n",
    "    plt.plot(ts['date'], ts['RECPTCL_FID'], marker='o', label=dest)\n",
    "\n",
    "plt.title('Monthly Receptacle count by Destination Country')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Receptacles')\n",
    "plt.legend(title='Destination Country')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158",
   "metadata": {
    "id": "155"
   },
   "source": [
    "## Creating additional features\n",
    " 1. 'flow_type' column with values: 'inbound' (to DZ), 'outbound' (from DZ), 'local' (DZ to DZ), otherwise 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "156",
    "outputId": "bd4f697c-ad11-434d-8ef3-86f12de571d7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_flow_type(df):\n",
    "    # Define the conditions\n",
    "    conditions = [\n",
    "        (df['destination_country'] == 'DZ') & (df['origin_country'] == 'DZ'), # local\n",
    "        (df['destination_country'] == 'DZ'),                                # inbound\n",
    "        (df['origin_country'] == 'DZ')                                     # outbound\n",
    "    ]\n",
    "\n",
    "    # Define the results for each condition\n",
    "    choices = ['local', 'inbound', 'outbound']\n",
    "\n",
    "    # Apply logic with 'other' as the default\n",
    "    return np.select(conditions, choices, default='other')\n",
    "\n",
    "# Apply to both DataFrames instantly\n",
    "parsed_packages_df['flow_type'] = get_flow_type(parsed_packages_df)\n",
    "parsed_receptacles_df['flow_type'] = get_flow_type(parsed_receptacles_df)\n",
    "\n",
    "# Print counts\n",
    "print(\"Flow type counts in parsed_packages_df:\\n\", parsed_packages_df['flow_type'].value_counts())\n",
    "print(\"\\nFlow type counts in parsed_receptacles_df:\\n\", parsed_receptacles_df['flow_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160",
   "metadata": {
    "id": "157"
   },
   "source": [
    "we can see that there are some values of flow type with the type \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "158",
    "outputId": "b5754a7e-4e1a-43f5-8a2c-87fbaa4f3bae"
   },
   "outputs": [],
   "source": [
    "# Display the first 10 rows of origin_country and destination_country for flow_type 'other' in parsed_receptacles_df\n",
    "print(parsed_receptacles_df.loc[parsed_receptacles_df['flow_type'] == 'other', ['origin_country', 'destination_country']].head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162",
   "metadata": {
    "id": "159"
   },
   "source": [
    "## Analysis of the relation between the flow_type and the event_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163",
   "metadata": {
    "id": "160"
   },
   "source": [
    "## 1. parsed_packages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "161",
    "outputId": "173fc55e-4039-4cc2-eb71-ba8f40e3e092"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Analyse relation between flow_type and EVENT_TYPE_CD in parsed_packages_df\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "ax = sns.countplot(data=parsed_packages_df, x=\"EVENT_TYPE_CD\", hue=\"flow_type\", palette='Set2')\n",
    "plt.title(\"Distribution of EVENT_TYPE_CD by flow_type in parsed_packages_df\")\n",
    "plt.xlabel(\"EVENT_TYPE_CD\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"Flow Type\")\n",
    "plt.tight_layout()\n",
    "for c in ax.containers:\n",
    "    ax.bar_label(c, label_type='edge', fontsize=8, padding=2)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165",
   "metadata": {
    "id": "162"
   },
   "source": [
    "## 2. parsed_receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "163",
    "outputId": "dfd65fd8-b7da-427b-cd12-4f3586419276"
   },
   "outputs": [],
   "source": [
    "# For each flow_type, list the unique EVENT_TYPE_CD values and the most frequent EVENT_TYPE_CD value\n",
    "for flow in parsed_receptacles_df['flow_type'].unique():\n",
    "    event_types = parsed_receptacles_df.loc[parsed_receptacles_df['flow_type'] == flow, 'EVENT_TYPE_CD'].unique()\n",
    "    most_common_event_type = parsed_receptacles_df.loc[parsed_receptacles_df['flow_type'] == flow, 'EVENT_TYPE_CD'].mode()\n",
    "    print(f\"Flow type: {flow}\")\n",
    "    print(f\"EVENT_TYPE_CD values: {sorted(event_types)}\")\n",
    "    if not most_common_event_type.empty:\n",
    "        print(f\"Most frequent EVENT_TYPE_CD: {most_common_event_type.iloc[0]}\")\n",
    "    else:\n",
    "        print(\"No EVENT_TYPE_CD available\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "164",
    "outputId": "93862530-993b-4674-dc66-c8ca66d81076"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "ax = sns.countplot(data=parsed_receptacles_df, x=\"EVENT_TYPE_CD\", hue=\"flow_type\", palette='Set1')\n",
    "plt.title(\"Distribution of EVENT_TYPE_CD by flow_type in parsed_receptacles_df\")\n",
    "plt.xlabel(\"EVENT_TYPE_CD\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"Flow Type\")\n",
    "plt.tight_layout()\n",
    "\n",
    "for c in ax.containers:\n",
    "    ax.bar_label(c, label_type='edge', fontsize=8, padding=2)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168",
   "metadata": {
    "id": "165"
   },
   "source": [
    "we can see that there are major event types related to the inbound flow type( coming to DZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169",
   "metadata": {
    "id": "166"
   },
   "source": [
    "# Track multiple receptacles just to see the flow of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "167",
    "outputId": "a72ccdba-9259-47ab-e8a3-44c74e05e34d"
   },
   "outputs": [],
   "source": [
    "# Track multiple receptacles: visualize all events for 5 different RECPTCL_FID in parsed_receptacles_df\n",
    "# Pick 5 unique RECPTCL_FID values to demonstrate\n",
    "num_examples = 5\n",
    "example_receptacle_ids = parsed_receptacles_df['RECPTCL_FID'].drop_duplicates().iloc[:num_examples]\n",
    "for rid in example_receptacle_ids:\n",
    "    print(\"\\n--- Events for RECPTCL_FID:\", rid, \"---\")\n",
    "    display(parsed_receptacles_df[parsed_receptacles_df['RECPTCL_FID'] == rid][['RECPTCL_FID', 'date', 'EVENT_TYPE_CD', 'etablissement_postal', 'next_etablissement_postal']].sort_values('date').reset_index(drop=True))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "for i, receptacle_id in enumerate(example_receptacle_ids):\n",
    "    ex_df = parsed_receptacles_df[parsed_receptacles_df['RECPTCL_FID'] == receptacle_id].sort_values('date')\n",
    "    plt.plot(\n",
    "        ex_df['date'],\n",
    "        ex_df['EVENT_TYPE_CD'],\n",
    "        marker='o',\n",
    "        label=f\"RECPTCL_FID: {receptacle_id}\"\n",
    "    )\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(f\"Event Timeline (EVENT_TYPE_CD) for {num_examples} Receptacles\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"EVENT_TYPE_CD\")\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171",
   "metadata": {
    "id": "168"
   },
   "outputs": [],
   "source": [
    "packages_df=parsed_packages_df.copy()\n",
    "receptacles_df=parsed_receptacles_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172",
   "metadata": {
    "id": "169"
   },
   "outputs": [],
   "source": [
    "packages_df['EVENT_TYPE_CD'] = packages_df['EVENT_TYPE_CD'].astype(str)\n",
    "receptacles_df['EVENT_TYPE_CD'] = receptacles_df['EVENT_TYPE_CD'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173",
   "metadata": {
    "id": "170"
   },
   "outputs": [],
   "source": [
    "packages_df['month'] = packages_df['date'].dt.month\n",
    "receptacles_df['month'] = receptacles_df['date'].dt.month\n",
    "packages_df['hour'] = packages_df['date'].dt.hour\n",
    "receptacles_df['hour'] = receptacles_df['date'].dt.hour\n",
    "packages_df['day_of_week'] = packages_df['date'].dt.day_name()\n",
    "receptacles_df['day_of_week'] = receptacles_df['date'].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174",
   "metadata": {
    "id": "1d94b590"
   },
   "outputs": [],
   "source": [
    "clus_packages_df = packages_df.copy()\n",
    "clus_receptacles_df = receptacles_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175",
   "metadata": {
    "id": "171"
   },
   "source": [
    "# **START OF MODELING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176",
   "metadata": {
    "id": "172"
   },
   "source": [
    "- we need to delete the first record of each package to not take into consideration the time it was inside its receptacle since this time would be calculated separatly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177",
   "metadata": {
    "id": "173"
   },
   "outputs": [],
   "source": [
    "def drop_first_pkg_row(df):\n",
    "    # keep='first' marks the first occurrence of each ID as False\n",
    "    # Subsequent occurrences are marked as True\n",
    "    is_not_first_row = df.duplicated(subset=['MAILITM_FID'], keep='first')\n",
    "\n",
    "    # Return the filtered dataframe WITHOUT resetting the index\n",
    "    return df[is_not_first_row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178",
   "metadata": {
    "id": "174"
   },
   "outputs": [],
   "source": [
    "packages_df = drop_first_pkg_row(packages_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "175",
    "outputId": "4f642aae-52a5-4202-fcb2-befab6c62871"
   },
   "outputs": [],
   "source": [
    "packages_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180",
   "metadata": {
    "id": "176"
   },
   "source": [
    "### Adding the target `route_duration`\n",
    "* In the following cell, we're adding the target `route_duration` for each row.\n",
    "* for rows (of the same package/receptacle) having `next_etablissement_postal` different than `etablissement_postal` of their next row, we keep the value of `route_duration` NaN.\n",
    "* otherwise, we get the difference of `date` of the row and its next (of the same package/receptacle) in `hours` and store it in `route_duration`.\n",
    "- **NOTE**: We'll drop the first row of each package before adding the target `route_duration`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181",
   "metadata": {
    "id": "177"
   },
   "outputs": [],
   "source": [
    "def add_target(df, id):\n",
    "\n",
    "    # 1. Sort the entire dataset chronologically\n",
    "    df = df.sort_values(by=[id, 'date'], ascending=True)\n",
    "\n",
    "    # ... ADD THE route_duration TARGET ...\n",
    "\n",
    "    df['next_event_date'] = df.groupby(id)['date'].shift(-1)\n",
    "    df['route_duration'] = (df['next_event_date'] - df['date']).dt.total_seconds() / 3600\n",
    "    # Create logical consistency mask m\n",
    "    df['next_row_etab'] = df.groupby(id)['etablissement_postal'].shift(-1)\n",
    "    valid_route_duration_mask = (df['next_etablissement_postal'] == df['next_row_etab'])\n",
    "\n",
    "    df.loc[~valid_route_duration_mask, 'route_duration'] = np.nan\n",
    "    df = df.drop(columns=['next_event_date', 'next_row_etab']) # to keep only to the original features.\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182",
   "metadata": {
    "id": "178"
   },
   "outputs": [],
   "source": [
    "packages_df = add_target(packages_df, 'MAILITM_FID')\n",
    "receptacles_df = add_target(receptacles_df, 'RECPTCL_FID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183",
   "metadata": {
    "id": "c1efcd7a"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "179",
    "outputId": "cac30557-84a2-46a7-e183-17d003d17966"
   },
   "outputs": [],
   "source": [
    "large_duration_pkg = packages_df[packages_df['route_duration'] > 1000]\n",
    "large_duration_pkg.sort_values('route_duration', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "180",
    "outputId": "ba9ee3e8-f57f-4e5a-ad6a-91cf302c7a9d"
   },
   "outputs": [],
   "source": [
    "large_duration_rcptcl = receptacles_df[receptacles_df['route_duration'] > 1000]\n",
    "large_duration_rcptcl.sort_values('route_duration', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186",
   "metadata": {
    "id": "181"
   },
   "source": [
    "### Handling null values in the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "182",
    "outputId": "59f69c59-bb2d-4c9d-ed7b-7753852b7e6c"
   },
   "outputs": [],
   "source": [
    "packages_df[packages_df['route_duration'].isna()].shape[0] / packages_df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188",
   "metadata": {
    "id": "183"
   },
   "source": [
    "* 75% of the rows of `packages_df` have null target, these will be dropped since target values needs to be **correct** and not imputed or guessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "184",
    "outputId": "d3190e3c-1fae-4eec-be56-cae7aebaa175"
   },
   "outputs": [],
   "source": [
    "packages_df = packages_df[~packages_df['route_duration'].isna()]\n",
    "packages_df.shape, packages_df['next_etablissement_postal'].isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190",
   "metadata": {
    "id": "185"
   },
   "source": [
    "* For `receptacles_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "186",
    "outputId": "a7807065-3126-4e8b-a236-f3cd79e200ae"
   },
   "outputs": [],
   "source": [
    "receptacles_df[receptacles_df['route_duration'].isna()].shape[0] / receptacles_df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192",
   "metadata": {
    "id": "187"
   },
   "source": [
    "* 77% of the rows of `receptacles_df` have null target, these will be dropped since target values needs to be **correct** and not imputed or guessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "188",
    "outputId": "410a0909-114e-494c-bebb-b0fc0b75fe34"
   },
   "outputs": [],
   "source": [
    "receptacles_df = receptacles_df[~receptacles_df['route_duration'].isna()]\n",
    "receptacles_df.shape, receptacles_df['next_etablissement_postal'].isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194",
   "metadata": {
    "id": "189"
   },
   "source": [
    "### Before splitting We add :\n",
    "  - `packages_per_receptacle`: number of packages per receptacle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195",
   "metadata": {
    "id": "190"
   },
   "outputs": [],
   "source": [
    "def add_packages_per_receptacle(receptacles_df, packages_df):\n",
    "    # 1. Calculate counts directly from packages\n",
    "    package_counts = packages_df.groupby('RECPTCL_FID')['MAILITM_FID'].count().reset_index()\n",
    "    package_counts.columns = ['RECPTCL_FID', 'packages_per_receptacle']\n",
    "\n",
    "    # 2. Merge once onto the receptacles dataframe\n",
    "    receptacles_df = pd.merge(receptacles_df, package_counts, on='RECPTCL_FID', how='left')\n",
    "\n",
    "    # 3. Fill empty receptacles with 0\n",
    "    receptacles_df['packages_per_receptacle'] = receptacles_df['packages_per_receptacle'].fillna(0)\n",
    "\n",
    "    return receptacles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196",
   "metadata": {
    "id": "191"
   },
   "outputs": [],
   "source": [
    "receptacles_df = add_packages_per_receptacle(receptacles_df, packages_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197",
   "metadata": {
    "id": "192"
   },
   "source": [
    "# Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198",
   "metadata": {
    "id": "193"
   },
   "source": [
    "- Since the data is dependent with time we need to take that into consideration and split into past and future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199",
   "metadata": {
    "id": "194"
   },
   "outputs": [],
   "source": [
    "def my_train_test_split(df):\n",
    "    df = df.sort_values('date')\n",
    "\n",
    "    split_idx = int(len(df) * 0.8)\n",
    "\n",
    "    train_df = df.iloc[:split_idx]\n",
    "    test_df = df.iloc[split_idx:]\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200",
   "metadata": {
    "id": "195"
   },
   "source": [
    "* Splitting `packages` and dataset into training and testing sets using `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "196",
    "outputId": "b2105ea9-583c-4d2d-e585-ee72e5cd402e"
   },
   "outputs": [],
   "source": [
    "\n",
    "pkg_X_train, pkg_X_test= my_train_test_split(packages_df)\n",
    "print (f\"Training set size: {pkg_X_train.shape[0]} rows\")\n",
    "print (f\"Testing set size: {pkg_X_test.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202",
   "metadata": {
    "id": "197"
   },
   "source": [
    "* Splitting `receptacles` and dataset into training and testing sets using `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "198",
    "outputId": "7bbacc69-1760-4085-fda7-fe255ec6277d"
   },
   "outputs": [],
   "source": [
    "rcp_X_train, rcp_X_test= my_train_test_split(receptacles_df)\n",
    "print (f\"Training set size: {rcp_X_train.shape[0]} rows\")\n",
    "print (f\"Testing set size: {rcp_X_test.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204",
   "metadata": {
    "id": "199"
   },
   "source": [
    "# Adding New features\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205",
   "metadata": {
    "id": "200"
   },
   "source": [
    "### Dynamic Route load and Etablissement load\n",
    "* We'll add two features `route_load_1h` and `ETAB_load_1h` which for each row, calculates the number of packages/receptacles passing by a specific route (ETAB_XXXX -> ETAB_YYYY) and the number of packages/receptacles at a specific ETAB_XXXX for the previous 1h, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206",
   "metadata": {
    "id": "201"
   },
   "outputs": [],
   "source": [
    "def calculate_etab_load_1h(df,id_col):\n",
    "    # 1. Store the original order/index\n",
    "    df = df.copy()\n",
    "    df['original_index'] = df.index\n",
    "\n",
    "    # 2. Sort by etab and date for the rolling calculation\n",
    "    # We keep the unique original_index to map values back correctly\n",
    "    df_sorted = df.sort_values(['etablissement_postal', 'date'])\n",
    "\n",
    "    # 3. Calculate rolling count\n",
    "    # We use 'date' as the window, but we keep it in the index alongside the original_index\n",
    "    rolling_series = (\n",
    "        df_sorted.set_index('date')\n",
    "        .groupby('etablissement_postal')[id_col]\n",
    "        .rolling('1h', closed='left')\n",
    "        .count()\n",
    "    )\n",
    "\n",
    "    # 4. Map it back safely\n",
    "    # We reset the index of rolling_series to get a flat dataframe\n",
    "    # Then we align it back to the original dataframe\n",
    "    rolling_df = rolling_series.reset_index()\n",
    "\n",
    "    # Since rolling and groupby can reorder rows, we merge on\n",
    "    # the specific columns to ensure every package gets its correct count\n",
    "    # To handle duplicates, we add a temporary 'sequence' within each millisecond\n",
    "    df_sorted['temp_seq'] = df_sorted.groupby(['etablissement_postal', 'date']).cumcount()\n",
    "    rolling_df['temp_seq'] = rolling_df.groupby(['etablissement_postal', 'date']).cumcount()\n",
    "\n",
    "    df_final = pd.merge(\n",
    "        df_sorted,\n",
    "        rolling_df.rename(columns={id_col: 'etab_load_1h'}),\n",
    "        on=['etablissement_postal', 'date', 'temp_seq'],\n",
    "        how='left'\n",
    "    )\n",
    "    df_final['etab_load_1h'] = df_final['etab_load_1h'].fillna(0)\n",
    "\n",
    "    return df_final.drop(columns=['temp_seq', 'original_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207",
   "metadata": {
    "id": "202"
   },
   "outputs": [],
   "source": [
    "def calculate_route_load_1h(df,id_col):\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1. Sort by route and date\n",
    "    # We use next_etablissement_postal which you've already filled\n",
    "    df_sorted = df.sort_values(['etablissement_postal', 'next_etablissement_postal', 'date'])\n",
    "\n",
    "    # 2. Calculate rolling count for the specific LANE\n",
    "    rolling_series = (\n",
    "        df_sorted.set_index('date')\n",
    "        .groupby(['etablissement_postal', 'next_etablissement_postal'])[id_col]\n",
    "        .rolling('1h', closed='left')\n",
    "        .count()\n",
    "    )\n",
    "\n",
    "    # 3. Flatten and prepare for merge\n",
    "    rolling_df = rolling_series.reset_index()\n",
    "\n",
    "    # Handle duplicates with sequence counts\n",
    "    df_sorted['temp_seq'] = df_sorted.groupby(['etablissement_postal', 'next_etablissement_postal', 'date']).cumcount()\n",
    "    rolling_df['temp_seq'] = rolling_df.groupby(['etablissement_postal', 'next_etablissement_postal', 'date']).cumcount()\n",
    "\n",
    "    # 4. Merge back\n",
    "    df_final = pd.merge(\n",
    "        df_sorted,\n",
    "        rolling_df.rename(columns={id_col: 'route_load_1h'}),\n",
    "        on=['etablissement_postal', 'next_etablissement_postal', 'date', 'temp_seq'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Fill NaNs with 0 (No other packages on this lane in the last 1h)\n",
    "    df_final['route_load_1h'] = df_final['route_load_1h'].fillna(0)\n",
    "\n",
    "    return df_final.drop(columns=['temp_seq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208",
   "metadata": {
    "id": "203"
   },
   "source": [
    "### Time since first scan\n",
    "- The difference between the time of the actual scan with the first scan this will be usefull when giving the total duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209",
   "metadata": {
    "id": "204"
   },
   "outputs": [],
   "source": [
    "def add_time_since_first_scan(df, id_col):\n",
    "    # Ensure we are looking at the same entity in the right order\n",
    "    df = df.sort_values([id_col, 'date'])\n",
    "\n",
    "    # 1. TIME SINCE FIRST SCAN\n",
    "    # Subtract the minimum date of the group from the current date\n",
    "    df['time_since_first_scan'] = df.groupby(id_col)['date'].transform(\n",
    "        lambda x: (x - x.min()).dt.total_seconds() / 3600\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210",
   "metadata": {},
   "source": [
    "- The difference of time between the actual scan and its previous one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211",
   "metadata": {
    "id": "205"
   },
   "outputs": [],
   "source": [
    "def add_time_since_last_scan(df,id_col):\n",
    "    # Ensure we are looking at the same package in the right order\n",
    "    df = df.sort_values([id_col, 'date'])\n",
    "\n",
    "    # 1. TIME SINCE LAST SCAN\n",
    "    # This is NOT cumulative.\n",
    "    df['time_since_last_scan'] = df.groupby(id_col)['date'].diff().dt.total_seconds() / 3600\n",
    "    df['time_since_last_scan'] = df['time_since_last_scan'].fillna(0)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212",
   "metadata": {
    "id": "206"
   },
   "outputs": [],
   "source": [
    "for df in [pkg_X_train, pkg_X_test]:\n",
    "    # 4 = Friday, 5 = Saturday\n",
    "    df['is_weekend'] = df['date'].dt.dayofweek.isin([4, 5]).astype(int)\n",
    "    df['first_last_week_day']= df['date'].dt.dayofweek.isin([3,6]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213",
   "metadata": {},
   "source": [
    "- This adds more meaning to the service_indicator to make a relation with the origin country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214",
   "metadata": {
    "id": "207"
   },
   "outputs": [],
   "source": [
    "pkg_X_train['country_service'] = pkg_X_train['origin_country'].astype(str) + \"_\" + pkg_X_train['service_indicator'].astype(str)\n",
    "pkg_X_test['country_service'] = pkg_X_test['origin_country'].astype(str) + \"_\" + pkg_X_test['service_indicator'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215",
   "metadata": {
    "id": "208"
   },
   "source": [
    "- Adding features that represents events that could affect the flow of packages in algeria ``Note that this is only for packages because they are the ones that are related to Algeria``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216",
   "metadata": {
    "id": "209"
   },
   "outputs": [],
   "source": [
    "import holidays\n",
    "\n",
    "# 1. Setup Algeria Holidays (Keep this outside the function for speed)\n",
    "al_holidays = holidays.Algeria(years=[2023, 2024, 2025, 2026])\n",
    "holiday_dates = sorted(al_holidays.keys())\n",
    "holiday_df = pd.DataFrame({'holiday_date': pd.to_datetime(holiday_dates)})\n",
    "\n",
    "def add_holidays_features(df):\n",
    "    # Ensure Date is datetime and SORTED (merge_asof requires sorting)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').copy() # .copy() avoids SettingWithCopy warnings\n",
    "\n",
    "    # 3. Calculate \"Days Since Last Holiday\"\n",
    "    # direction='backward' looks for the last holiday <= current date\n",
    "    df = pd.merge_asof(df, holiday_df, left_on='date', right_on='holiday_date', direction='backward')\n",
    "    df['days_since_last_holiday'] = (df['date'] - df['holiday_date']).dt.days\n",
    "    df = df.drop(columns=['holiday_date']) # Drop it so the next merge doesn't conflict\n",
    "\n",
    "    # 4. Calculate \"Days Until Next Holiday\"\n",
    "    # direction='forward' looks for the next holiday >= current date\n",
    "    df = pd.merge_asof(df, holiday_df, left_on='date', right_on='holiday_date', direction='forward')\n",
    "    df['days_until_next_holiday'] = (df['holiday_date'] - df['date']).dt.days\n",
    "    df = df.drop(columns=['holiday_date'])\n",
    "\n",
    "    # 5. Clean up and Cap\n",
    "    df[['days_since_last_holiday', 'days_until_next_holiday']] = df[['days_since_last_holiday', 'days_until_next_holiday']].fillna(30)\n",
    "\n",
    "    df['days_since_last_holiday'] = df['days_since_last_holiday'].clip(upper=30)\n",
    "    df['days_until_next_holiday'] = df['days_until_next_holiday'].clip(upper=30)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217",
   "metadata": {
    "id": "210"
   },
   "outputs": [],
   "source": [
    "pkg_X_train = add_holidays_features(pkg_X_train)\n",
    "pkg_X_test = add_holidays_features(pkg_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218",
   "metadata": {
    "id": "211"
   },
   "source": [
    "- This will be tested later to see whether we can add it to receptacles or not(additional information are needed to be applied since we don't know if the receptacle is still at the origin country or at the destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219",
   "metadata": {
    "id": "212"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import holidays\n",
    "\n",
    "# # 1. Pre-generate holidays for all required countries\n",
    "# # we can use the Countries_codes given in order to create the list of supported countries for holidays\n",
    "# SUPPORTED_COUNTRIES = [TO BE FILLED]\n",
    "# all_holidays = []\n",
    "\n",
    "# for country in SUPPORTED_COUNTRIES:\n",
    "#     # Get holidays for each country\n",
    "#     country_hols = holidays.CountryHoliday(country, years=[2023, 2024, 2025, 2026])\n",
    "#     for date, name in country_hols.items():\n",
    "#         all_holidays.append({'holiday_date': pd.to_datetime(date), 'country_code': country})\n",
    "\n",
    "# # Create the master holiday DataFrame\n",
    "# holiday_df = pd.DataFrame(all_holidays).sort_values('holiday_date')\n",
    "\n",
    "# def add_holidays_features(df, country_col='country_name'):\n",
    "#     \"\"\"\n",
    "#     df: input dataframe\n",
    "#     country_col: the column containing the country name (must match holidays package names)\n",
    "#     \"\"\"\n",
    "#     df['date'] = pd.to_datetime(df['date'])\n",
    "#     df = df.sort_values('date').copy()\n",
    "\n",
    "#     # We will process each country group separately and concat them back\n",
    "#     processed_groups = []\n",
    "\n",
    "#     for country, group in df.groupby(country_col):\n",
    "#         # Filter holiday_df for just this country\n",
    "#         country_hols = holiday_df[holiday_df['country_code'] == country]\n",
    "\n",
    "#         if country_hols.empty:\n",
    "#             # If country not in holidays, set defaults immediately\n",
    "#             group['days_since_last_holiday'] = 30\n",
    "#             group['days_until_next_holiday'] = 30\n",
    "#         else:\n",
    "#             # Days Since Last Holiday\n",
    "#             group = pd.merge_asof(group, country_hols, left_on='date', right_on='holiday_date', direction='backward')\n",
    "#             group['days_since_last_holiday'] = (group['date'] - group['holiday_date']).dt.days\n",
    "#             group = group.drop(columns=['holiday_date', 'country_code'])\n",
    "\n",
    "#             # Days Until Next Holiday\n",
    "#             group = pd.merge_asof(group, country_hols, left_on='date', right_on='holiday_date', direction='forward')\n",
    "#             group['days_until_next_holiday'] = (group['holiday_date'] - group['date']).dt.days\n",
    "#             group = group.drop(columns=['holiday_date', 'country_code'])\n",
    "\n",
    "#         processed_groups.append(group)\n",
    "\n",
    "#     # Combine back into one dataframe\n",
    "#     df_final = pd.concat(processed_groups)\n",
    "\n",
    "#     # 5. Clean up and Cap (same as your original logic)\n",
    "#     cols = ['days_since_last_holiday', 'days_until_next_holiday']\n",
    "#     df_final[cols] = df_final[cols].fillna(30).clip(upper=30)\n",
    "\n",
    "#     return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220",
   "metadata": {
    "id": "213"
   },
   "source": [
    "### Creating the final function to create all the shared features at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221",
   "metadata": {
    "id": "214"
   },
   "outputs": [],
   "source": [
    "def create_all_shared_features(df, id_col):\n",
    "    df = calculate_etab_load_1h(df, id_col)\n",
    "    df = calculate_route_load_1h(df, id_col)\n",
    "    df = add_time_since_first_scan(df, id_col)\n",
    "    df = add_time_since_last_scan(df, id_col)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222",
   "metadata": {
    "id": "215"
   },
   "outputs": [],
   "source": [
    "pkg_X_train = create_all_shared_features(pkg_X_train, 'MAILITM_FID')\n",
    "pkg_X_test = create_all_shared_features(pkg_X_test, 'MAILITM_FID')\n",
    "rcp_X_train = create_all_shared_features(rcp_X_train, 'RECPTCL_FID')\n",
    "rcp_X_test = create_all_shared_features(rcp_X_test, 'RECPTCL_FID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223",
   "metadata": {
    "id": "216"
   },
   "source": [
    "- check for features creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "217",
    "outputId": "6f2a5bd4-0bd2-408b-b136-7e1ba3e2f26f"
   },
   "outputs": [],
   "source": [
    "pkg_X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "218",
    "outputId": "14965bc5-4d0a-4d0a-dc03-9092c321bb20"
   },
   "outputs": [],
   "source": [
    "rcp_X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226",
   "metadata": {
    "id": "219"
   },
   "source": [
    "# Use CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227",
   "metadata": {
    "id": "220"
   },
   "source": [
    " ## 1. `For packages`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228",
   "metadata": {
    "id": "221"
   },
   "source": [
    "### Use Catboost with Huber loss function\n",
    "- Huber with delta=20 hours act as MAE for the ones with less than 20 hours and act as RMSE for the ones greater than delta.\n",
    "- This will solve the obssession of RMSE toward predicting the outliers(large values)\n",
    "- parameters are set manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229",
   "metadata": {
    "id": "222"
   },
   "source": [
    "- This could be run for testing since the hyperparameter tuning would take time(note that the parameters used are the ones given by the hyperparameters tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-gq_Fn4hs7oE",
    "outputId": "81cea3c7-e08d-4fac-b4e0-3e5b42a43776"
   },
   "outputs": [],
   "source": [
    "# !pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "id": "223",
    "outputId": "f6afd694-10b6-40dd-df1a-9f900d978fb1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from catboost import CatBoostRegressor, Pool, cv\n",
    "\n",
    "# 1. Ensure chronological order\n",
    "pkg_X_train = pkg_X_train.sort_values('date').reset_index(drop=True)\n",
    "pkg_X_test = pkg_X_test.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# --- Define Model Parameters and Features ---\n",
    "TARGET_COL = 'route_duration'\n",
    "\n",
    "cat_features = [\n",
    "    'etablissement_postal',\n",
    "    'next_etablissement_postal',\n",
    "    'day_of_week',\n",
    "    'service_indicator',\n",
    "    'origin_destination',\n",
    "    'EVENT_TYPE_CD',\n",
    "    'hour',\n",
    "    'month',\n",
    "    'is_weekend',\n",
    "    'first_last_week_day',\n",
    "    'country_service'\n",
    "]\n",
    "\n",
    "# Columns to drop\n",
    "drop_cols = [TARGET_COL, 'MAILITM_FID', 'RECPTCL_FID', 'date', 'serial_number',\n",
    "             'flow_type', 'country_code', 'destination_country', 'origin_country']\n",
    "\n",
    "# Prepare the full 80% training set\n",
    "X_train_full = pkg_X_train.drop(columns=[c for c in drop_cols if c in pkg_X_train.columns])\n",
    "y_train_full = pkg_X_train[TARGET_COL]\n",
    "\n",
    "# Identify categorical features by name\n",
    "categorical_feature_names = [c for c in cat_features if c in X_train_full.columns]\n",
    "\n",
    "# --- TimeSeries Cross-Validation to Find Optimal Iterations ---\n",
    "\n",
    "# Create the training Pool\n",
    "train_pool = Pool(\n",
    "    data=X_train_full,\n",
    "    label=y_train_full,\n",
    "    cat_features=categorical_feature_names,\n",
    ")\n",
    "\n",
    "cv_params = {\n",
    "    'loss_function': 'Huber:delta=20.0',          #Hurber(acts as MAE for normal ones but acts as RMSE for outliers)\n",
    "    'eval_metric': 'MAE',\n",
    "    'iterations': 1500,\n",
    "    'learning_rate': 0.014927636750123532,\n",
    "    'depth': 8,\n",
    "    'random_seed': 42,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 8.27674007602611,\n",
    "}\n",
    "\n",
    "print(\"Starting TimeSeries Cross-Validation (5 Folds) optimizing for MAE...\")\n",
    "\n",
    "cv_results = cv(\n",
    "    params=cv_params,\n",
    "    pool=train_pool,\n",
    "    fold_count=5,\n",
    "    shuffle=False,\n",
    "    type='TimeSeries',\n",
    ")\n",
    "\n",
    "# Find the best iteration based on the minimum average MAE\n",
    "# Note: cv_results keys change based on the loss_function used\n",
    "best_iter = cv_results['test-MAE-mean'].values.argmin()\n",
    "best_mae = cv_results['test-MAE-mean'].min()\n",
    "print(f\"Optimal number of CatBoost iterations: {best_iter + 1} (Best CV MAE: {best_mae:.4f})\")\n",
    "\n",
    "# --- Train Final Model on ENTIRE Training Set ---\n",
    "\n",
    "final_model = CatBoostRegressor(\n",
    "    iterations=best_iter + 1,\n",
    "    learning_rate=0.014927636750123532,\n",
    "    depth=8,\n",
    "    loss_function='Huber:delta=20.0',\n",
    "    eval_metric='MAE',\n",
    "    random_seed=42,\n",
    "    cat_features=categorical_feature_names,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "print(\"\\nTraining final model on the entire 80% training set...\")\n",
    "final_model.fit(train_pool)\n",
    "#saving the model\n",
    "final_model.save_model(\"../models/final_pkg_route_duration_model.cbm\")\n",
    "print(\"Model trained and saved as 'final_pkg_route_duration_model.cbm'\")\n",
    "# --- Predict and Evaluate on the Future Test Set ---\n",
    "\n",
    "X_test_features = pkg_X_test.drop(columns=[c for c in drop_cols if c in pkg_X_test.columns])\n",
    "y_test_target = pkg_X_test[TARGET_COL]\n",
    "\n",
    "predictions = final_model.predict(X_test_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232",
   "metadata": {
    "id": "224"
   },
   "source": [
    "### Using Optuna for hyperparameters tuning before passing them to CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233",
   "metadata": {
    "id": "225"
   },
   "source": [
    "- Preparing the data pool\n",
    "- Uncomment to run (would taake time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234",
   "metadata": {
    "id": "226"
   },
   "outputs": [],
   "source": [
    "# from catboost import Pool\n",
    "\n",
    "# # ---  Data Preparation ---\n",
    "# pkg_X_train = pkg_X_train.sort_values('date').reset_index(drop=True)\n",
    "# pkg_X_test = pkg_X_test.sort_values('date').reset_index(drop=True)\n",
    "# TARGET_COL = 'route_duration'\n",
    "# cat_features = [\n",
    "#     'etablissement_postal', 'next_etablissement_postal', 'day_of_week',\n",
    "#     'service_indicator', 'origin_destination', 'EVENT_TYPE_CD', 'hour', 'month',\n",
    "#     'is_weekend', 'first_last_week_day', 'service_country'\n",
    "# ]\n",
    "# drop_cols = [TARGET_COL, 'MAILITM_FID', 'RECPTCL_FID', 'date', 'serial_number',\n",
    "#              'flow_type', 'country_code', 'destination_country', 'origin_country']\n",
    "\n",
    "# X_train_full = pkg_X_train.drop(columns=[c for c in drop_cols if c in pkg_X_train.columns])\n",
    "# y_train_full = pkg_X_train[TARGET_COL]\n",
    "# categorical_feature_names = [c for c in cat_features if c in X_train_full.columns]\n",
    "\n",
    "# train_pool = Pool(data=X_train_full, label=y_train_full, cat_features=categorical_feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235",
   "metadata": {
    "id": "227"
   },
   "source": [
    "- This will find the best parameters and saves them into a json file so they can be used later without the need to do the tuning again unless changes are made in the data or the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236",
   "metadata": {
    "id": "UeIvIUawtB_9"
   },
   "outputs": [],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237",
   "metadata": {
    "id": "228"
   },
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import json\n",
    "# from catboost import cv\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         'loss_function': 'Huber:delta=20.0',\n",
    "#         'eval_metric': 'MAE',\n",
    "#         'custom_metric': ['RMSE'], # Ensure RMSE is calculated\n",
    "#         'random_seed': 42,\n",
    "#         'verbose': 0,\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "#         'depth': trial.suggest_int('depth', 4, 8),\n",
    "#         'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "#         'iterations': 1000,\n",
    "#         'early_stopping_rounds': 30\n",
    "#     }\n",
    "\n",
    "#     cv_results = cv(\n",
    "#         params=params,\n",
    "#         pool=train_pool,\n",
    "#         fold_count=3,\n",
    "#         shuffle=False,\n",
    "#         type='TimeSeries',\n",
    "#         plot=False\n",
    "#     )\n",
    "\n",
    "#     # Track both metrics\n",
    "#     best_mae = cv_results['test-MAE-mean'].min()\n",
    "#     best_rmse = cv_results['test-RMSE-mean'].min()\n",
    "\n",
    "#     return best_mae, best_rmse\n",
    "\n",
    "# # --- 2. Run Multi-Objective Optimization ---\n",
    "# print(\"Starting Hyperparameter Tuning (Tracking MAE & RMSE)...\")\n",
    "# study = optuna.create_study(directions=['minimize', 'minimize'])\n",
    "# study.optimize(objective, n_trials=20)\n",
    "\n",
    "# # --- 3. Pick the best trial and find optimal iterations ---\n",
    "# # We pick the trial with the lowest MAE\n",
    "# best_trial = min(study.best_trials, key=lambda t: t.values[0])\n",
    "\n",
    "# print(f\"\\nBest Trial selected (MAE: {best_trial.values[0]:.4f}, RMSE: {best_trial.values[1]:.4f})\")\n",
    "\n",
    "# # Find best iterations for the winning set on 5 folds for stability\n",
    "# final_params_temp = {\n",
    "#     'loss_function': 'Huber:delta=20.0',\n",
    "#     'eval_metric': 'MAE',\n",
    "#     **best_trial.params\n",
    "# }\n",
    "\n",
    "# print(\"Calculating final optimal iterations...\")\n",
    "# final_cv = cv(params=final_params_temp, pool=train_pool, fold_count=5, shuffle=False, type='TimeSeries', verbose=0)\n",
    "# best_iteration = int(final_cv['test-MAE-mean'].values.argmin() + 1)\n",
    "\n",
    "# # --- 4. Save to JSON ---\n",
    "# best_config = {\n",
    "#     **best_trial.params,\n",
    "#     \"iterations\": best_iteration,\n",
    "#     \"loss_function\": 'Huber:delta=20.0',\n",
    "#     \"eval_metric\": 'MAE',\n",
    "#     \"final_mae\": best_trial.values[0],\n",
    "#     \"final_rmse\": best_trial.values[1]\n",
    "# }\n",
    "\n",
    "# with open(\"catboost_pkg_best_params.json\", \"w\") as f:\n",
    "#     json.dump(best_config, f, indent=4)\n",
    "\n",
    "# print(f\"Tuning complete. Best parameters saved to 'catboost_pkg_best_params.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238",
   "metadata": {
    "id": "229"
   },
   "source": [
    "- Load the parameters from the json file and use them for the final training and then test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239",
   "metadata": {
    "id": "230"
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# from catboost import CatBoostRegressor\n",
    "\n",
    "# # --- 1. Load Parameters ---\n",
    "# with open(\"catboost_pkg_best_params.json\", \"r\") as f:\n",
    "#     best_config = json.load(f)\n",
    "\n",
    "# # Extract iterations and remove metrics/metadata so CatBoost doesn't crash\n",
    "# iters = best_config.pop(\"iterations\")\n",
    "# best_config.pop(\"final_mae\", None)   # Remove if present\n",
    "# best_config.pop(\"final_rmse\", None)  # Remove if present\n",
    "\n",
    "# # --- 2. Initialize and Train Model ---\n",
    "# # Ensure categorical_feature_names and train_pool are defined in your environment\n",
    "# final_model = CatBoostRegressor(\n",
    "#     iterations=iters,\n",
    "#     **best_config,\n",
    "#     cat_features=categorical_feature_names,\n",
    "#     random_seed=42,\n",
    "#     verbose=100\n",
    "# )\n",
    "\n",
    "# print(\"Training final model with saved parameters...\")\n",
    "# final_model.fit(train_pool)\n",
    "\n",
    "# # --- 3. Save the actual Model weights ---\n",
    "# final_model.save_model(\"final_pkg_route_duration_model.cbm\")\n",
    "# print(\"Model trained and saved as 'final_pkg_route_duration_model.cbm'\")\n",
    "\n",
    "# # --- 4. Predict on Test Set ---\n",
    "# # Ensure pkg_X_test and drop_cols are defined in your environment\n",
    "# X_test_features = pkg_X_test.drop(columns=[c for c in drop_cols if c in pkg_X_test.columns])\n",
    "# predictions = final_model.predict(X_test_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240",
   "metadata": {
    "id": "231"
   },
   "source": [
    " ## 2. `For receptacles`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241",
   "metadata": {
    "id": "232"
   },
   "source": [
    "### Use Catboost with Huber loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242",
   "metadata": {
    "id": "233"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from catboost import CatBoostRegressor, Pool, cv\n",
    "\n",
    "# 1. Ensure chronological order\n",
    "rcp_X_train = rcp_X_train.sort_values('date').reset_index(drop=True)\n",
    "rcp_X_test = rcp_X_test.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# --- Define Model Parameters and Features ---\n",
    "TARGET_COL = 'route_duration'\n",
    "\n",
    "cat_features = [\n",
    "    'etablissement_postal',\n",
    "    'next_etablissement_postal',\n",
    "    'day_of_week',\n",
    "    'origin_destination',\n",
    "    'EVENT_TYPE_CD',\n",
    "    'hour',\n",
    "    'month',\n",
    "    'flow_type',\n",
    "]\n",
    "\n",
    "# Columns to drop\n",
    "drop_cols = [TARGET_COL, 'RECPTCL_FID', 'date', 'destination_country', 'origin_country']\n",
    "\n",
    "# Prepare the full 80% training set\n",
    "X_train_full = rcp_X_train.drop(columns=[c for c in drop_cols if c in rcp_X_train.columns])\n",
    "y_train_full = rcp_X_train[TARGET_COL]\n",
    "\n",
    "# Identify categorical features by name\n",
    "categorical_feature_names = [c for c in cat_features if c in X_train_full.columns]\n",
    "\n",
    "# --- TimeSeries Cross-Validation to Find Optimal Iterations ---\n",
    "\n",
    "# Create the training Pool\n",
    "train_pool = Pool(\n",
    "    data=X_train_full,\n",
    "    label=y_train_full,\n",
    "    cat_features=categorical_feature_names,\n",
    ")\n",
    "\n",
    "cv_params = {\n",
    "    'loss_function': 'Huber:delta=20.0',          #  Hurber(acts as MAE for normal ones but acts as RMSE for outliers)\n",
    "    'eval_metric': 'MAE',\n",
    "    'iterations': 1500,\n",
    "    'learning_rate': 0.08188269603595484,\n",
    "    'depth': 5,\n",
    "    'random_seed': 42,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 5.899785245396042,          # Standard regularization\n",
    "}\n",
    "\n",
    "print(\"Starting TimeSeries Cross-Validation (5 Folds) optimizing for MAE...\")\n",
    "\n",
    "cv_results = cv(\n",
    "    params=cv_params,\n",
    "    pool=train_pool,\n",
    "    fold_count=5,\n",
    "    shuffle=False,\n",
    "    type='TimeSeries',\n",
    ")\n",
    "\n",
    "# Find the best iteration based on the minimum average MAE\n",
    "# Note: cv_results keys change based on the loss_function used\n",
    "best_iter = cv_results['test-MAE-mean'].values.argmin()\n",
    "best_mae = cv_results['test-MAE-mean'].min()\n",
    "print(f\"Optimal number of CatBoost iterations: {best_iter + 1} (Best CV MAE: {best_mae:.4f})\")\n",
    "\n",
    "# --- Train Final Model on ENTIRE Training Set ---\n",
    "\n",
    "final_model = CatBoostRegressor(\n",
    "    iterations=best_iter + 1,\n",
    "    learning_rate=0.08188269603595484,\n",
    "    depth=5,\n",
    "    loss_function='Huber:delta=20.0',\n",
    "    eval_metric='MAE',\n",
    "    random_seed=42,\n",
    "    cat_features=categorical_feature_names,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "print(\"\\nTraining final model on the entire 80% training set...\")\n",
    "final_model.fit(train_pool)\n",
    "# --- 3. Save the actual Model weights ---\n",
    "final_model.save_model(\"final_rcp_route_duration_model.cbm\")\n",
    "print(\"Model trained and saved as 'final_rcp_route_duration_model.cbm'\")\n",
    "\n",
    "# --- Predict and Evaluate on the Future Test Set ---\n",
    "\n",
    "X_test_features = rcp_X_test.drop(columns=[c for c in drop_cols if c in rcp_X_test.columns])\n",
    "y_test_target = rcp_X_test[TARGET_COL]\n",
    "\n",
    "predictions = final_model.predict(X_test_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243",
   "metadata": {
    "id": "234"
   },
   "source": [
    "### Using Optuna for hyperparameters tuning\n",
    "- Uncomment to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244",
   "metadata": {
    "id": "235"
   },
   "outputs": [],
   "source": [
    "# from catboost import Pool\n",
    "\n",
    "# # ---  Data Preparation ---\n",
    "# rcp_X_train = rcp_X_train.sort_values('date').reset_index(drop=True)\n",
    "# rcp_X_test = rcp_X_test.sort_values('date').reset_index(drop=True)\n",
    "# TARGET_COL = 'route_duration'\n",
    "# cat_features = [\n",
    "#     'etablissement_postal',\n",
    "#     'next_etablissement_postal',\n",
    "#     'day_of_week',\n",
    "#     'origin_destination',\n",
    "#     'EVENT_TYPE_CD',\n",
    "#     'hour',\n",
    "#     'month',\n",
    "#     'flow_type',\n",
    "# ]\n",
    "# drop_cols = [TARGET_COL, 'RECPTCL_FID', 'date', 'destination_country', 'origin_country']\n",
    "# X_train_full = rcp_X_train.drop(columns=[c for c in drop_cols if c in rcp_X_train.columns])\n",
    "# y_train_full = rcp_X_train[TARGET_COL]\n",
    "# categorical_feature_names = [c for c in cat_features if c in X_train_full.columns]\n",
    "\n",
    "# train_pool = Pool(data=X_train_full, label=y_train_full, cat_features=categorical_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245",
   "metadata": {
    "id": "236"
   },
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import json\n",
    "# from catboost import cv\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         'loss_function': 'Huber:delta=20.0',\n",
    "#         'eval_metric': 'MAE',\n",
    "#         'custom_metric': ['RMSE'], # Ensure RMSE is calculated\n",
    "#         'random_seed': 42,\n",
    "#         'verbose': 0,\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "#         'depth': trial.suggest_int('depth', 4, 8),\n",
    "#         'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "#         'iterations': 1000,\n",
    "#         'early_stopping_rounds': 30\n",
    "#     }\n",
    "\n",
    "#     cv_results = cv(\n",
    "#         params=params,\n",
    "#         pool=train_pool,\n",
    "#         fold_count=3,\n",
    "#         shuffle=False,\n",
    "#         type='TimeSeries',\n",
    "#         plot=False\n",
    "#     )\n",
    "\n",
    "#     # Track both metrics\n",
    "#     best_mae = cv_results['test-MAE-mean'].min()\n",
    "#     best_rmse = cv_results['test-RMSE-mean'].min()\n",
    "\n",
    "#     return best_mae, best_rmse\n",
    "\n",
    "# # --- 2. Run Multi-Objective Optimization ---\n",
    "# print(\"Starting Hyperparameter Tuning (Tracking MAE & RMSE)...\")\n",
    "# study = optuna.create_study(directions=['minimize', 'minimize'])\n",
    "# study.optimize(objective, n_trials=20)\n",
    "\n",
    "# # --- 3. Pick the best trial and find optimal iterations ---\n",
    "# # We pick the trial with the lowest MAE\n",
    "# best_trial = min(study.best_trials, key=lambda t: t.values[0])\n",
    "\n",
    "# print(f\"\\nBest Trial selected (MAE: {best_trial.values[0]:.4f}, RMSE: {best_trial.values[1]:.4f})\")\n",
    "\n",
    "# # Find best iterations for the winning set on 5 folds for stability\n",
    "# final_params_temp = {\n",
    "#     'loss_function': 'Huber:delta=20.0',\n",
    "#     'eval_metric': 'MAE',\n",
    "#     **best_trial.params\n",
    "# }\n",
    "\n",
    "# print(\"Calculating final optimal iterations...\")\n",
    "# final_cv = cv(params=final_params_temp, pool=train_pool, fold_count=5, shuffle=False, type='TimeSeries', verbose=0)\n",
    "# best_iteration = int(final_cv['test-MAE-mean'].values.argmin() + 1)\n",
    "\n",
    "# # --- 4. Save to JSON ---\n",
    "# best_config = {\n",
    "#     **best_trial.params,\n",
    "#     \"iterations\": best_iteration,\n",
    "#     \"loss_function\": 'Huber:delta=20.0',\n",
    "#     \"eval_metric\": 'MAE',\n",
    "#     \"final_mae\": best_trial.values[0],\n",
    "#     \"final_rmse\": best_trial.values[1]\n",
    "# }\n",
    "\n",
    "# with open(\"catboost_rcp_best_params.json\", \"w\") as f:\n",
    "#     json.dump(best_config, f, indent=4)\n",
    "\n",
    "# print(f\"Tuning complete. Best parameters saved to 'catboost_rcp_best_params.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246",
   "metadata": {
    "id": "237"
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# from catboost import CatBoostRegressor\n",
    "\n",
    "# # --- 1. Load Parameters ---\n",
    "# with open(\"catboost_rcp_best_params.json\", \"r\") as f:\n",
    "#     best_config = json.load(f)\n",
    "\n",
    "# # Extract iterations and remove metrics/metadata so CatBoost doesn't crash\n",
    "# iters = best_config.pop(\"iterations\")\n",
    "# best_config.pop(\"final_mae\", None)   # Remove if present\n",
    "# best_config.pop(\"final_rmse\", None)  # Remove if present\n",
    "\n",
    "# # --- 2. Initialize and Train Model ---\n",
    "# # Ensure categorical_feature_names and train_pool are defined in your environment\n",
    "# final_model = CatBoostRegressor(\n",
    "#     iterations=iters,\n",
    "#     **best_config,\n",
    "#     cat_features=categorical_feature_names,\n",
    "#     random_seed=42,\n",
    "#     verbose=100\n",
    "# )\n",
    "\n",
    "# print(\"Training final model with saved parameters...\")\n",
    "# final_model.fit(train_pool)\n",
    "\n",
    "# # --- 3. Save the actual Model weights ---\n",
    "# final_model.save_model(\"final_rcp_route_duration_model.cbm\")\n",
    "# print(\"Model trained and saved as 'final_rcp_route_duration_model.cbm'\")\n",
    "\n",
    "# # --- 4. Predict on Test Set ---\n",
    "# # Ensure rcp_X_test and drop_cols are defined in your environment\n",
    "# X_test_features = rcp_X_test.drop(columns=[c for c in drop_cols if c in rcp_X_test.columns])\n",
    "# predictions = final_model.predict(X_test_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247",
   "metadata": {
    "id": "238"
   },
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248",
   "metadata": {
    "id": "239"
   },
   "source": [
    "- choose which one to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249",
   "metadata": {
    "id": "240"
   },
   "outputs": [],
   "source": [
    "X_test=pkg_X_test.copy()\n",
    "#X_test=rcp_X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250",
   "metadata": {
    "id": "241"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error,root_mean_squared_error\n",
    "y_test_true = X_test[TARGET_COL]\n",
    "y_test_pred_subset = predictions\n",
    "\n",
    "final_rmse = root_mean_squared_error(y_test_true, y_test_pred_subset)\n",
    "final_mae = mean_absolute_error(y_test_true, y_test_pred_subset)\n",
    "\n",
    "print(f\"\\nFinal Model Evaluation on Test Set:\")\n",
    "print(f\"RMSE: {final_rmse:.4f} hours\")\n",
    "print(f\"MAE: {final_mae:.4f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251",
   "metadata": {
    "id": "242"
   },
   "outputs": [],
   "source": [
    "# --- Feature Importance Analysis ---\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the feature importances from the trained model\n",
    "feature_importances = final_model.get_feature_importance()\n",
    "feature_names = X_train_full.columns\n",
    "\n",
    "# Create a Series for easy sorting and handling\n",
    "importance_series = pd.Series(feature_importances, index=feature_names)\n",
    "\n",
    "# Sort the features by importance (descending)\n",
    "sorted_importance = importance_series.sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n--- Feature Importances (Top 10) ---\")\n",
    "print(sorted_importance.head(10))\n",
    "\n",
    "# Optional: Plot the top 10 features\n",
    "plt.figure(figsize=(10, 6))\n",
    "sorted_importance.head(10).plot(kind='barh', color='skyblue')\n",
    "plt.title('Top 10 CatBoost Feature Importances')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252",
   "metadata": {
    "id": "243"
   },
   "outputs": [],
   "source": [
    "interactions = final_model.get_feature_importance(type='Interaction', prettified=True)\n",
    "print(\"\\nTop Feature Interactions:\")\n",
    "print(interactions.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253",
   "metadata": {
    "id": "244"
   },
   "source": [
    "### Analysis of the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254",
   "metadata": {
    "id": "245"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate residuals\n",
    "y_test_target = X_test[TARGET_COL]\n",
    "residuals = y_test_target - predictions\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=predictions, y=residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Residual Plot: Prediction vs. Error')\n",
    "plt.xlabel('Predicted Route Duration (Hours)')\n",
    "plt.ylabel('Residuals (Actual - Predicted)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255",
   "metadata": {
    "id": "246"
   },
   "outputs": [],
   "source": [
    "# 1. Calculate the raw error (not absolute)\n",
    "results = pd.DataFrame({\n",
    "    'Actual': y_test_target,\n",
    "    'Predicted': predictions\n",
    "})\n",
    "results['Residual'] = results['Predicted'] - results['Actual']\n",
    "\n",
    "# 2. Filter for large errors (greater than 50 hours)\n",
    "big_errors = results[np.abs(results['Residual']) > 50]\n",
    "\n",
    "# 3. Count which is more common\n",
    "missed_route_duration = big_errors[big_errors['Residual'] < 0].shape[0]\n",
    "model_hallucinations = big_errors[big_errors['Residual'] > 0].shape[0]\n",
    "\n",
    "print(f\"Total Large Errors (>50h): {len(big_errors)}\")\n",
    "print(f\"---\")\n",
    "print(f\"Missed route_duration (Under-predicted): {missed_route_duration}\")\n",
    "print(f\"Model Hallucinations (Over-predicted): {model_hallucinations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256",
   "metadata": {
    "id": "247"
   },
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257",
   "metadata": {
    "id": "248"
   },
   "source": [
    "## Preprocessing Pipeline\n",
    "This section consolidates all the preprocessing steps defined above into a reusable pipeline that can be applied to `X_train_full` and `y_train_full`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258",
   "metadata": {
    "id": "249"
   },
   "outputs": [],
   "source": [
    "# 1. Ensure chronological order\n",
    "pkg_X_train = pkg_X_train.sort_values('date').reset_index(drop=True)\n",
    "pkg_X_test = pkg_X_test.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# --- Define Model Parameters and Features ---\n",
    "TARGET_COL = 'route_duration'\n",
    "\n",
    "# List of categorical features (Kept country_service as it was helping!)\n",
    "cat_features = [\n",
    "    'etablissement_postal',\n",
    "    'next_etablissement_postal',\n",
    "    'day_of_week',\n",
    "    'service_indicator',\n",
    "    'origin_destination',\n",
    "    'EVENT_TYPE_CD',\n",
    "    'hour',\n",
    "    'month',\n",
    "    'is_weekend',\n",
    "    'first_last_week_day',\n",
    "    'country_service'\n",
    "]\n",
    "\n",
    "# Columns to drop\n",
    "drop_cols = [TARGET_COL, 'MAILITM_FID', 'RECPTCL_FID', 'date', 'serial_number',\n",
    "             'flow_type', 'country_code', 'destination_country', 'origin_country']\n",
    "\n",
    "# Prepare the full 80% training set\n",
    "X_train_full = pkg_X_train.drop(columns=[c for c in drop_cols if c in pkg_X_train.columns])\n",
    "y_train_full = pkg_X_train[TARGET_COL]\n",
    "\n",
    "# Identify categorical features by name\n",
    "categorical_feature_names = [c for c in cat_features if c in X_train_full.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259",
   "metadata": {
    "id": "WthxmvbHtLaO"
   },
   "outputs": [],
   "source": [
    "# !pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260",
   "metadata": {
    "id": "250"
   },
   "outputs": [],
   "source": [
    "# --- Import necessary libraries ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from category_encoders import CountEncoder\n",
    "\n",
    "# --- Define feature groups based on encoding strategy ---\n",
    "# Low cardinality features: One-Hot Encoding\n",
    "ONE_HOT_COLS = ['EVENT_TYPE_CD', 'day_of_week','is_weekend', 'first_last_week_day']\n",
    "\n",
    "# High cardinality features: Target Encoding\n",
    "HIGH_CARDINALITY_COLS = ['etablissement_postal', 'next_etablissement_postal']\n",
    "\n",
    "# Medium cardinality features: Count Encoding\n",
    "MEDIUM_CARDINALITY_COLS = ['origin_country', 'service_indicator','country_service']\n",
    "\n",
    "# Cyclical features: Sin/Cos Transformation\n",
    "CYCLICAL_COLS = ['hour', 'month']\n",
    "\n",
    "# Numerical features that should be scaled\n",
    "NUMERICAL_COLS = [\n",
    "    'etab_load_1h',\n",
    "    'route_load_1h',\n",
    "    'time_since_first_scan',\n",
    "    'days_since_last_holiday',\n",
    "    'days_until_next_holiday',\n",
    "    'time_since_last_scan'\n",
    "]\n",
    "\n",
    "print(\"Feature groups defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261",
   "metadata": {
    "id": "251"
   },
   "outputs": [],
   "source": [
    "# --- Define encoding functions ---\n",
    "def cyclical_transformer(period):\n",
    "    \"\"\"Create a cyclical encoding function for a given period.\"\"\"\n",
    "    def transform(X):\n",
    "        X = np.asarray(X).astype(float)\n",
    "        sin = np.sin(2 * np.pi * X / period)\n",
    "        cos = np.cos(2 * np.pi * X / period)\n",
    "        return np.c_[sin, cos]\n",
    "    return transform\n",
    "\n",
    "# Hour transformer (24-hour cycle)\n",
    "hour_transformer = FunctionTransformer(\n",
    "    cyclical_transformer(24),\n",
    "    feature_names_out=lambda transformer, names: [\"hour_sin\", \"hour_cos\"]\n",
    ")\n",
    "\n",
    "# Month transformer (12-month cycle)\n",
    "month_transformer = FunctionTransformer(\n",
    "    cyclical_transformer(12),\n",
    "    feature_names_out=lambda transformer, names: [\"month_sin\", \"month_cos\"]\n",
    ")\n",
    "\n",
    "print(\"Cyclical transformers created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262",
   "metadata": {
    "id": "252"
   },
   "outputs": [],
   "source": [
    "# --- Build the preprocessing pipeline function ---\n",
    "def create_preprocessing_pipeline(\n",
    "    one_hot_cols=ONE_HOT_COLS,\n",
    "    high_cardinality_cols=HIGH_CARDINALITY_COLS,\n",
    "    medium_cardinality_cols=MEDIUM_CARDINALITY_COLS,\n",
    "    numerical_cols=NUMERICAL_COLS,\n",
    "    scale_numerical=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a comprehensive preprocessing pipeline for the packages dataset.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    one_hot_cols : list\n",
    "        Columns for one-hot encoding (low cardinality categorical)\n",
    "    high_cardinality_cols : list\n",
    "        Columns for target encoding (high cardinality categorical)\n",
    "    medium_cardinality_cols : list\n",
    "        Columns for count encoding (medium cardinality categorical)\n",
    "    numerical_cols : list\n",
    "        Numerical columns to scale\n",
    "    scale_numerical : bool\n",
    "        Whether to apply StandardScaler to numerical features\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    sklearn.compose.ColumnTransformer\n",
    "        A preprocessing pipeline\n",
    "    \"\"\"\n",
    "\n",
    "    transformers = [\n",
    "        # Cyclical encoding for hour\n",
    "        ('hour_cyclical', hour_transformer, ['hour']),\n",
    "\n",
    "        # Cyclical encoding for month\n",
    "        ('month_cyclical', month_transformer, ['month']),\n",
    "\n",
    "        # One-hot encoding for low cardinality categorical features\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False), one_hot_cols),\n",
    "\n",
    "        # Target encoding for high cardinality features\n",
    "        ('target_enc', TargetEncoder(), high_cardinality_cols),\n",
    "\n",
    "        # Count encoding for medium cardinality features\n",
    "        ('count_enc', CountEncoder(cols=medium_cardinality_cols), medium_cardinality_cols),\n",
    "    ]\n",
    "\n",
    "    # Add numerical scaler if requested\n",
    "    if scale_numerical and numerical_cols:\n",
    "        transformers.append(\n",
    "            ('num_scaler', StandardScaler(), numerical_cols)\n",
    "        )\n",
    "    elif numerical_cols:\n",
    "        transformers.append(\n",
    "            ('num_passthrough', 'passthrough', numerical_cols)\n",
    "        )\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=transformers,\n",
    "        remainder='drop',  # Drop any columns not explicitly handled\n",
    "        verbose_feature_names_out=True\n",
    "    )\n",
    "\n",
    "    return preprocessor\n",
    "\n",
    "print(\"Pipeline function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263",
   "metadata": {
    "id": "253"
   },
   "outputs": [],
   "source": [
    "# --- Helper function to preprocess data ---\n",
    "def preprocess_data(X, y=None, preprocessor=None, fit=True):\n",
    "    \"\"\"\n",
    "    Apply preprocessing to the data.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pd.DataFrame\n",
    "        Feature dataframe\n",
    "    y : pd.Series, optional\n",
    "        Target variable (required for target encoding during fit)\n",
    "    preprocessor : ColumnTransformer, optional\n",
    "        Pre-fitted preprocessor (for transform only)\n",
    "    fit : bool\n",
    "        Whether to fit the preprocessor or just transform\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (X_processed, preprocessor)\n",
    "        Processed features and the fitted preprocessor\n",
    "    \"\"\"\n",
    "    if preprocessor is None:\n",
    "        preprocessor = create_preprocessing_pipeline()\n",
    "\n",
    "    # Set output to pandas DataFrame\n",
    "    preprocessor.set_output(transform='pandas')\n",
    "\n",
    "    if fit:\n",
    "        if y is None:\n",
    "            raise ValueError(\"y is required for fitting (needed for TargetEncoder)\")\n",
    "        X_processed = preprocessor.fit_transform(X, y)\n",
    "    else:\n",
    "        X_processed = preprocessor.transform(X)\n",
    "\n",
    "    return X_processed, preprocessor\n",
    "\n",
    "print(\"Preprocessing helper function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264",
   "metadata": {
    "id": "254"
   },
   "source": [
    "## Apply the pipeline to X_train_full and y_train_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265",
   "metadata": {
    "id": "255"
   },
   "outputs": [],
   "source": [
    "# --- Apply the pipeline to X_train_full and y_train_full ---\n",
    "print(\"Creating preprocessing pipeline...\")\n",
    "\n",
    "# Filter columns that exist in X_train_full\n",
    "available_one_hot = [c for c in ONE_HOT_COLS if c in X_train_full.columns]\n",
    "available_high_card = [c for c in HIGH_CARDINALITY_COLS if c in X_train_full.columns]\n",
    "available_medium_card = [c for c in MEDIUM_CARDINALITY_COLS if c in X_train_full.columns]\n",
    "available_numerical = [c for c in NUMERICAL_COLS if c in X_train_full.columns]\n",
    "\n",
    "print(f\"One-hot encoding columns: {available_one_hot}\")\n",
    "print(f\"Target encoding columns: {available_high_card}\")\n",
    "print(f\"Count encoding columns: {available_medium_card}\")\n",
    "print(f\"Numerical columns: {available_numerical}\")\n",
    "\n",
    "# Create the preprocessor with available columns\n",
    "preprocessor = create_preprocessing_pipeline(\n",
    "    one_hot_cols=available_one_hot,\n",
    "    high_cardinality_cols=available_high_card,\n",
    "    medium_cardinality_cols=available_medium_card,\n",
    "    numerical_cols=available_numerical,\n",
    "    scale_numerical=True\n",
    ")\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_processed, fitted_preprocessor = preprocess_data(\n",
    "    X_train_full,\n",
    "    y_train_full,\n",
    "    preprocessor=preprocessor,\n",
    "    fit=True\n",
    ")\n",
    "\n",
    "print(f\"\\nOriginal X_train_full shape: {X_train_full.shape}\")\n",
    "print(f\"Processed X_train shape: {X_train_processed.shape}\")\n",
    "print(f\"\\nProcessed feature names:\")\n",
    "print(X_train_processed.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266",
   "metadata": {
    "id": "256"
   },
   "outputs": [],
   "source": [
    "# --- Transform test data using the fitted preprocessor ---\n",
    "# Prepare X_test_full similar to X_train_full\n",
    "X_test_full = pkg_X_test.drop(columns=[c for c in drop_cols if c in pkg_X_test.columns])\n",
    "y_test_full = pkg_X_test[TARGET_COL]\n",
    "\n",
    "# Transform test data (fit=False to avoid data leakage)\n",
    "X_test_processed, _ = preprocess_data(\n",
    "    X_test_full,\n",
    "    preprocessor=fitted_preprocessor,\n",
    "    fit=False\n",
    ")\n",
    "\n",
    "print(f\"X_test_full shape: {X_test_full.shape}\")\n",
    "print(f\"X_test_processed shape: {X_test_processed.shape}\")\n",
    "\n",
    "# Display sample of processed data\n",
    "print(\"\\n--- Sample of processed training data ---\")\n",
    "X_train_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267",
   "metadata": {
    "id": "257"
   },
   "source": [
    "## KNN Regressor with RandomizedSearchCV\n",
    "Using the processed data from the preprocessing pipeline to train a KNN Regressor with hyperparameter tuning via RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268",
   "metadata": {
    "id": "258"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, make_scorer\n",
    "from scipy.stats import randint, uniform\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269",
   "metadata": {
    "id": "259"
   },
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'n_neighbors': randint(20, 25),           # Number of neighbors (3 to 50)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270",
   "metadata": {
    "id": "260"
   },
   "outputs": [],
   "source": [
    "# Using TimeSeriesSplit to avoid data leakage in time-series data\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Initialize KNN Regressor\n",
    "knn_base = KNeighborsRegressor(n_jobs=-1)\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "# Using negative MAE as scoring (sklearn convention: higher is better)\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=knn_base,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=5,                    # Number of random combinations to try\n",
    "    scoring='neg_mean_absolute_error',  # Optimize for MAE\n",
    "    cv=tscv,                      # TimeSeriesSplit cross-validation\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,                    # Use all available cores\n",
    "    return_train_score=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271",
   "metadata": {
    "id": "261"
   },
   "outputs": [],
   "source": [
    "# Fit the random search\n",
    "random_search.fit(X_train_processed, y_train_full)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RandomizedSearchCV Complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272",
   "metadata": {
    "id": "262"
   },
   "outputs": [],
   "source": [
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest Cross-Validation MAE: {-random_search.best_score_:.4f} hours\")\n",
    "\n",
    "# Get the best model\n",
    "best_knn_model = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273",
   "metadata": {
    "id": "263"
   },
   "outputs": [],
   "source": [
    "#save best model using joblib\n",
    "import joblib\n",
    "filename = 'knn-model.joblib'\n",
    "joblib.dump(best_knn_model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274",
   "metadata": {
    "id": "264"
   },
   "source": [
    "## Evaluate Best KNN Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275",
   "metadata": {
    "id": "265"
   },
   "outputs": [],
   "source": [
    "knn_predictions = best_knn_model.predict(X_test_processed)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "knn_mae = mean_absolute_error(y_test_full, knn_predictions)\n",
    "knn_rmse = root_mean_squared_error(y_test_full, knn_predictions)\n",
    "\n",
    "# MAE as percentage of mean\n",
    "mean_actual = np.mean(y_test_full)\n",
    "knn_mae_pct = (knn_mae / mean_actual) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KNN Regressor - Test Set Evaluation\")\n",
    "print(\"=\"*60)\n",
    "print(f\"MAE:  {knn_mae:.4f} hours\")\n",
    "print(f\"RMSE: {knn_rmse:.4f} hours\")\n",
    "print(f\"\\nMean Actual route duration: {mean_actual:.4f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276",
   "metadata": {
    "id": "266"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "knn_residuals = y_test_full - knn_predictions\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Actual vs Predicted\n",
    "axes[0].scatter(y_test_full, knn_predictions, alpha=0.3, s=10)\n",
    "axes[0].plot([y_test_full.min(), y_test_full.max()],\n",
    "             [y_test_full.min(), y_test_full.max()],\n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual route_duration (hours)')\n",
    "axes[0].set_ylabel('Predicted route_duration (hours)')\n",
    "axes[0].set_title('KNN: Actual vs Predicted')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Residual Distribution\n",
    "axes[1].hist(knn_residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Residual (Actual - Predicted)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('KNN: Residual Distribution')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Residuals vs Predicted\n",
    "axes[2].scatter(knn_predictions, knn_residuals, alpha=0.3, s=10)\n",
    "axes[2].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[2].set_xlabel('Predicted route_duration (hours)')\n",
    "axes[2].set_ylabel('Residual (hours)')\n",
    "axes[2].set_title('KNN: Residuals vs Predicted')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print residual statistics\n",
    "print(\"\\nResidual Statistics:\")\n",
    "print(f\"  Mean Residual: {np.mean(knn_residuals):.4f} hours\")\n",
    "print(f\"  Std Residual: {np.std(knn_residuals):.4f} hours\")\n",
    "print(f\"  Min Residual: {np.min(knn_residuals):.4f} hours\")\n",
    "print(f\"  Max Residual: {np.max(knn_residuals):.4f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277",
   "metadata": {
    "id": "267"
   },
   "source": [
    "## KNN with Log-Transformed Target\n",
    "Since the delay has a long right tail, we'll try log-transforming the target to see if it improves KNN performance.\n",
    "\n",
    "**Why this should help:**\n",
    "- KNN averages neighbors' values, so extreme outliers dominate predictions\n",
    "- Log transformation compresses outliers (7000h → ~8.85 in log-space)\n",
    "- Creates a more symmetric distribution better suited for distance-based averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278",
   "metadata": {
    "id": "268"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Original distribution\n",
    "axes[0].hist(y_train_full, bins=100, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('route_duration (hours)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Original route_duration Distribution')\n",
    "axes[0].axvline(y_train_full.mean(), color='r', linestyle='--', label=f'Mean: {y_train_full.mean():.2f}')\n",
    "axes[0].axvline(y_train_full.median(), color='g', linestyle='--', label=f'Median: {y_train_full.median():.2f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Log-transformed distribution (using log1p to handle zeros)\n",
    "y_train_log = np.log1p(y_train_full)\n",
    "axes[1].hist(y_train_log, bins=100, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1].set_xlabel('Log(1 + route_duration)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Log-Transformed route_duration Distribution')\n",
    "axes[1].axvline(y_train_log.mean(), color='r', linestyle='--', label=f'Mean: {y_train_log.mean():.2f}')\n",
    "axes[1].axvline(y_train_log.median(), color='g', linestyle='--', label=f'Median: {y_train_log.median():.2f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Original - Skewness: {y_train_full.skew():.2f}\")\n",
    "print(f\"Log-transformed - Skewness: {y_train_log.skew():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279",
   "metadata": {
    "id": "269"
   },
   "outputs": [],
   "source": [
    "y_train_log = np.log1p(y_train_full)  # log(1 + y) to handle zeros\n",
    "y_test_log = np.log1p(y_test_full)\n",
    "\n",
    "print(f\"Training target - Original range: [{y_train_full.min():.2f}, {y_train_full.max():.2f}]\")\n",
    "print(f\"Training target - Log range: [{y_train_log.min():.2f}, {y_train_log.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280",
   "metadata": {
    "id": "270"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_distributions_log = {\n",
    "    'n_neighbors': randint(9, 30),\n",
    "}\n",
    "\n",
    "# Set up TimeSeriesSplit\n",
    "tscv_log = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Initialize KNN and RandomizedSearchCV\n",
    "knn_log = KNeighborsRegressor(n_jobs=-1)\n",
    "\n",
    "random_search_log = RandomizedSearchCV(\n",
    "    estimator=knn_log,\n",
    "    param_distributions=param_distributions_log,\n",
    "    n_iter=5,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=tscv_log,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"RandomizedSearchCV configured for log-transformed target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281",
   "metadata": {
    "id": "271"
   },
   "outputs": [],
   "source": [
    "print(\"Starting RandomizedSearchCV for KNN with log-transformed target...\")\n",
    "print(f\"Training data shape: {X_train_processed.shape}\")\n",
    "print(f\"Log target shape: {y_train_log.shape}\")\n",
    "print(\"This may take a while...\\n\")\n",
    "\n",
    "random_search_log.fit(X_train_processed, y_train_log)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RandomizedSearchCV Complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282",
   "metadata": {
    "id": "272"
   },
   "outputs": [],
   "source": [
    "# --- Display Best Hyperparameters (Log Target) ---\n",
    "print(\"Best Hyperparameters for KNN with Log-Transformed Target:\")\n",
    "print(\"-\" * 50)\n",
    "for param, value in random_search_log.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"\\nBest CV Score (Neg MSE on log scale): {random_search_log.best_score_:.4f}\")\n",
    "print(f\"Best CV RMSE (on log scale): {np.sqrt(-random_search_log.best_score_):.4f}\")\n",
    "\n",
    "# Get the best model\n",
    "best_knn_log = random_search_log.best_estimator_\n",
    "print(f\"\\nBest KNN Model (Log Target): {best_knn_log}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283",
   "metadata": {
    "id": "273"
   },
   "outputs": [],
   "source": [
    "# --- Evaluate KNN (Log Target) on Test Set ---\n",
    "# Predict on log scale\n",
    "y_pred_log = best_knn_log.predict(X_test_processed)\n",
    "\n",
    "# Inverse transform to get predictions on original scale\n",
    "y_pred_original = np.expm1(y_pred_log)\n",
    "\n",
    "# Ensure no negative predictions (can happen due to log transform edge cases)\n",
    "y_pred_original = np.maximum(y_pred_original, 0)\n",
    "\n",
    "print(\"Predictions converted back to original scale using exp(x) - 1\")\n",
    "print(f\"Prediction range: [{y_pred_original.min():.2f}, {y_pred_original.max():.2f}] hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284",
   "metadata": {
    "id": "274"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Calculate metrics on original scale for fair comparison\n",
    "knn_log_mae = mean_absolute_error(y_test_full, y_pred_original)\n",
    "knn_log_rmse = np.sqrt(mean_squared_error(y_test_full, y_pred_original))\n",
    "knn_log_mape = np.mean(np.abs((y_test_full - y_pred_original) / (y_test_full + 1e-8))) * 100\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"KNN with Log-Transformed Target - Test Set Performance\")\n",
    "print(\"(Evaluated on ORIGINAL scale after inverse transform)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  MAE:  {knn_log_mae:.2f} hours\")\n",
    "print(f\"  RMSE: {knn_log_rmse:.2f} hours\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285",
   "metadata": {
    "id": "275"
   },
   "outputs": [],
   "source": [
    "# --- Visualize Predictions: Original vs Log Target KNN ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Get predictions from original KNN for comparison\n",
    "y_pred_original_knn = best_knn_model.predict(X_test_processed)\n",
    "\n",
    "# 1. Actual vs Predicted scatter plot\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(y_test_full, y_pred_original_knn, alpha=0.3, label='Original Target', s=10)\n",
    "ax1.scatter(y_test_full, y_pred_original, alpha=0.3, label='Log Target', s=10)\n",
    "max_val = max(y_test_full.max(), y_pred_original_knn.max(), y_pred_original.max())\n",
    "ax1.plot([0, max_val], [0, max_val], 'r--', label='Perfect Prediction')\n",
    "ax1.set_xlabel('Actual route_duration (hours)')\n",
    "ax1.set_ylabel('Predicted route_duration (hours)')\n",
    "ax1.set_title('Actual vs Predicted')\n",
    "ax1.legend()\n",
    "ax1.set_xlim(0, np.percentile(y_test_full, 99))\n",
    "ax1.set_ylim(0, np.percentile(y_test_full, 99))\n",
    "\n",
    "# 2. Residual distribution\n",
    "ax2 = axes[1]\n",
    "residuals_orig = y_test_full - y_pred_original_knn\n",
    "residuals_log = y_test_full - y_pred_original\n",
    "ax2.hist(residuals_orig, bins=50, alpha=0.5, label=f'Original (std={residuals_orig.std():.1f})', density=True)\n",
    "ax2.hist(residuals_log, bins=50, alpha=0.5, label=f'Log (std={residuals_log.std():.1f})', density=True)\n",
    "ax2.axvline(x=0, color='r', linestyle='--')\n",
    "ax2.set_xlabel('Residual (hours)')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.set_title('Residual Distribution')\n",
    "ax2.legend()\n",
    "ax2.set_xlim(-100, 100)\n",
    "\n",
    "# 3. Prediction distribution comparison\n",
    "ax3 = axes[2]\n",
    "ax3.hist(y_pred_original_knn, bins=50, alpha=0.5, label='Original Target KNN', density=True)\n",
    "ax3.hist(y_pred_original, bins=50, alpha=0.5, label='Log Target KNN', density=True)\n",
    "ax3.hist(y_test_full, bins=50, alpha=0.3, label='Actual', density=True)\n",
    "ax3.set_xlabel('route_duration (hours)')\n",
    "ax3.set_ylabel('Density')\n",
    "ax3.set_title('Prediction Distribution vs Actual')\n",
    "ax3.legend()\n",
    "ax3.set_xlim(0, np.percentile(y_test_full, 99))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286",
   "metadata": {
    "id": "276"
   },
   "source": [
    "# Anomaly Detection\n",
    "\n",
    "## Overview\n",
    "\n",
    "This section implements a comprehensive anomaly detection framework to identify unusual delivery patterns and performance issues. The analysis detects packages with abnormal delivery durations that deviate significantly from predicted baselines.\n",
    "\n",
    "### Detection Strategy\n",
    "\n",
    "We implement a **multi-method ensemble approach** combining three complementary techniques:\n",
    "\n",
    "1. **Statistical Methods (Z-Score & IQR)** - Detect extreme residuals using statistical thresholds\n",
    "2. **Machine Learning (Isolation Forest)** - Identify unusual patterns across multiple feature dimensions\n",
    "3. **Ensemble Scoring** - Combine methods for robust anomaly confidence assessment (0-3 scale)\n",
    "\n",
    "### Expected Outcomes\n",
    "\n",
    "- Labeled dataset with binary anomaly flags and confidence scores\n",
    "- Quantification of anomaly prevalence across routes and time periods\n",
    "- Prioritized list of problematic delivery patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287",
   "metadata": {
    "id": "278"
   },
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "Prepare the test dataset for anomaly detection by computing prediction residuals and organizing features for analysis.\n",
    "\n",
    "**Key Operations:**\n",
    "- Calculate residuals (actual - predicted duration)\n",
    "- Include temporal and categorical features\n",
    "- Optimize memory with efficient data types\n",
    "- Establish baseline statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288",
   "metadata": {
    "id": "efc8a06c"
   },
   "outputs": [],
   "source": [
    "# Prepare data for anomaly detection\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Calculate residuals (prediction errors)\n",
    "y_actual = X_test[TARGET_COL]\n",
    "residuals = y_actual - predictions\n",
    "\n",
    "# Add residuals to test data for analysis (use pkg_X_test to include time features)\n",
    "anomaly_df = pkg_X_test.copy()\n",
    "anomaly_df['predicted_duration'] = predictions\n",
    "anomaly_df['residual'] = residuals\n",
    "anomaly_df['absolute_residual'] = np.abs(residuals)\n",
    "\n",
    "# Convert categorical columns to category dtype for memory efficiency\n",
    "for col in ['etablissement_postal', 'next_etablissement_postal', 'EVENT_TYPE_CD']:\n",
    "    if col in anomaly_df.columns:\n",
    "        anomaly_df[col] = anomaly_df[col].astype('category')\n",
    "\n",
    "print(f\"✓ Data prepared for anomaly detection\")\n",
    "print(f\"Shape: {anomaly_df.shape}\")\n",
    "print(f\"Memory usage: {anomaly_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"\\nResidual statistics:\")\n",
    "print(residuals.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289",
   "metadata": {
    "id": "14f20e05"
   },
   "source": [
    "### Residual Distribution Visualization\n",
    "\n",
    "Visualize prediction errors across the test set to understand model performance.\n",
    "\n",
    "**Key Elements:**\n",
    "- Zero Error Line: Perfect predictions baseline\n",
    "- ±2σ Bounds: Contains ~95% of predictions\n",
    "- Color Intensity: Magnitude of prediction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290",
   "metadata": {
    "id": "6c1dedd0"
   },
   "outputs": [],
   "source": [
    "# Visualize Residuals Across Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "sample_indices = np.arange(len(residuals))\n",
    "ax.scatter(sample_indices, residuals, alpha=0.3, s=5, c=np.abs(residuals),\n",
    "           cmap='Reds', rasterized=True)\n",
    "ax.axhline(0, color='black', linestyle='-', linewidth=2, alpha=0.7, label='Zero Error')\n",
    "ax.axhline(residuals.mean() + 2*residuals.std(), color='black', linestyle='--',\n",
    "           linewidth=1.5, alpha=0.6, label='±2σ')\n",
    "ax.axhline(residuals.mean() - 2*residuals.std(), color='black', linestyle='--',\n",
    "           linewidth=1.5, alpha=0.6)\n",
    "ax.set_xlabel('Sample Index', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Residual (hours)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Residuals Across Dataset', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3, linestyle=':')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Residual visualization complete!\")\n",
    "print(f\"  Mean Absolute Error (MAE): {np.abs(residuals).mean():.2f} hours\")\n",
    "print(f\"  Root Mean Squared Error (RMSE): {np.sqrt((residuals**2).mean()):.2f} hours\")\n",
    "print(f\"  Percentage within ±24h: {((np.abs(residuals) <= 24).sum() / len(residuals) * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291",
   "metadata": {
    "id": "bfe006d4"
   },
   "source": [
    "## 2. Z-Score Method\n",
    "\n",
    "Statistical anomaly detection based on standard deviation from the mean.\n",
    "\n",
    "**Threshold:** |Z| > 3 (99.7% confidence interval)  \n",
    "**Formula:** $Z = \\frac{(x - \\mu)}{\\sigma}$\n",
    "\n",
    "**Advantages:**\n",
    "- Simple and interpretable\n",
    "- Objective mathematical thresholds\n",
    "- Grounded in statistical theory\n",
    "\n",
    "**Limitations:**\n",
    "- Assumes normal distribution\n",
    "- Univariate (only considers residual magnitude)\n",
    "- Sensitive to overall variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292",
   "metadata": {
    "id": "b3b264e6"
   },
   "outputs": [],
   "source": [
    "# Statistical anomaly detection using Z-Score\n",
    "# Calculate Z-scores for residuals\n",
    "z_scores = np.abs(stats.zscore(residuals))\n",
    "\n",
    "# Flag anomalies: Z-score > 3 (99.7% confidence interval)\n",
    "z_threshold = 3\n",
    "anomaly_df['z_score'] = z_scores\n",
    "anomaly_df['anomaly_zscore'] = (z_scores > z_threshold).astype(int)\n",
    "\n",
    "print(f\"Z-Score Method:\")\n",
    "print(f\"Threshold: {z_threshold}\")\n",
    "print(f\"Anomalies detected: {anomaly_df['anomaly_zscore'].sum()} ({anomaly_df['anomaly_zscore'].mean()*100:.2f}%)\")\n",
    "print(f\"\\nZ-score statistics:\")\n",
    "print(f\"Mean: {z_scores.mean():.3f}\")\n",
    "print(f\"Median: {np.median(z_scores):.3f}\")\n",
    "print(f\"Max: {z_scores.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293",
   "metadata": {
    "id": "3da0369e"
   },
   "outputs": [],
   "source": [
    "# Visualize Z-Score Distribution\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.hist(anomaly_df[anomaly_df['anomaly_zscore']==0]['z_score'], bins=50,\n",
    "        alpha=0.6, color='steelblue', label='Normal', edgecolor='black')\n",
    "ax.hist(anomaly_df[anomaly_df['anomaly_zscore']==1]['z_score'], bins=30,\n",
    "        alpha=0.9, color='red', label='Anomalies', edgecolor='black')\n",
    "ax.axvline(z_threshold, color='orange', linestyle='--', linewidth=3,\n",
    "           label=f'Threshold (Z={z_threshold})', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Z-Score', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Z-Score Distribution', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right', fontsize=10)\n",
    "ax.grid(alpha=0.3, linestyle=':')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "zscore_anomalies = anomaly_df[anomaly_df['anomaly_zscore'] == 1]\n",
    "print(f\"✓ Z-Score visualization complete: {len(zscore_anomalies)} anomalies detected ({len(zscore_anomalies)/len(anomaly_df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294",
   "metadata": {
    "id": "279"
   },
   "source": [
    "## 3. IQR Method\n",
    "\n",
    "Quartile-based outlier detection using interquartile range.\n",
    "\n",
    "**Bounds:** $Q_1 - 1.5 \\times IQR$ and $Q_3 + 1.5 \\times IQR$  \n",
    "**Detection:** Values outside bounds are flagged as anomalies\n",
    "\n",
    "**Advantages:**\n",
    "- Robust to extreme outliers\n",
    "- Distribution-free\n",
    "- Easy to visualize\n",
    "\n",
    "**Limitations:**\n",
    "- Fixed threshold multiplier\n",
    "- Univariate approach\n",
    "- Treats positive/negative residuals equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295",
   "metadata": {
    "id": "d358c500"
   },
   "outputs": [],
   "source": [
    "# IQR (Interquartile Range) Method\n",
    "Q1 = residuals.quantile(0.25)\n",
    "Q3 = residuals.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define outlier bounds\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Flag anomalies outside IQR bounds\n",
    "anomaly_df['anomaly_iqr'] = ((residuals < lower_bound) | (residuals > upper_bound)).astype(int)\n",
    "\n",
    "print(f\"\\nIQR Method:\")\n",
    "print(f\"Q1: {Q1:.2f}, Q3: {Q3:.2f}, IQR: {IQR:.2f}\")\n",
    "print(f\"Lower bound: {lower_bound:.2f}, Upper bound: {upper_bound:.2f}\")\n",
    "print(f\"Anomalies detected: {anomaly_df['anomaly_iqr'].sum()} ({anomaly_df['anomaly_iqr'].mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296",
   "metadata": {
    "id": "2f5807df"
   },
   "outputs": [],
   "source": [
    "# Visualize IQR Detection Results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Left: Scatter plot\n",
    "iqr_anomalies = anomaly_df[anomaly_df['anomaly_iqr'] == 1]\n",
    "iqr_normal = anomaly_df[anomaly_df['anomaly_iqr'] == 0]\n",
    "\n",
    "ax1.scatter(iqr_normal['predicted_duration'], iqr_normal[TARGET_COL],\n",
    "            alpha=0.2, s=10, color='green', label=f'Normal (n={len(iqr_normal):,})',\n",
    "            rasterized=True)\n",
    "ax1.scatter(iqr_anomalies['predicted_duration'], iqr_anomalies[TARGET_COL],\n",
    "            alpha=0.9, s=80, color='orange', marker='s', linewidths=2,\n",
    "            label=f'IQR Anomalies (n={len(iqr_anomalies):,})')\n",
    "\n",
    "max_val = max(anomaly_df[TARGET_COL].max(), anomaly_df['predicted_duration'].max())\n",
    "ax1.plot([0, max_val], [0, max_val], 'k--', linewidth=2, alpha=0.6, label='Perfect Prediction')\n",
    "\n",
    "ax1.set_xlabel('Predicted Duration (hours)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Actual Duration (hours)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('IQR Method: Anomaly Detection', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='upper left', fontsize=10)\n",
    "ax1.grid(alpha=0.3, linestyle=':')\n",
    "\n",
    "# Right: Boxplot showing IQR bounds\n",
    "data_iqr = [iqr_normal['residual'], iqr_anomalies['residual']]\n",
    "bp = ax2.boxplot(data_iqr, labels=['Normal', 'IQR Anomalies'], patch_artist=True,\n",
    "                  widths=0.6, showmeans=True, meanline=True)\n",
    "\n",
    "# Customize colors\n",
    "bp['boxes'][0].set_facecolor('green')\n",
    "bp['boxes'][0].set_alpha(0.5)\n",
    "bp['boxes'][1].set_facecolor('orange')\n",
    "bp['boxes'][1].set_alpha(0.7)\n",
    "\n",
    "# Add IQR bounds as horizontal lines\n",
    "ax2.axhline(lower_bound, color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Lower: {lower_bound:.1f}h', alpha=0.7)\n",
    "ax2.axhline(upper_bound, color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Upper: {upper_bound:.1f}h', alpha=0.7)\n",
    "ax2.axhline(0, color='black', linestyle='-', linewidth=1, alpha=0.5)\n",
    "\n",
    "ax2.set_ylabel('Residual (hours)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('IQR Method: Residual Distribution', fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc='upper right', fontsize=10)\n",
    "ax2.grid(alpha=0.3, axis='y', linestyle=':')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ IQR visualization complete: {len(iqr_anomalies):,} anomalies detected ({len(iqr_anomalies)/len(anomaly_df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297",
   "metadata": {
    "id": "280"
   },
   "source": [
    "## 4. Isolation Forest Method\n",
    "\n",
    "Unsupervised machine learning algorithm designed for anomaly detection using random decision trees.\n",
    "\n",
    "**Core Principle:** Anomalies are easier to isolate from normal points  \n",
    "**Score:** Based on path length (shorter paths = more anomalous)\n",
    "\n",
    "**Advantages:**\n",
    "- Multivariate (considers multiple features)\n",
    "- Distribution-free\n",
    "- Detects novel patterns\n",
    "- Scalable\n",
    "\n",
    "**Limitations:**\n",
    "- Less interpretable\n",
    "- Requires parameter tuning\n",
    "- More computationally intensive\n",
    "\n",
    "**Implementation:** Tests contamination values (0.005-0.15) to find best agreement with statistical methods. Uses 100 trees for stable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298",
   "metadata": {
    "id": "449fad86"
   },
   "outputs": [],
   "source": [
    "# Isolation Forest for anomaly detection\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Extract time features from date if they don't exist\n",
    "if 'hour' not in anomaly_df.columns and 'date' in anomaly_df.columns:\n",
    "    anomaly_df['hour'] = pd.to_datetime(anomaly_df['date']).dt.hour\n",
    "    anomaly_df['day_of_week'] = pd.to_datetime(anomaly_df['date']).dt.dayofweek\n",
    "    anomaly_df['month'] = pd.to_datetime(anomaly_df['date']).dt.month\n",
    "\n",
    "# Select numerical features for Isolation Forest\n",
    "numerical_features = ['residual', 'absolute_residual']\n",
    "\n",
    "# Add time features if they exist\n",
    "if 'hour' in anomaly_df.columns:\n",
    "    numerical_features.append('hour')\n",
    "if 'day_of_week' in anomaly_df.columns:\n",
    "    numerical_features.append('day_of_week')\n",
    "if 'month' in anomaly_df.columns:\n",
    "    numerical_features.append('month')\n",
    "\n",
    "# Add engineered features if they exist\n",
    "if 'etab_load_1h' in anomaly_df.columns:\n",
    "    numerical_features.append('etab_load_1h')\n",
    "if 'route_load_1h' in anomaly_df.columns:\n",
    "    numerical_features.append('route_load_1h')\n",
    "if 'time_since_first_scan' in anomaly_df.columns:\n",
    "    numerical_features.append('time_since_first_scan')\n",
    "\n",
    "# Prepare feature matrix\n",
    "X_iso = anomaly_df[numerical_features].fillna(0)\n",
    "\n",
    "print(f\"Features used for Isolation Forest: {numerical_features}\")\n",
    "print(f\"Feature matrix shape: {X_iso.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299",
   "metadata": {
    "id": "842a52be"
   },
   "outputs": [],
   "source": [
    "# Train Isolation Forest - Find best contamination by agreement with Z-Score and IQR\n",
    "\n",
    "# Test different contamination values\n",
    "contamination_values = [0.005, 0.01, 0.02, 0.03, 0.05, 0.07, 0.10, 0.15]\n",
    "best_contamination = None\n",
    "best_agreement_score = 0\n",
    "results_comparison = []\n",
    "\n",
    "print(\"Testing contamination values for best agreement with Z-Score and IQR...\\n\")\n",
    "\n",
    "for cont in contamination_values:\n",
    "    # Train Isolation Forest with current contamination\n",
    "    iso_temp = IsolationForest(\n",
    "        contamination=cont,\n",
    "        random_state=42,\n",
    "        n_estimators=100\n",
    "    )\n",
    "    iso_pred_temp = iso_temp.fit_predict(X_iso)\n",
    "    anomaly_if_temp = (iso_pred_temp == -1).astype(int)\n",
    "\n",
    "    # Calculate agreement with Z-Score and IQR\n",
    "    zscore_agreement = ((anomaly_df['anomaly_zscore'] == 1) & (anomaly_if_temp == 1)).sum()\n",
    "    iqr_agreement = ((anomaly_df['anomaly_iqr'] == 1) & (anomaly_if_temp == 1)).sum()\n",
    "\n",
    "    # Agreement with either method (union)\n",
    "    either_agreement = ((anomaly_df['anomaly_zscore'] == 1) | (anomaly_df['anomaly_iqr'] == 1)) & (anomaly_if_temp == 1)\n",
    "    either_count = either_agreement.sum()\n",
    "\n",
    "    # All three agree\n",
    "    all_agree = ((anomaly_df['anomaly_zscore'] == 1) & (anomaly_df['anomaly_iqr'] == 1) & (anomaly_if_temp == 1)).sum()\n",
    "\n",
    "    # Calculate agreement score: weighted combination\n",
    "    # Higher weight for all-three agreement, moderate for either agreement\n",
    "    agreement_score = (all_agree * 3) + (either_count * 1)\n",
    "\n",
    "    total_if_anomalies = anomaly_if_temp.sum()\n",
    "\n",
    "    print(f\"Contamination: {cont:.3f}\")\n",
    "    print(f\"  IF Anomalies: {total_if_anomalies} ({total_if_anomalies/len(anomaly_df)*100:.2f}%)\")\n",
    "    print(f\"  Agreement with Z-Score: {zscore_agreement}\")\n",
    "    print(f\"  Agreement with IQR: {iqr_agreement}\")\n",
    "    print(f\"  Agreement with either: {either_count}\")\n",
    "    print(f\"  All three agree: {all_agree}\")\n",
    "    print(f\"  Agreement Score: {agreement_score}\")\n",
    "    print()\n",
    "\n",
    "    results_comparison.append({\n",
    "        'contamination': cont,\n",
    "        'if_anomalies': total_if_anomalies,\n",
    "        'zscore_agreement': zscore_agreement,\n",
    "        'iqr_agreement': iqr_agreement,\n",
    "        'either_agreement': either_count,\n",
    "        'all_three_agree': all_agree,\n",
    "        'agreement_score': agreement_score\n",
    "    })\n",
    "\n",
    "    # Track best contamination\n",
    "    if agreement_score > best_agreement_score:\n",
    "        best_agreement_score = agreement_score\n",
    "        best_contamination = cont\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Best contamination value: {best_contamination}\")\n",
    "print(f\"Best agreement score: {best_agreement_score}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Train final model with best contamination\n",
    "iso_forest = IsolationForest(\n",
    "    contamination=best_contamination,\n",
    "    random_state=42,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "iso_predictions = iso_forest.fit_predict(X_iso)\n",
    "\n",
    "# Convert to binary (1 for anomaly, 0 for normal)\n",
    "anomaly_df['anomaly_iforest'] = (iso_predictions == -1).astype(int)\n",
    "\n",
    "# Get anomaly scores (lower scores = more anomalous)\n",
    "anomaly_df['iforest_score'] = iso_forest.score_samples(X_iso)\n",
    "\n",
    "print(f\"\\nFinal Isolation Forest Results:\")\n",
    "print(f\"Contamination parameter: {best_contamination}\")\n",
    "print(f\"Anomalies detected: {anomaly_df['anomaly_iforest'].sum()} ({anomaly_df['anomaly_iforest'].mean()*100:.2f}%)\")\n",
    "print(f\"\\nAnomaly score statistics:\")\n",
    "print(f\"Mean: {anomaly_df['iforest_score'].mean():.3f}\")\n",
    "print(f\"Min (most anomalous): {anomaly_df['iforest_score'].min():.3f}\")\n",
    "print(f\"Max (least anomalous): {anomaly_df['iforest_score'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300",
   "metadata": {
    "id": "8dc2649d"
   },
   "outputs": [],
   "source": [
    "# Visualize Isolation Forest Detection Results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Left: Scatter plot\n",
    "iforest_anomalies = anomaly_df[anomaly_df['anomaly_iforest'] == 1]\n",
    "iforest_normal = anomaly_df[anomaly_df['anomaly_iforest'] == 0]\n",
    "\n",
    "ax1.scatter(iforest_normal['predicted_duration'], iforest_normal[TARGET_COL],\n",
    "            alpha=0.2, s=10, color='purple', label=f'Normal (n={len(iforest_normal):,})',\n",
    "            rasterized=True)\n",
    "ax1.scatter(iforest_anomalies['predicted_duration'], iforest_anomalies[TARGET_COL],\n",
    "            alpha=0.9, s=80, color='magenta', marker='^', linewidths=2,\n",
    "            label=f'IF Anomalies (n={len(iforest_anomalies):,})')\n",
    "\n",
    "max_val = max(anomaly_df[TARGET_COL].max(), anomaly_df['predicted_duration'].max())\n",
    "ax1.plot([0, max_val], [0, max_val], 'k--', linewidth=2, alpha=0.6, label='Perfect Prediction')\n",
    "\n",
    "ax1.set_xlabel('Predicted Duration (hours)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Actual Duration (hours)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Isolation Forest Method: Anomaly Detection', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='upper left', fontsize=10)\n",
    "ax1.grid(alpha=0.3, linestyle=':')\n",
    "\n",
    "# Right: Anomaly score distribution\n",
    "ax2.hist(iforest_normal['iforest_score'], bins=50, alpha=0.6, color='purple',\n",
    "         label='Normal', edgecolor='black')\n",
    "ax2.hist(iforest_anomalies['iforest_score'], bins=30, alpha=0.9, color='magenta',\n",
    "         label='Anomalies', edgecolor='black')\n",
    "\n",
    "# Add threshold line (approximate decision boundary)\n",
    "threshold = anomaly_df[anomaly_df['anomaly_iforest']==1]['iforest_score'].max()\n",
    "ax2.axvline(threshold, color='red', linestyle='--', linewidth=3,\n",
    "            label=f'Decision Boundary ≈ {threshold:.3f}', alpha=0.8)\n",
    "\n",
    "ax2.set_xlabel('Isolation Forest Anomaly Score', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Anomaly Score Distribution (lower = more anomalous)', fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc='upper left', fontsize=10)\n",
    "ax2.grid(alpha=0.3, linestyle=':')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Isolation Forest visualization complete: {len(iforest_anomalies):,} anomalies detected ({len(iforest_anomalies)/len(anomaly_df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301",
   "metadata": {
    "id": "281"
   },
   "source": [
    "## 5. Ensemble Method & Comparison\n",
    "\n",
    "Combine all three methods for robust anomaly detection with confidence scoring.\n",
    "\n",
    "### Ensemble Score Calculation\n",
    "\n",
    "$$\\text{Anomaly Score} = \\text{Z-Score Flag} + \\text{IQR Flag} + \\text{Isolation Forest Flag}$$\n",
    "\n",
    "**Score Interpretation:**\n",
    "- **0**: No methods detected anomaly (normal)\n",
    "- **1**: One method detected (weak signal)\n",
    "- **2**: Two methods agree (moderate confidence)\n",
    "- **3**: All methods agree (high confidence)\n",
    "\n",
    "**Classification:** `is_anomaly = 1` if score ≥ 2\n",
    "\n",
    "**Advantages:**\n",
    "- Reduced false positives through consensus\n",
    "- Probabilistic confidence scoring\n",
    "- Combines statistical and ML strengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302",
   "metadata": {
    "id": "7141697b"
   },
   "outputs": [],
   "source": [
    "# Ensemble Scoring: Combine all methods\n",
    "# Calculate ensemble anomaly score (average of all methods)\n",
    "anomaly_df['anomaly_score'] = (\n",
    "    anomaly_df['anomaly_zscore'] +\n",
    "    anomaly_df['anomaly_iqr'] +\n",
    "    anomaly_df['anomaly_iforest']\n",
    ") / 3\n",
    "\n",
    "# High-confidence anomalies: flagged by at least 2 out of 3 methods\n",
    "anomaly_df['is_anomaly'] = (anomaly_df['anomaly_score'] >= 0.67).astype(int)\n",
    "\n",
    "print(f\"\\nEnsemble Anomaly Detection Results:\")\n",
    "print(f\"\\nMethod comparison:\")\n",
    "print(f\"Z-Score anomalies: {anomaly_df['anomaly_zscore'].sum()}\")\n",
    "print(f\"IQR anomalies: {anomaly_df['anomaly_iqr'].sum()}\")\n",
    "print(f\"Isolation Forest anomalies: {anomaly_df['anomaly_iforest'].sum()}\")\n",
    "print(f\"\\nEnsemble (≥2 methods agree): {anomaly_df['is_anomaly'].sum()} ({anomaly_df['is_anomaly'].mean()*100:.2f}%)\")\n",
    "\n",
    "# Show distribution of ensemble scores\n",
    "print(f\"\\nEnsemble score distribution:\")\n",
    "print(anomaly_df['anomaly_score'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303",
   "metadata": {
    "id": "8c61d732"
   },
   "outputs": [],
   "source": [
    "# Comprehensive Method Comparison Visualization\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Prepare data\n",
    "methods_data = {\n",
    "    'Z-Score': anomaly_df['anomaly_zscore'],\n",
    "    'IQR': anomaly_df['anomaly_iqr'],\n",
    "    'Isolation Forest': anomaly_df['anomaly_iforest'],\n",
    "    'Ensemble': anomaly_df['is_anomaly']\n",
    "}\n",
    "\n",
    "colors_methods = {'Z-Score': 'red', 'IQR': 'orange', 'Isolation Forest': 'magenta', 'Ensemble': 'crimson'}\n",
    "\n",
    "# Top row: Actual vs Predicted for each method\n",
    "for idx, (method, mask_col) in enumerate(methods_data.items()):\n",
    "    ax = fig.add_subplot(gs[0, idx if idx < 3 else 0])\n",
    "\n",
    "    method_anomalies = anomaly_df[mask_col == 1]\n",
    "    method_normal = anomaly_df[mask_col == 0]\n",
    "\n",
    "    ax.scatter(method_normal['predicted_duration'], method_normal[TARGET_COL],\n",
    "               alpha=0.15, s=5, color='lightblue', label=f'Normal')\n",
    "    ax.scatter(method_anomalies['predicted_duration'], method_anomalies[TARGET_COL],\n",
    "               alpha=0.9, s=50, color=colors_methods[method], marker='x', linewidths=2,\n",
    "               label=f'Anomalies (n={len(method_anomalies)})')\n",
    "\n",
    "    max_val = max(anomaly_df[TARGET_COL].max(), anomaly_df['predicted_duration'].max())\n",
    "    ax.plot([0, max_val], [0, max_val], 'k--', linewidth=1.5, alpha=0.5)\n",
    "\n",
    "    ax.set_xlabel('Predicted (h)', fontsize=10, fontweight='bold')\n",
    "    ax.set_ylabel('Actual (h)', fontsize=10, fontweight='bold')\n",
    "    ax.set_title(f'{method}\\n{len(method_anomalies)} anomalies ({len(method_anomalies)/len(anomaly_df)*100:.2f}%)',\n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='upper left', fontsize=8)\n",
    "    ax.grid(alpha=0.2, linestyle=':')\n",
    "\n",
    "# Middle row: Venn diagram comparison and overlap analysis\n",
    "ax_venn = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "# Calculate overlaps\n",
    "zscore_set = set(anomaly_df[anomaly_df['anomaly_zscore']==1].index)\n",
    "iqr_set = set(anomaly_df[anomaly_df['anomaly_iqr']==1].index)\n",
    "iforest_set = set(anomaly_df[anomaly_df['anomaly_iforest']==1].index)\n",
    "\n",
    "overlap_all = zscore_set & iqr_set & iforest_set\n",
    "overlap_zs_iqr = (zscore_set & iqr_set) - iforest_set\n",
    "overlap_zs_if = (zscore_set & iforest_set) - iqr_set\n",
    "overlap_iqr_if = (iqr_set & iforest_set) - zscore_set\n",
    "only_zs = zscore_set - iqr_set - iforest_set\n",
    "only_iqr = iqr_set - zscore_set - iforest_set\n",
    "only_if = iforest_set - zscore_set - iqr_set\n",
    "\n",
    "overlap_data = {\n",
    "    'All 3 methods': len(overlap_all),\n",
    "    'Z-Score & IQR': len(overlap_zs_iqr),\n",
    "    'Z-Score & IF': len(overlap_zs_if),\n",
    "    'IQR & IF': len(overlap_iqr_if),\n",
    "    'Only Z-Score': len(only_zs),\n",
    "    'Only IQR': len(only_iqr),\n",
    "    'Only IF': len(only_if)\n",
    "}\n",
    "\n",
    "bars = ax_venn.barh(list(overlap_data.keys()), list(overlap_data.values()),\n",
    "                     color=['darkred', 'red', 'orange', 'yellow', 'lightcoral', 'lightyellow', 'pink'],\n",
    "                     edgecolor='black', linewidth=1.5)\n",
    "\n",
    "for i, (bar, val) in enumerate(zip(bars, overlap_data.values())):\n",
    "    ax_venn.text(val + 10, bar.get_y() + bar.get_height()/2, f'{val}',\n",
    "                 va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax_venn.set_xlabel('Number of Anomalies', fontsize=11, fontweight='bold')\n",
    "ax_venn.set_title('Method Overlap Analysis', fontsize=13, fontweight='bold')\n",
    "ax_venn.grid(alpha=0.3, axis='x', linestyle=':')\n",
    "\n",
    "# Middle center: Method agreement matrix\n",
    "ax_matrix = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "methods_list = ['Z-Score', 'IQR', 'IF']\n",
    "agreement_matrix = np.zeros((3, 3))\n",
    "\n",
    "for i, method1 in enumerate(['anomaly_zscore', 'anomaly_iqr', 'anomaly_iforest']):\n",
    "    for j, method2 in enumerate(['anomaly_zscore', 'anomaly_iqr', 'anomaly_iforest']):\n",
    "        if i == j:\n",
    "            agreement_matrix[i, j] = anomaly_df[method1].sum()\n",
    "        else:\n",
    "            agreement_matrix[i, j] = ((anomaly_df[method1] == 1) & (anomaly_df[method2] == 1)).sum()\n",
    "\n",
    "im = ax_matrix.imshow(agreement_matrix, cmap='Reds', aspect='auto')\n",
    "ax_matrix.set_xticks(range(3))\n",
    "ax_matrix.set_yticks(range(3))\n",
    "ax_matrix.set_xticklabels(methods_list, fontsize=10)\n",
    "ax_matrix.set_yticklabels(methods_list, fontsize=10)\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        text = ax_matrix.text(j, i, int(agreement_matrix[i, j]),\n",
    "                              ha=\"center\", va=\"center\", color=\"white\" if agreement_matrix[i, j] > 500 else \"black\",\n",
    "                              fontsize=12, fontweight='bold')\n",
    "\n",
    "ax_matrix.set_title('Agreement Matrix\\n(overlapping detections)', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax_matrix, fraction=0.046, pad=0.04)\n",
    "\n",
    "# Middle right: Ensemble score distribution\n",
    "ax_ensemble = fig.add_subplot(gs[1, 2])\n",
    "\n",
    "score_counts = anomaly_df['anomaly_score'].value_counts().sort_index()\n",
    "score_labels = ['0.0\\n(None)', '0.33\\n(1/3)', '0.67\\n(2/3)', '1.0\\n(All)']\n",
    "colors_score = ['green', 'gold', 'orange', 'crimson']\n",
    "\n",
    "bars_score = ax_ensemble.bar(range(len(score_counts)), score_counts.values,\n",
    "                               color=colors_score, edgecolor='black', linewidth=2, alpha=0.8)\n",
    "\n",
    "for bar, count in zip(bars_score, score_counts.values):\n",
    "    height = bar.get_height()\n",
    "    percentage = (count / len(anomaly_df)) * 100\n",
    "    ax_ensemble.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                     f'{count:,}\\n({percentage:.1f}%)',\n",
    "                     ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax_ensemble.set_xticks(range(len(score_counts)))\n",
    "ax_ensemble.set_xticklabels(score_labels, fontsize=10)\n",
    "ax_ensemble.set_ylabel('Count', fontsize=11, fontweight='bold')\n",
    "ax_ensemble.set_title('Ensemble Score Distribution\\n(method agreement)', fontsize=12, fontweight='bold')\n",
    "ax_ensemble.grid(alpha=0.3, axis='y', linestyle=':')\n",
    "\n",
    "# Bottom row: Residual distributions for each method\n",
    "for idx, (method, mask_col) in enumerate(methods_data.items()):\n",
    "    if idx >= 3:  # Skip ensemble in this row\n",
    "        break\n",
    "\n",
    "    ax = fig.add_subplot(gs[2, idx])\n",
    "\n",
    "    method_anomalies = anomaly_df[mask_col == 1]\n",
    "    method_normal = anomaly_df[mask_col == 0]\n",
    "\n",
    "    ax.hist(method_normal['residual'], bins=50, alpha=0.5, color='steelblue',\n",
    "            label='Normal', edgecolor='black')\n",
    "    ax.hist(method_anomalies['residual'], bins=30, alpha=0.8, color=colors_methods[method],\n",
    "            label='Anomalies', edgecolor='black')\n",
    "    ax.axvline(0, color='black', linestyle='--', linewidth=2, alpha=0.6)\n",
    "\n",
    "    ax.set_xlabel('Residual (hours)', fontsize=10, fontweight='bold')\n",
    "    ax.set_ylabel('Frequency', fontsize=10, fontweight='bold')\n",
    "    ax.set_title(f'{method}: Residual Distribution', fontsize=11, fontweight='bold')\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "    ax.grid(alpha=0.3, linestyle=':')\n",
    "\n",
    "plt.suptitle('Comprehensive Anomaly Detection Method Comparison', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.show()\n",
    "\n",
    "print(\"COMPREHENSIVE METHOD COMPARISON SUMMARY\")\n",
    "print(f\"\\nIndividual Method Results:\")\n",
    "print(f\"  Z-Score:          {anomaly_df['anomaly_zscore'].sum():,} anomalies ({anomaly_df['anomaly_zscore'].mean()*100:.2f}%)\")\n",
    "print(f\"  IQR:              {anomaly_df['anomaly_iqr'].sum():,} anomalies ({anomaly_df['anomaly_iqr'].mean()*100:.2f}%)\")\n",
    "print(f\"  Isolation Forest: {anomaly_df['anomaly_iforest'].sum():,} anomalies ({anomaly_df['anomaly_iforest'].mean()*100:.2f}%)\")\n",
    "print(f\"\\nMethod Overlap:\")\n",
    "print(f\"  All 3 agree:      {len(overlap_all)} anomalies\")\n",
    "print(f\"  2 methods agree:  {len(overlap_zs_iqr) + len(overlap_zs_if) + len(overlap_iqr_if)} anomalies\")\n",
    "print(f\"  Single method:    {len(only_zs) + len(only_iqr) + len(only_if)} anomalies\")\n",
    "print(f\"\\nFINAL ENSEMBLE RESULT: {anomaly_df['is_anomaly'].sum()} anomalies ({anomaly_df['is_anomaly'].mean()*100:.2f}%)\")\n",
    "print(f\"   (Using ≥2 methods agreement threshold)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304",
   "metadata": {
    "id": "282"
   },
   "source": [
    "## 6. Country & Establishment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305",
   "metadata": {
    "id": "a2904165"
   },
   "outputs": [],
   "source": [
    "# Anomaly Characterization Summary\n",
    "anomalies = anomaly_df[anomaly_df['is_anomaly'] == 1]\n",
    "normal = anomaly_df[anomaly_df['is_anomaly'] == 0]\n",
    "\n",
    "print(f\"Total Samples: {len(anomaly_df):,}\")\n",
    "print(f\"Anomalies: {len(anomalies)} ({len(anomalies)/len(anomaly_df)*100:.2f}%)\")\n",
    "print(f\"Normal: {len(normal)} ({len(normal)/len(anomaly_df)*100:.2f}%)\")\n",
    "\n",
    "if len(anomalies) > 0:\n",
    "    print(f\"\\nResidual Analysis:\")\n",
    "    print(f\"  Anomalies: μ={anomalies['residual'].mean():.1f}h, σ={anomalies['residual'].std():.1f}h\")\n",
    "    print(f\"  Normal: μ={normal['residual'].mean():.1f}h, σ={normal['residual'].std():.1f}h\")\n",
    "\n",
    "    print(f\"\\nAnomaly Types:\")\n",
    "    over_pred = anomalies[anomalies['residual'] < 0]\n",
    "    under_pred = anomalies[anomalies['residual'] > 0]\n",
    "    print(f\"  Over-predictions (faster): {len(over_pred)} ({len(over_pred)/len(anomalies)*100:.1f}%)\")\n",
    "    print(f\"  Under-predictions (slower): {len(under_pred)} ({len(under_pred)/len(anomalies)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306",
   "metadata": {
    "id": "81569162"
   },
   "outputs": [],
   "source": [
    "# Enhanced Boxplot with Statistical Annotations\n",
    "fig, (ax_box, ax_stats) = plt.subplots(1, 2, figsize=(16, 7), gridspec_kw={'width_ratios': [2, 1]})\n",
    "\n",
    "# Left: Boxplot\n",
    "data_to_plot = [normal['residual'], anomalies['residual']]\n",
    "box = ax_box.boxplot(data_to_plot, labels=['Normal', 'Anomaly'], patch_artist=True,\n",
    "                     widths=0.6, showmeans=True, meanline=True)\n",
    "\n",
    "# Customize colors and styling\n",
    "colors = ['steelblue', 'crimson']\n",
    "for patch, color in zip(box['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "    patch.set_linewidth(2)\n",
    "\n",
    "for element in ['whiskers', 'fliers', 'means', 'medians', 'caps']:\n",
    "    plt.setp(box[element], linewidth=2)\n",
    "\n",
    "ax_box.axhline(0, color='black', linestyle='--', linewidth=2, alpha=0.7, label='Perfect Prediction')\n",
    "ax_box.set_ylabel('Residual (Actual - Predicted) in Hours', fontsize=13, fontweight='bold')\n",
    "ax_box.set_title('Residual Distribution Comparison', fontsize=15, fontweight='bold')\n",
    "ax_box.grid(alpha=0.3, axis='y', linestyle=':')\n",
    "ax_box.legend(fontsize=10)\n",
    "\n",
    "# Add individual data points for anomalies (semi-transparent)\n",
    "ax_box.scatter(np.ones(len(anomalies)) * 2 + np.random.normal(0, 0.04, len(anomalies)),\n",
    "               anomalies['residual'], alpha=0.3, s=20, color='darkred', zorder=3)\n",
    "\n",
    "# Right: Statistical Summary Table\n",
    "ax_stats.axis('tight')\n",
    "ax_stats.axis('off')\n",
    "\n",
    "# Calculate comprehensive statistics\n",
    "stats_data = [\n",
    "    ['Metric', 'Normal', 'Anomaly', 'Difference'],\n",
    "    ['Count', f\"{len(normal):,}\", f\"{len(anomalies)}\", f\"{len(normal)-len(anomalies):,}\"],\n",
    "    ['Mean (μ)', f\"{normal['residual'].mean():.2f}h\", f\"{anomalies['residual'].mean():.2f}h\",\n",
    "     f\"{anomalies['residual'].mean() - normal['residual'].mean():.2f}h\"],\n",
    "    ['Median', f\"{normal['residual'].median():.2f}h\", f\"{anomalies['residual'].median():.2f}h\",\n",
    "     f\"{anomalies['residual'].median() - normal['residual'].median():.2f}h\"],\n",
    "    ['Std Dev (σ)', f\"{normal['residual'].std():.2f}h\", f\"{anomalies['residual'].std():.2f}h\",\n",
    "     f\"{anomalies['residual'].std() - normal['residual'].std():.2f}h\"],\n",
    "    ['Min', f\"{normal['residual'].min():.2f}h\", f\"{anomalies['residual'].min():.2f}h\", '-'],\n",
    "    ['Max', f\"{normal['residual'].max():.2f}h\", f\"{anomalies['residual'].max():.2f}h\", '-'],\n",
    "    ['Q1 (25%)', f\"{normal['residual'].quantile(0.25):.2f}h\", f\"{anomalies['residual'].quantile(0.25):.2f}h\", '-'],\n",
    "    ['Q3 (75%)', f\"{normal['residual'].quantile(0.75):.2f}h\", f\"{anomalies['residual'].quantile(0.75):.2f}h\", '-'],\n",
    "    ['MAE', f\"{np.abs(normal['residual']).mean():.2f}h\", f\"{np.abs(anomalies['residual']).mean():.2f}h\",\n",
    "     f\"{np.abs(anomalies['residual']).mean() / np.abs(normal['residual']).mean():.1f}x\"]\n",
    "]\n",
    "\n",
    "table = ax_stats.table(cellText=stats_data, cellLoc='left', loc='center',\n",
    "                       colWidths=[0.25, 0.25, 0.25, 0.25])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "# Style the header row\n",
    "for i in range(4):\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "    table[(0, i)].set_facecolor('#4472C4')\n",
    "\n",
    "# Alternate row colors\n",
    "for i in range(1, len(stats_data)):\n",
    "    for j in range(4):\n",
    "        if i % 2 == 0:\n",
    "            table[(i, j)].set_facecolor('#E7E6E6')\n",
    "        table[(i, j)].set_linewidth(1.5)\n",
    "        table[(i, j)].set_edgecolor('black')\n",
    "\n",
    "ax_stats.set_title('Statistical Summary', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.suptitle(f'Anomaly Analysis: {len(anomalies)} Anomalies out of {len(anomaly_df):,} Total Samples',\n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Statistical comparison complete: Anomalies show {anomalies['residual'].mean() / normal['residual'].mean():.1f}x higher mean residual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307",
   "metadata": {
    "id": "4514f47c"
   },
   "outputs": [],
   "source": [
    "# Route & Establishment Analysis\n",
    "if len(anomalies) > 0 and 'etablissement_postal' in anomalies.columns:\n",
    "    print(\"\\nRoute & Establishment Analysis\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Top 5 Origin Establishments\n",
    "    print(\"\\nTop 5 Origin Establishments:\")\n",
    "    etab_counts = anomalies['etablissement_postal'].value_counts().head(5)\n",
    "    for rank, (etab, count) in enumerate(etab_counts.items(), 1):\n",
    "        pct = (count / len(anomalies)) * 100\n",
    "        etab_data = anomalies[anomalies['etablissement_postal'] == etab]\n",
    "        print(f\"  {rank}. {etab}: {count} anomalies ({pct:.1f}%), Avg Error: {etab_data['absolute_residual'].mean():.1f}h\")\n",
    "\n",
    "    # Top 5 Destination Establishments\n",
    "    if 'next_etablissement_postal' in anomalies.columns:\n",
    "        print(\"\\nTop 5 Destination Establishments:\")\n",
    "        next_etab_counts = anomalies['next_etablissement_postal'].value_counts().head(5)\n",
    "        for rank, (etab, count) in enumerate(next_etab_counts.items(), 1):\n",
    "            pct = (count / len(anomalies)) * 100\n",
    "            etab_data = anomalies[anomalies['next_etablissement_postal'] == etab]\n",
    "            print(f\"  {rank}. {etab}: {count} anomalies ({pct:.1f}%), Avg Error: {etab_data['absolute_residual'].mean():.1f}h\")\n",
    "\n",
    "        # Top 5 Problematic Routes\n",
    "        print(\"\\nTop 5 Most Problematic Routes:\")\n",
    "        anomalies['route_pair'] = anomalies['etablissement_postal'].astype(str) + ' → ' + anomalies['next_etablissement_postal'].astype(str)\n",
    "        route_counts = anomalies['route_pair'].value_counts().head(5)\n",
    "        for rank, (route, count) in enumerate(route_counts.items(), 1):\n",
    "            route_data = anomalies[anomalies['route_pair'] == route]\n",
    "            print(f\"  {rank}. {route}\")\n",
    "            print(f\"      {count} anomalies ({count/len(anomalies)*100:.1f}%), Avg Error: {route_data['absolute_residual'].mean():.1f}h\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308",
   "metadata": {
    "id": "ecd9aae7"
   },
   "outputs": [],
   "source": [
    "# Country Analysis\n",
    "if len(anomalies) > 0 and 'MAILITM_FID' in anomalies.columns:\n",
    "    print(\"\\nCountry Analysis\")\n",
    "\n",
    "    def parse_mailitm_fid(id_str):\n",
    "        if pd.isna(id_str) or str(id_str) == 'nan':\n",
    "            return None, None, None\n",
    "        return str(id_str)[0:2], str(id_str)[2:11], str(id_str)[11:14].strip()\n",
    "\n",
    "    anomalies_country = anomalies.copy()\n",
    "    anomalies_country[['service_ind', 'serial_num', 'country_code']] = anomalies_country['MAILITM_FID'].apply(\n",
    "        lambda x: pd.Series(parse_mailitm_fid(x))\n",
    "    )\n",
    "\n",
    "    anomaly_df_country = anomaly_df.copy()\n",
    "    anomaly_df_country[['service_ind', 'serial_num', 'country_code']] = anomaly_df_country['MAILITM_FID'].apply(\n",
    "        lambda x: pd.Series(parse_mailitm_fid(x))\n",
    "    )\n",
    "\n",
    "    country_names = {\n",
    "        'DZ': 'Algeria', 'FR': 'France', 'US': 'United States', 'AE': 'UAE', 'GB': 'United Kingdom',\n",
    "        'CA': 'Canada', 'DE': 'Germany', 'IT': 'Italy', 'ES': 'Spain', 'CN': 'China'\n",
    "    }\n",
    "\n",
    "    print(\"\\nTop 5 Countries with Most Anomalies:\")\n",
    "    country_anomaly_counts = anomalies_country['country_code'].value_counts().head(5)\n",
    "\n",
    "    for rank, (country_code, anom_count) in enumerate(country_anomaly_counts.items(), 1):\n",
    "        country_name = country_names.get(country_code, country_code)\n",
    "        total = anomaly_df_country[anomaly_df_country['country_code'] == country_code].shape[0]\n",
    "        rate = (anom_count / total) * 100 if total > 0 else 0\n",
    "        country_data = anomalies_country[anomalies_country['country_code'] == country_code]\n",
    "        avg_error = country_data['absolute_residual'].mean()\n",
    "\n",
    "        print(f\"  {rank}. {country_name} ({country_code}): {anom_count} anomalies ({rate:.2f}% rate), Avg Error: {avg_error:.1f}h\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309",
   "metadata": {
    "id": "283"
   },
   "source": [
    "## 7. Save Results\n",
    "\n",
    "Save anomaly detection results to CSV files in `data/interim/` directory.\n",
    "\n",
    "### Output Files\n",
    "\n",
    "**1. packages_anomaly_detection.csv**\n",
    "- Complete test dataset with all detection scores and flags\n",
    "- Includes: identifiers, route info, temporal features, predictions, residuals, detection flags, ensemble scores\n",
    "\n",
    "**2. packages_anomalies_only.csv**\n",
    "- Filtered dataset containing only flagged anomalies (`is_anomaly = 1`)\n",
    "- For quick analysis and operational reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310",
   "metadata": {
    "id": "14e2bcc8"
   },
   "outputs": [],
   "source": [
    "# Save anomaly detection results\n",
    "import os\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = '../data/interim'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Select columns to save\n",
    "columns_to_save = [\n",
    "    'MAILITM_FID', 'RECPTCL_FID', 'date',\n",
    "    'etablissement_postal', 'next_etablissement_postal',\n",
    "    'EVENT_TYPE_CD', 'hour', 'day_of_week', 'month',\n",
    "    TARGET_COL, 'predicted_duration', 'residual', 'absolute_residual',\n",
    "    'z_score', 'anomaly_zscore', 'anomaly_iqr', 'anomaly_iforest',\n",
    "    'iforest_score', 'anomaly_score', 'is_anomaly'\n",
    "]\n",
    "\n",
    "# Filter to only include columns that exist\n",
    "columns_to_save = [col for col in columns_to_save if col in anomaly_df.columns]\n",
    "\n",
    "# Save full results\n",
    "output_file = os.path.join(output_dir, 'packages_anomaly_detection.csv')\n",
    "anomaly_df[columns_to_save].to_csv(output_file, index=False)\n",
    "print(f\"✓ Saved full results to: {output_file}\")\n",
    "\n",
    "# Save only anomalies\n",
    "anomalies_file = os.path.join(output_dir, 'packages_anomalies_only.csv')\n",
    "anomaly_df[anomaly_df['is_anomaly'] == 1][columns_to_save].to_csv(anomalies_file, index=False)\n",
    "print(f\"✓ Saved anomalies only to: {anomalies_file}\")\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"Total records: {len(anomaly_df)}\")\n",
    "print(f\"Anomalies detected: {anomaly_df['is_anomaly'].sum()} ({anomaly_df['is_anomaly'].mean()*100:.2f}%)\")\n",
    "print(f\"Normal records: {(anomaly_df['is_anomaly'] == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311",
   "metadata": {
    "id": "809773ca"
   },
   "source": [
    "# Comprehensive Anomaly Insights\n",
    "\n",
    "Consolidated findings from anomaly detection analysis combining country-level and establishment-level insights for actionable intelligence.\n",
    "\n",
    "## Report Structure\n",
    "\n",
    "### 1. Country-Level Analysis (Top 3)\n",
    "- Volume metrics, anomaly rates, performance impact\n",
    "- National contribution to total anomalies\n",
    "\n",
    "### 2. Establishment-Level Breakdown\n",
    "- **Top 5 Problematic Origins**: Anomaly volume, rate, error magnitude\n",
    "- **Top 5 Problematic Destinations**: Delivery concentration and error metrics\n",
    "\n",
    "### 3. Route-Level Intelligence\n",
    "- **Top 3 Most Problematic Routes**: Anomaly frequency, prediction errors, contribution to country burden\n",
    "\n",
    "### 4. Detection Method Insights\n",
    "- Method breakdown by Z-Score, IQR, and Isolation Forest\n",
    "- Agreement patterns across methods\n",
    "\n",
    "### 5. Temporal Patterns\n",
    "- Weekly distribution identifying systematic operational issues\n",
    "\n",
    "### 6. Overall System Statistics\n",
    "- Global summary and key performance indicators\n",
    "\n",
    "## Applications\n",
    "\n",
    "- **Immediate Action**: Prioritized intervention list\n",
    "- **Trend Monitoring**: Track patterns over time\n",
    "- **Capacity Planning**: Identify infrastructure needs\n",
    "- **Quality Assurance**: Verify delivery SLAs\n",
    "\n",
    "**Interpretation:**\n",
    "- Anomaly rates >5% indicate systematic issues\n",
    "- Error magnitudes >24h suggest infrastructure/staffing problems\n",
    "- Concentration patterns identify critical intervention points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312",
   "metadata": {
    "id": "058c5fcd"
   },
   "outputs": [],
   "source": [
    "# FINAL COMPREHENSIVE SUMMARY: Country Analysis with Establishment Details\n",
    "\n",
    "if len(anomalies) > 0 and 'MAILITM_FID' in anomalies.columns:\n",
    "    print(\"=\"*100)\n",
    "    print(\" \" * 25 + \"COMPREHENSIVE ANOMALY SUMMARY BY COUNTRY AND ESTABLISHMENT\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "    # Get top 3 countries from earlier analysis\n",
    "    top_countries = anomalies_country['country_code'].value_counts().head(3)\n",
    "\n",
    "    for rank, (country_code, country_anomaly_count) in enumerate(top_countries.items(), 1):\n",
    "        country_name = country_names.get(country_code, country_code)\n",
    "        country_anomalies_detail = anomalies_country[anomalies_country['country_code'] == country_code]\n",
    "        country_total = anomaly_df_country[anomaly_df_country['country_code'] == country_code]\n",
    "\n",
    "        # Calculate country-level metrics\n",
    "        anomaly_rate = (country_anomaly_count / len(country_total)) * 100 if len(country_total) > 0 else 0\n",
    "        pct_of_all_anomalies = (country_anomaly_count / len(anomalies)) * 100\n",
    "\n",
    "        avg_residual = country_anomalies_detail['residual'].mean()\n",
    "        avg_actual_duration = country_anomalies_detail[TARGET_COL].mean()\n",
    "        avg_predicted_duration = country_anomalies_detail['predicted_duration'].mean()\n",
    "\n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(f\"#{rank}. {country_name.upper()} ({country_code})\")\n",
    "        print(f\"{'='*100}\")\n",
    "\n",
    "        # Country Overview\n",
    "        print(f\"\\nCOUNTRY OVERVIEW:\")\n",
    "        print(f\"   • Total Samples: {len(country_total):,}\")\n",
    "        print(f\"   • Anomalies: {country_anomaly_count:,} ({anomaly_rate:.2f}% of country samples)\")\n",
    "        print(f\"   • Share of All Anomalies: {pct_of_all_anomalies:.1f}%\")\n",
    "        print(f\"   • Avg Prediction Error: {avg_residual:.1f} hours\")\n",
    "        print(f\"   • Avg Actual Duration: {avg_actual_duration:.1f}h | Avg Predicted: {avg_predicted_duration:.1f}h\")\n",
    "\n",
    "        # Establishment Analysis for this country\n",
    "        if 'etablissement_postal' in country_anomalies_detail.columns:\n",
    "            print(f\"\\nTOP 5 PROBLEMATIC ESTABLISHMENTS (ORIGIN) IN {country_name.upper()}:\")\n",
    "            print(f\"   {'Rank':<6} {'Establishment':<20} {'Anomalies':<12} {'% of Country':<15} {'Anomaly Rate':<15} {'Avg Error (h)'}\")\n",
    "            print(f\"   {'-'*85}\")\n",
    "\n",
    "            etab_in_country = country_anomalies_detail['etablissement_postal'].value_counts().head(5)\n",
    "\n",
    "            for etab_rank, (etab_code, etab_anom_count) in enumerate(etab_in_country.items(), 1):\n",
    "                # Get total samples from this establishment in this country\n",
    "                etab_total_in_country = country_total[country_total['etablissement_postal'] == etab_code].shape[0]\n",
    "                etab_anomaly_rate = (etab_anom_count / etab_total_in_country) * 100 if etab_total_in_country > 0 else 0\n",
    "                pct_of_country_anomalies = (etab_anom_count / country_anomaly_count) * 100\n",
    "\n",
    "                # Get average error for this establishment\n",
    "                etab_data = country_anomalies_detail[country_anomalies_detail['etablissement_postal'] == etab_code]\n",
    "                etab_avg_error = etab_data['absolute_residual'].mean()\n",
    "\n",
    "                print(f\"   {etab_rank:<6} {etab_code:<20} {etab_anom_count:<12} {pct_of_country_anomalies:<14.1f}% \"\n",
    "                      f\"{etab_anomaly_rate:<14.1f}% {etab_avg_error:>10.1f}\")\n",
    "\n",
    "        # Destination Establishment Analysis\n",
    "        if 'next_etablissement_postal' in country_anomalies_detail.columns:\n",
    "            print(f\"\\nTOP 5 PROBLEMATIC DESTINATIONS IN {country_name.upper()}:\")\n",
    "            print(f\"   {'Rank':<6} {'Destination':<20} {'Anomalies':<12} {'% of Country':<15} {'Avg Error (h)'}\")\n",
    "            print(f\"   {'-'*70}\")\n",
    "\n",
    "            dest_in_country = country_anomalies_detail['next_etablissement_postal'].value_counts().head(5)\n",
    "\n",
    "            for dest_rank, (dest_code, dest_anom_count) in enumerate(dest_in_country.items(), 1):\n",
    "                pct_of_country_anomalies = (dest_anom_count / country_anomaly_count) * 100\n",
    "\n",
    "                # Get average error for this destination\n",
    "                dest_data = country_anomalies_detail[country_anomalies_detail['next_etablissement_postal'] == dest_code]\n",
    "                dest_avg_error = dest_data['absolute_residual'].mean()\n",
    "\n",
    "                print(f\"   {dest_rank:<6} {dest_code:<20} {dest_anom_count:<12} {pct_of_country_anomalies:<14.1f}% {dest_avg_error:>10.1f}\")\n",
    "\n",
    "        # Top problematic routes within this country\n",
    "        if 'etablissement_postal' in country_anomalies_detail.columns and 'next_etablissement_postal' in country_anomalies_detail.columns:\n",
    "            print(f\"\\nTOP 3 MOST PROBLEMATIC ROUTES IN {country_name.upper()}:\")\n",
    "\n",
    "            country_anomalies_detail_temp = country_anomalies_detail.copy()\n",
    "            country_anomalies_detail_temp['route_pair'] = (\n",
    "                country_anomalies_detail_temp['etablissement_postal'].astype(str) + ' → ' +\n",
    "                country_anomalies_detail_temp['next_etablissement_postal'].astype(str)\n",
    "            )\n",
    "\n",
    "            route_counts_country = country_anomalies_detail_temp['route_pair'].value_counts().head(3)\n",
    "\n",
    "            for route_rank, (route, route_count) in enumerate(route_counts_country.items(), 1):\n",
    "                route_data = country_anomalies_detail_temp[country_anomalies_detail_temp['route_pair'] == route]\n",
    "                route_avg_error = route_data['absolute_residual'].mean()\n",
    "                route_avg_actual = route_data[TARGET_COL].mean()\n",
    "                route_avg_predicted = route_data['predicted_duration'].mean()\n",
    "\n",
    "                print(f\"\\n   #{route_rank}. {route}\")\n",
    "                print(f\"      • Anomalies: {route_count} ({route_count/country_anomaly_count*100:.1f}% of country anomalies)\")\n",
    "                print(f\"      • Avg Error: {route_avg_error:.1f}h\")\n",
    "                print(f\"      • Avg Actual Duration: {route_avg_actual:.1f}h | Predicted: {route_avg_predicted:.1f}h\")\n",
    "\n",
    "        # Detection Method Breakdown\n",
    "        if 'anomaly_zscore' in country_anomalies_detail.columns:\n",
    "            zscore_count = country_anomalies_detail['anomaly_zscore'].sum()\n",
    "            iqr_count = country_anomalies_detail['anomaly_iqr'].sum()\n",
    "            iforest_count = country_anomalies_detail['anomaly_iforest'].sum()\n",
    "\n",
    "            print(f\"\\nDETECTION METHOD BREAKDOWN:\")\n",
    "            print(f\"   • Z-Score: {zscore_count} ({zscore_count/country_anomaly_count*100:.1f}%)\")\n",
    "            print(f\"   • IQR: {iqr_count} ({iqr_count/country_anomaly_count*100:.1f}%)\")\n",
    "            print(f\"   • Isolation Forest: {iforest_count} ({iforest_count/country_anomaly_count*100:.1f}%)\")\n",
    "\n",
    "        # Temporal Patterns\n",
    "        if 'day_of_week' in country_anomalies_detail.columns:\n",
    "            day_map = {0: 'Mon', 1: 'Tue', 2: 'Wed', 3: 'Thu', 4: 'Fri', 5: 'Sat', 6: 'Sun'}\n",
    "            day_dist = country_anomalies_detail['day_of_week'].value_counts().sort_index()\n",
    "\n",
    "            print(f\"\\nWEEKLY DISTRIBUTION:\")\n",
    "            day_bars = \"   \"\n",
    "            for day, count in day_dist.items():\n",
    "                pct = (count / country_anomaly_count) * 100\n",
    "                day_name = day_map.get(day, str(day))\n",
    "                day_bars += f\"{day_name}: {count} ({pct:.0f}%)  |  \"\n",
    "            print(day_bars)\n",
    "\n",
    "    # Overall Summary Statistics\n",
    "    print(f\"\\n\\n{'='*100}\")\n",
    "    print(f\" \" * 35 + \"OVERALL SUMMARY STATISTICS\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "\n",
    "    print(f\"DETECTION SUMMARY:\")\n",
    "    print(f\"   • Total Test Samples: {len(anomaly_df):,}\")\n",
    "    print(f\"   • Total Anomalies Detected: {len(anomalies):,} ({len(anomalies)/len(anomaly_df)*100:.2f}%)\")\n",
    "    print(f\"   • Z-Score Anomalies: {anomalies['anomaly_zscore'].sum():,} ({anomalies['anomaly_zscore'].sum()/len(anomalies)*100:.1f}% of anomalies)\")\n",
    "    print(f\"   • IQR Anomalies: {anomalies['anomaly_iqr'].sum():,} ({anomalies['anomaly_iqr'].sum()/len(anomalies)*100:.1f}% of anomalies)\")\n",
    "    print(f\"   • Isolation Forest Anomalies: {anomalies['anomaly_iforest'].sum():,} ({anomalies['anomaly_iforest'].sum()/len(anomalies)*100:.1f}% of anomalies)\")\n",
    "\n",
    "    print(f\"\\nKEY INSIGHTS:\")\n",
    "    print(f\"   • Top Country: {country_names.get(top_countries.index[0], top_countries.index[0])} with {top_countries.iloc[0]:,} anomalies\")\n",
    "    print(f\"   • Average Prediction Error: {anomalies['absolute_residual'].mean():.1f} hours\")\n",
    "    print(f\"   • Maximum Prediction Error: {anomalies['absolute_residual'].max():.1f} hours\")\n",
    "    print(f\"   • Countries with Anomalies: {anomalies_country['country_code'].nunique()}\")\n",
    "    print(f\"   • Establishments with Anomalies: {anomalies['etablissement_postal'].nunique()}\")\n",
    "\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"Analysis complete. Use the detailed breakdowns above to identify and address specific problem areas.\")\n",
    "    print(f\"{'='*100}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313",
   "metadata": {
    "id": "4c6aaba7"
   },
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314",
   "metadata": {
    "id": "182e860c"
   },
   "outputs": [],
   "source": [
    "clus_packages_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315",
   "metadata": {
    "id": "e1d1f5ac"
   },
   "outputs": [],
   "source": [
    "clus_receptacles_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316",
   "metadata": {
    "id": "308e530e"
   },
   "source": [
    "### Null Values Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317",
   "metadata": {
    "id": "3b0fedc1"
   },
   "source": [
    "- `next_etablissement_postal` is the only remaining feature having null values\n",
    "- For each dataset, we'll look at the `next_etablissement_postal`'s values and take the most frequent one corresponding to a specific `etablissement_potsal`'s value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318",
   "metadata": {
    "id": "c0937dca"
   },
   "outputs": [],
   "source": [
    "def get_etab_mapping(df):\n",
    "    # 1. Calculate the global mode once (to use as a safe fallback)\n",
    "    global_mode = df['next_etablissement_postal'].mode().iat[0]\n",
    "\n",
    "    # 2. Get the mode for every group at once\n",
    "    # This creates a Series where index = etablissement, value = most frequent next\n",
    "    modes_per_group = df.groupby('etablissement_postal')['next_etablissement_postal'].agg(\n",
    "        lambda x: x.mode().iat[0] if not x.mode().empty else global_mode\n",
    "    )\n",
    "\n",
    "    # 3. Convert to dictionary\n",
    "    etablissement_dict = modes_per_group.to_dict()\n",
    "    return etablissement_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319",
   "metadata": {
    "id": "b12607c2"
   },
   "outputs": [],
   "source": [
    "pkg_etab_mapping = get_etab_mapping(clus_packages_df)\n",
    "rcp_etab_mapping = get_etab_mapping(clus_receptacles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5eed01b6",
    "outputId": "b2056a7d-d603-4b1a-a689-925b7dac6d58"
   },
   "outputs": [],
   "source": [
    "null_mask = clus_packages_df['next_etablissement_postal'].isna()\n",
    "\n",
    "# Apply to original dataframe directly\n",
    "clus_packages_df.loc[null_mask, 'next_etablissement_postal'] = (\n",
    "    clus_packages_df.loc[null_mask, 'etablissement_postal'].map(pkg_etab_mapping)\n",
    ")\n",
    "\n",
    "clus_packages_df['next_etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321",
   "metadata": {
    "id": "WdC19HSpsX8n"
   },
   "source": [
    "* We'll drop the columns that affect clustering's results, and keep only columns that indicate similarity between packages/receptacles.\n",
    "* Make sure that `year` and `day_of_month` columns exist in both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322",
   "metadata": {
    "id": "nfBrtBoWqzHH"
   },
   "outputs": [],
   "source": [
    "clus_packages_df['year'] = clus_packages_df['date'].dt.year\n",
    "clus_packages_df['day_of_month'] = clus_packages_df['date'].dt.day\n",
    "clus_packages_df = clus_packages_df.drop(columns=['RECPTCL_FID', 'MAILITM_FID', 'date', 'serial_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wuKeVFTptyo1",
    "outputId": "956cff73-5640-4d27-ac02-742d1ed85c68"
   },
   "outputs": [],
   "source": [
    "clus_packages_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324",
   "metadata": {
    "id": "KyNiX73vrElX"
   },
   "outputs": [],
   "source": [
    "clus_receptacles_df['year'] = clus_receptacles_df['date'].dt.year\n",
    "clus_receptacles_df['day_of_month'] = clus_receptacles_df['date'].dt.day\n",
    "clus_receptacles_df = clus_receptacles_df.drop(columns=['RECPTCL_FID', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DylBBkmAtrCr",
    "outputId": "6ff243b5-f4b3-4322-d915-248d7f62b6c4"
   },
   "outputs": [],
   "source": [
    "clus_receptacles_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fe6d8561",
    "outputId": "4f8e8dbd-9ac3-424b-ac37-882bfbcd69d6"
   },
   "outputs": [],
   "source": [
    "null_mask = clus_receptacles_df['next_etablissement_postal'].isna()\n",
    "\n",
    "# Apply to original dataframe directly\n",
    "clus_receptacles_df.loc[null_mask, 'next_etablissement_postal'] = (\n",
    "    clus_receptacles_df.loc[null_mask, 'etablissement_postal'].map(rcp_etab_mapping)\n",
    ")\n",
    "\n",
    "clus_receptacles_df['next_etablissement_postal'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fznYU0q6rh0p",
    "outputId": "611d1c92-e1da-4adb-973d-a61f3aab98ed"
   },
   "outputs": [],
   "source": [
    "# !pip install kmodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328",
   "metadata": {
    "id": "7d8d5eb2"
   },
   "outputs": [],
   "source": [
    "from kmodes.kmodes import KModes\n",
    "\n",
    "# Iterating through k values (clusters)\n",
    "for k in range(5, 30, 5):\n",
    "    # Fixed n_clusters to k, and set n_init to a reasonable constant like 2\n",
    "    km_pkg = KModes(n_clusters=k, init='Huang', n_init=2, verbose=0)\n",
    "    km_rcp = KModes(n_clusters=k, init='Huang', n_init=2, verbose=0)\n",
    "\n",
    "    # Fit and predict\n",
    "    clusters_pkg = km_pkg.fit_predict(clus_packages_df)\n",
    "    clusters_rcp = km_rcp.fit_predict(clus_receptacles_df)\n",
    "\n",
    "    # Print evaluation (Cost)\n",
    "    print(f\"K={k} | Packages Cost: {km_pkg.cost_:.2f} | Receptacles Cost: {km_rcp.cost_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329",
   "metadata": {
    "id": "_ug0vsqastV1"
   },
   "source": [
    "#### Clustering on Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330",
   "metadata": {
    "id": "B9LWWJhzue3O"
   },
   "source": [
    "* We Only keep columns that existed in the original dataset\n",
    "* We decompose the `date` columns into multiple features to allow fair and less-restrictive comparison (not on minutes or seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331",
   "metadata": {
    "id": "yBq4DqTFswa_"
   },
   "outputs": [],
   "source": [
    "raw_clustering_pkg = clus_packages_df[['etablissement_postal', 'next_etablissement_postal', 'EVENT_TYPE_CD', 'day_of_month', 'month', 'year', 'hour', 'day_of_week']]\n",
    "raw_clustering_rcp = clus_receptacles_df[['etablissement_postal', 'next_etablissement_postal', 'EVENT_TYPE_CD', 'day_of_month', 'month', 'year', 'hour', 'day_of_week']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332",
   "metadata": {
    "id": "a4-zWE70tHBR"
   },
   "outputs": [],
   "source": [
    "# Iterating through k values (clusters)\n",
    "for k in range(5, 30, 5):\n",
    "    # Fixed n_clusters to k, and set n_init to a reasonable constant like 2\n",
    "    km_pkg = KModes(n_clusters=k, init='Huang', n_init=2, verbose=0)\n",
    "    km_rcp = KModes(n_clusters=k, init='Huang', n_init=2, verbose=0)\n",
    "\n",
    "    # Fit and predict\n",
    "    raw_clusters_pkg = km_pkg.fit_predict(raw_clustering_pkg)\n",
    "    raw_clusters_rcp = km_rcp.fit_predict(raw_clustering_rcp)\n",
    "\n",
    "    # Print evaluation (Cost)\n",
    "    print(f\"K={k} | Packages Cost: {km_pkg.cost_:.2f} | Receptacles Cost: {km_rcp.cost_:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
