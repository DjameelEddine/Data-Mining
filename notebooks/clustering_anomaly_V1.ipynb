{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Load the clean merged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##from data/interim/interim_merged load the data\n",
    "merged_df = pd.read_csv( '../data/interim/interim_merged_packages_receptacle_df.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "- print the 10 first rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the first 10 rows of merged_df\n",
    "print(merged_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(columns=[\n",
    "    'Unnamed: 0_package',\n",
    "    'Unnamed: 0_receptacle',\n",
    "    'RECPTCL_FID', 'MAILITM_FID', 'serial_number'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sort chronologically\n",
    "merged_df = merged_df.sort_values('date_package').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_clean = merged_df.copy()\n",
    "# Convert date columns\n",
    "date_cols = ['date_package', 'date_receptacle']\n",
    "for col in date_cols:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = pd.to_datetime(df_clean[col])\n",
    "        \n",
    "# Extract temporal features\n",
    "def extract_datetime_features(df, date_col):\n",
    "    df[f'{date_col}_year'] = df[date_col].dt.year\n",
    "    df[f'{date_col}_month'] = df[date_col].dt.month\n",
    "    df[f'{date_col}_day'] = df[date_col].dt.day\n",
    "    df[f'{date_col}_hour'] = df[date_col].dt.hour\n",
    "    df[f'{date_col}_dayofweek'] = df[date_col].dt.dayofweek\n",
    "    return df\n",
    "\n",
    "for date_col in date_cols:\n",
    "    if date_col in df_clean.columns:\n",
    "        df_clean = extract_datetime_features(df_clean, date_col)\n",
    "\n",
    "# Convert timedelta to numeric\n",
    "duration_cols = ['processing_duration_package', 'processing_duration_receptacle']\n",
    "for col in duration_cols:\n",
    "    if col in df_clean.columns:\n",
    "        if df_clean[col].dtype == 'object':\n",
    "            df_clean[col] = pd.to_timedelta(df_clean[col]).dt.total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "- for the service indicator remeber to do the mapping for all existing ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Copy cleaned dataframe\n",
    "df_features = df_clean.copy()\n",
    "\n",
    "# 1. Route consistency features\n",
    "df_features['same_origin_destination'] = (\n",
    "    df_features['origin_destination_package'] == df_features['origin_destination_receptacle']\n",
    ").astype(int)\n",
    "\n",
    "# 2. Service type mapping (S10-12)\n",
    "service_mapping = {\n",
    "    # Domestic / private use\n",
    "    'AP': 8, 'AR': 8, 'AS': 8,\n",
    "    # Registered Letter Post (Priority 2)\n",
    "    'BC': 2,\n",
    "    # Parcel Post (Standard)\n",
    "    'CA': 5,\n",
    "    # Lower-priority Parcel / Letter Goods\n",
    "    'CB': 4, 'CC': 4, 'CD': 4, 'CE': 4, 'CF': 4, 'CG': 4, 'CH': 4,\n",
    "    # Parcel Post insured or special\n",
    "    'CI': 6, 'CJ': 6, 'CK': 6, 'CL': 6, 'CM': 6, 'CN': 6, 'CO': 6, 'CP': 6, \n",
    "    'CQ': 6, 'CR': 6, 'CS': 6, 'CU': 6, 'CV': 6, 'CX': 6, 'CY': 6,\n",
    "    # Tracked Letter Post (Priority 3)\n",
    "    'LA': 3, 'LB': 3, 'LD': 3, 'LE': 3, 'LF': 3, 'LG': 3, 'LH': 3, 'LI': 3, \n",
    "    'LJ': 3, 'LK': 3, 'LL': 3, 'LM': 3, 'LN': 3, 'LP': 3, 'LR': 3, 'LS': 3, \n",
    "    'LV': 3, 'LW': 3, 'LX': 3, 'LY': 3, 'LZ': 3\n",
    "}\n",
    "df_features['service_code_package'] = df_features['service_indicator'].map(service_mapping)\n",
    "\n",
    "# 4. Geographic features\n",
    "df_features['is_international'] = df_features['origin_country_package'] != df_features['destination_country_package']\n",
    "\n",
    "# 5. Frequency encoding for flow_type and etablissement_postal columns\n",
    "freq_encode_cols = [\n",
    "    'flow_type_package', 'flow_type_receptacle',\n",
    "    'etablissement_postal_package', 'next_etablissement_postal_package',\n",
    "    'etablissement_postal_receptacle', 'next_etablissement_postal_receptacle', 'EVENT_TYPE_CD_package', 'EVENT_TYPE_CD_receptacle'\n",
    "]\n",
    "\n",
    "for col in freq_encode_cols:\n",
    "    freq = df_features[col].value_counts(normalize=True)\n",
    "    df_features[col + '_freq'] = df_features[col].map(freq)\n",
    "\n",
    "# 6. Select features for clustering\n",
    "cluster_features = [\n",
    "    'service_code_package',\n",
    "    'same_origin_destination',\n",
    "    'num_etablissements_package',\n",
    "    'num_etablissements_receptacle',\n",
    "    'processing_duration_package',\n",
    "    'processing_duration_receptacle',\n",
    "    'is_international'\n",
    "]\n",
    "\n",
    "# Add frequency-encoded features\n",
    "for col in freq_encode_cols:\n",
    "    cluster_features.append(col + '_freq')\n",
    "\n",
    "# Final feature matrix\n",
    "X = df_features[cluster_features].fillna(0)\n",
    "\n",
    "# Optional: scale features for clustering\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Clustering algo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 1 K-mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine optimal number of clusters\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 10)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    silhouette_scores.append(silhouette_score(X, labels))\n",
    "\n",
    "# Plot elbow method\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, silhouette_scores, 'bo-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Optimal Number of Clusters')\n",
    "plt.show()\n",
    "\n",
    "# Apply K-Means with optimal k\n",
    "optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "df_features['cluster_kmeans'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Analyze clusters\n",
    "cluster_summary = df_features.groupby('cluster_kmeans').agg({\n",
    "    'same_origin_destination': 'mean',\n",
    "    'processing_duration_package': 'mean',\n",
    "    'num_etablissements_package': 'mean'\n",
    "})\n",
    "print(\"Cluster Summary:\")\n",
    "print(cluster_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 2. DBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Find optimal eps\n",
    "neighbors = NearestNeighbors(n_neighbors=5)\n",
    "neighbors_fit = neighbors.fit(X)\n",
    "distances, indices = neighbors_fit.kneighbors(X)\n",
    "distances = np.sort(distances[:, -1], axis=0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(distances)\n",
    "plt.xlabel('Points sorted by distance')\n",
    "plt.ylabel('5th Nearest Neighbor Distance')\n",
    "plt.title('K-Distance Graph for DBSCAN')\n",
    "plt.show()\n",
    "\n",
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "df_features['cluster_dbscan'] = dbscan.fit_predict(X)\n",
    "\n",
    "# Analyze DBSCAN results\n",
    "print(f\"Number of clusters found: {len(set(dbscan.labels_)) - (1 if -1 in dbscan.labels_ else 0)}\")\n",
    "print(f\"Number of outliers: {sum(dbscan.labels_ == -1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Isolation Forest\n",
    "iso_forest = IsolationForest(\n",
    "    contamination=0.05,  # expected proportion of outliers\n",
    "    random_state=42,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "df_features['anomaly_score'] = iso_forest.fit_predict(X_scaled)\n",
    "df_features['is_anomaly'] = df_features['anomaly_score'] == -1\n",
    "\n",
    "# Analyze anomalies\n",
    "anomalies = df_features[df_features['is_anomaly']]\n",
    "print(f\"Detected {len(anomalies)} anomalies\")\n",
    "print(\"\\nAnomaly characteristics:\")\n",
    "print(anomalies[cluster_features].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "lof = LocalOutlierFactor(\n",
    "    contamination=0.05,\n",
    "    novelty=False\n",
    ")\n",
    "\n",
    "df_features['lof_score'] = lof.fit_predict(X_scaled)\n",
    "df_features['is_lof_anomaly'] = df_features['lof_score'] == -1\n",
    "\n",
    "# Compare different anomaly detection methods\n",
    "anomaly_comparison = df_features.groupby(['is_anomaly', 'is_lof_anomaly']).size().unstack()\n",
    "print(\"Anomaly Detection Comparison:\")\n",
    "print(anomaly_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Visualization using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Clusters\n",
    "plt.subplot(131)\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                      c=df_features['cluster_kmeans'], \n",
    "                      cmap='viridis', alpha=0.6)\n",
    "plt.colorbar(scatter)\n",
    "plt.title('K-Means Clusters')\n",
    "\n",
    "# Plot 2: Anomalies\n",
    "plt.subplot(132)\n",
    "colors = ['blue' if not x else 'red' for x in df_features['is_anomaly']]\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=colors, alpha=0.6)\n",
    "plt.title('Anomalies (Isolation Forest)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_insights(df_features):\n",
    "    insights = []\n",
    "    \n",
    "    #  Unusual processing times\n",
    "    anomalies_processing = df_features[df_features['processing_duration_package'] > 15 * 24 * 60 * 60]  # 15 days in seconds\n",
    "    if len(anomalies_processing) > 0:\n",
    "        insights.append(f\"Found {len(anomalies_processing)} items with unusually long processing times\")\n",
    "    \n",
    "\n",
    "    #  Route inconsistencies\n",
    "    route_inconsistencies = df_features[df_features['same_origin_destination'] == 0]\n",
    "    if len(route_inconsistencies) > 0:\n",
    "        insights.append(f\"Found {len(route_inconsistencies)} cases with mismatched package-receptacle routes\")\n",
    "    \n",
    "    #  Establishment patterns\n",
    "    establishment_counts = df_features.groupby('etablissement_postal_package').size()\n",
    "    unusual_establishments = establishment_counts[establishment_counts < 3]  # rarely used establishments\n",
    "    if len(unusual_establishments) > 0:\n",
    "        insights.append(f\"Found {len(unusual_establishments)} rarely used postal establishments\")\n",
    "    \n",
    "    return insights\n",
    "\n",
    "insights = generate_insights(df_features)\n",
    "for i, insight in enumerate(insights, 1):\n",
    "    print(f\"{i}. {insight}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
